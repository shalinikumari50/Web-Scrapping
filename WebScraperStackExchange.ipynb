{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WebScraperStackExchange.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPLcrbzuyatzKpue/B9Pvyi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shalinikumari50/Web-Scrapping/blob/master/WebScraperStackExchange.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmuyIM3dQKHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import csv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL2-aAUCQRqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CategoryList = [\n",
        "                \"https://datascience.stackexchange.com/questions/tagged/machine-learning?tab=newest&page=1&pagesize=50\", \n",
        "                \n",
        "                ]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9STbNBeQZ4p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1e19defa-fb15-40b3-c0d6-0e7c35bd4abe"
      },
      "source": [
        "import math\n",
        "import re\n",
        "import time\n",
        "\n",
        "\n",
        "for category in CategoryList:\n",
        "            rowList = []\n",
        "            baseLinkIndex = category.find(\"/questions/\")\n",
        "            pageLinkIndex = category.find(\"1\")  \n",
        "            \n",
        "            categoryHTML = requests.get(category,headers = {'User-agent': 'your bot 0.1'})\n",
        "            if not categoryHTML.ok:\n",
        "                    print(\"scrapping stopped of a category\")\n",
        "                    time.sleep(300)\n",
        "                    continue\n",
        "            \n",
        "            parser = BeautifulSoup(categoryHTML.text, 'html.parser')\n",
        "            pages = parser.find('div',class_=\"fs-body3 grid--cell fl1 mr12 sm:mr0 sm:mb12\").getText()\n",
        "            noOfPages = int(\"\".join(map(str, re.findall('\\d+', pages))))\n",
        "            noOfPages = math.ceil(noOfPages/50)\n",
        "            \n",
        "            for i in range(1,noOfPages+1):\n",
        "              print(\"page number ------------------------------------\" + str(i))\n",
        "              pageLink = category[:pageLinkIndex]+ str(i) + category[pageLinkIndex+1:]\n",
        "             \n",
        "              pageHTML = requests.get(pageLink,headers = {'User-agent': 'your bot 0.1'})\n",
        "              if not pageHTML.ok:\n",
        "                    print(\"scrapping stopped of a page\")\n",
        "                    time.sleep(200)\n",
        "                    continue\n",
        "              parser = BeautifulSoup(pageHTML.text, 'html.parser')\n",
        "              questionLinks = parser.find_all('a',class_=\"question-hyperlink\")\n",
        "              for questionLink in questionLinks:\n",
        "                  anchor = questionLink.get('href')\n",
        "                  if not 'questions' in anchor or 'https://' in anchor:\n",
        "                        continue\n",
        "                  link=category[:baseLinkIndex]+questionLink.get('href')\n",
        "                  \n",
        "                  url = requests.get(link,headers = {'User-agent': 'your bot 0.1'})\n",
        "                \n",
        "                  if not url.ok:\n",
        "                    print(\"scrapping stopped of a question\")\n",
        "                    time.sleep(30)\n",
        "                    break\n",
        "                  ques = BeautifulSoup(url.text,'html.parser')\n",
        "                  post = ques.find('div', class_=\"post-text\")\n",
        "                  maths = post.find_all('span',class_=\"math-container\")\n",
        "                  codes = post.find_all('code')\n",
        "                  for math in maths:\n",
        "                    math.extract()\n",
        "                  for code in codes:\n",
        "                    code.extract() \n",
        "                  posts = post.getText()\n",
        "                  cleaned_post = ''.join([x.replace(\"\\n\",\" \") for x in posts])\n",
        "                  tags = ques.find_all('a', class_=\"post-tag\",rel = 'tag')\n",
        "                  tagList = []\n",
        "                  for t in tags:\n",
        "                    tagList.append(t.getText())\n",
        "                  \n",
        "                  \n",
        "                  row = {'link': link, 'post':cleaned_post,'Tags' : set(tagList)}\n",
        "                  print(row)\n",
        "                  rowList.append(row)\n",
        "                  scraped_data = pd.DataFrame(rowList)\n",
        "                  catIndex = category.find(\".\")\n",
        "                  tagIndex = category.find(\"/tagged/\")\n",
        "                  tagEndIndex = category.find(\"?\")\n",
        "                  name = category[8:catIndex] + \"_\" +category[tagIndex+8:tagEndIndex] + \".csv\"\n",
        "                  scraped_data.to_csv(name)                \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "page number ------------------------------------86\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39953/output-of-k-mean-cluster-as-collection-of-tweets', 'post': \" I want to cluster some 1000 tweets using k-mean algorithm. But I don't want the correct output, I just want clustering of tweets. Suppose 1 cluster contain 300 tweets than all the contain of 300 tweets (contain consist main words of tweet after preprocessing of tweets) in form of text as my output. I don't want top term of cluster but all the terms in cluster. Here in R I implement k-mean.  At the end I get error in below code.  Please suggest any other way to just cluster the tweets and the output like 1 cluster contain all text terms of clustered tweets. \", 'Tags': {'text-mining', 'machine-learning', 'k-means'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39936/estimating-class-prevalence-in-unlabelled-data-after-predicting-labels-with-a-bi', 'post': \" I'm looking to get an estimate of the prevalence of 1's (i.e. the rate of positive labels) in a very large dataset that I have. However, I am hoping to report this percentage as a 95% credible interval instead of as an exact estimate of rate, taking into account the model uncertainties. These are the steps I'm hoping to perform:  Train a binary classifier on labelled training data. Use a labelled test set to estimate the specificity and sensitivity of the classifier. Use the classifier to predict the label for the unlabelled records in the dataset. Obviously I could get an exact prevalence estimate by simply calculating the mean of the predicted outputs. But this is where I'm hoping to implement an approach for reporting the prevalence estimate as an interval.  So my question is: Is there a best-practice approach to doing this? I found this study which trains a binary classifier and then uses a Bayesian prevalence model to report the prevalence as a 95% confidence interval by incorporating the uncertainty associated with the model specificity and sensitivity. However, I'm having trouble understanding exactly what they did here. I'm also not finding many others who have done something similar. So, any suggestions for a reliable approach I could take to do this would be greatly appreciated. Thanks in advance! \", 'Tags': {'classification', 'machine-learning', 'statistics', 'bayesian'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39932/feature-scaling-both-training-and-test-data', 'post': ' It is stated that for: Feature Normalization -   The test set must use identical scaling to the training set.  And the point is given that:  Do not scale the training and test sets using different scalars: this   could lead to random skew in the data.  Could someone explain what that means? ', 'Tags': {'machine-learning', 'data-science-model'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39929/why-normalize-when-all-features-are-on-the-same-scale', 'post': \" So I'm doing the tensorflow tutorial found here: https://www.tensorflow.org/tutorials/keras/basic_classification Basically, my input is a [28x28] matrix (image) that I flatten to a [1x784] vector. The tutorial then says:  We scale these values to a range of 0 to 1 before feeding to the neural network model. For this, cast the datatype of the image components from an integer to a float, and divide by 255.  My question is why do we need to normalize in this case? My understanding is that when we have features that are on different scales, we need normalization if not the output of the model is distorted. But in this case all pixel ranges go from 0 to 255 (all features are the same scale) I went ahead and ran it with normalization, and get an accuracy of over 85%, whereas no normalization, my accuracy falls to 10%. Any ideas? \", 'Tags': {'tensorflow', 'machine-learning', 'normalization'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39916/loss-function-returns-nan-on-time-series-dataset-using-tensorflow', 'post': ' This was the follow up question of Prediction on timeseries data using tensorflow. I have an input and output of below format.  Its a timeseries data. The task is to predict the next number. Basically input was crafted by the below snippet  Now i want to train the model on the input and predict the next number. For instance  and the predicted output would be . In the previous problem, i had confronted the shape issue. Fortunately, i got a quick fill. Now, when i am training the model,I observe my mse values are nan. My implementation in keras is working but not in tensorflow. Most of the solutions in stackoverflow was pointing out to learning rate. Irrespective of giving different learning rate, my mse values are still nan. I have gone through many times reading my implementation but nothing turned up. Below is my tensorflow implementation.  And the output i got was  epoch 0 MSE= nan epoch 10 MSE= nan epoch 20 MSE= nan  Any help is greatly appreciated. Thanks ', 'Tags': {'time-series', 'tensorflow', 'machine-learning', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39910/how-can-we-use-neural-networks-for-decision-making-intead-of-bayesian-networks-o', 'post': ' I am working on Decision Making in Self driving cars and I am wondering how I can use Neural networks (is there any type) ? that can repleace or mimic the bayesian networks or Decision Tree for Decision Making process ?  ', 'Tags': {'neural-network', 'machine-learning', 'deep-learning', 'statistics', 'data-science-model'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39895/using-both-positive-and-negative-values-as-neural-network-input', 'post': ' In neural networks, we sometimes convert the input to z-scores. However, z-scores contain both negative and positive values, if we use such numbers as input, it seems that in some cases the neural network would not be trained well? For example, the loss function w.r.t. a weight variable in the first layer would completely flip if the input (z-score in this case) changes sign. Then the  in such two training sessions would be very likely to cancel each other. ', 'Tags': {'neural-network', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39883/k-nearest-neighbors-complexity', 'post': ' Why does the complexity of KNearest Neighbors increase with lower value of k? And when does the plot for k-nearest neighbor have smooth or complex decision boundary? Please explain in detail. And also , given a data instance to classify, does K-NN compute the probability of each possible class using a statistical model of the input features or just gets the class with the most number of points in favour of it? ', 'Tags': {'scikit-learn', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39879/sequence-models-word2vec', 'post': \" I am working a data-set with more than 100,000 records. This is how the data looks like:  I am trying to understand the campaign sequence and looking for the next possible campaign for the customers. Assume I have processed my data and stored it in 'camp'. With Word2Vec-  The problem with this model is that it accepts tokens and spits out text-tokens with probabilities in return when looking for similarities. Word2Vec accepts this form of input-  And gives this form of output -  Since I want to predict campaign sequence which occurs more frequently in combination with target word, I was wondering if there is anyway I can give below input to the model and get the campaign name in the output My input to be as -  Output -  I am also not sure if there is any functionality within the Word2Vec or any similar algorithms which can help finding the next possible campaign for individual users. \", 'Tags': {'neural-network', 'sequence', 'word2vec', 'machine-learning', 'nlp'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39872/recommender-system-that-connect-users-with-each-other-should-i-go-for-content', 'post': \" I am trying to build a system where user come on the platform and he chooses a topic(predefined few topics) and then we connect him with any random online user who chooses the same topic. Then they can do conversation. Now, I am trying to connect them smartly based on user's historical data (users with whom he had match earlier along with time duration of their conversation, and raing after the conversation etc). and his basic profile data. How can I use collaborative filtering here, because I don't have any product here and their are very few users available online(10-15) at any time so I have to connect any one of them. Thanks in advance! \", 'Tags': {'machine-learning', 'clustering', 'similarity', 'recommender-system', 'statistics'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39833/using-neural-networks-with-jumps-in-stock-returns', 'post': ' I am using an LSTM network to analyse stock return patterns. A problem is that, there is usually huge jumps in stock returns but if you are only using the trading data, the jumps would seem pretty random. (For example, the jumps from SEC ruling against or in favor of a company.) Thus, if the neural network learns too much from the jumps, the results would not generalize well. One might cap the returns or use auto encoders. What are some other methods to regularize such jumps and limit the changes the jumps cause to the network? ', 'Tags': {'neural-network', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39832/when-creating-a-classification-model-should-predictors-with-little-correlation', 'post': ' I am building a predictive model designed to predict attrition within my organization.  I am trying to decide whether to add certain predictors to my model.  I used a Kruskal-Wallis rank sum test to check the correlation between a few of my predictor  variables and my response variable and found that the predictors are independent from my response.  Should I still include these variables in my model? I am leaning towards no due to the lack of correlation with my response variable but don\\'t want to discard a potential \"split\" in my decision tree.  My data set contains 8,225 observations and 173 columns which I have been using as predictors. ', 'Tags': {'machine-learning', 'predictive-modeling', 'correlation', 'decision-trees', 'classification'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39821/binary-classificaiton-for-weather-data-if-its-class-1-or-class-0-alert', 'post': ' I am working on weather data and it has few features that are independent variables such as , ,  etc ... Based on these values, I would like to classify alerts into class 0 or 1. For example, below is row item from data source  For  1 it should be class 1 (Yes) and for others its class 0 (No).The objective is to build a general binary classifier using decision trees. So I started with DTClassifier, but later I realized it could also be done with logistic regression. I am confused which would be a better fit for this kind of data for classification.  Please advice and give me some starting points. ', 'Tags': {'machine-learning', 'binary', 'multilabel-classification'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39810/dataset-creation-for-images', 'post': ' I am creating an  of objects. We have  of objects and need to provide a color to the objects also. What colors(aroun 7-10) should we use? I am thinking of using , but not able to find any justification as to why we should be using VIBGYOR and not anything else. Is there any standard practice? Or do people who create such computer vision or image processing datasets have some justification on which colors to use? Some links to research/papers or datasets with justification would be great!   ', 'Tags': {'classification', 'dataset', 'machine-learning', 'image-classification'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39803/how-to-migrate-r-decision-tree-to-java', 'post': ' I have trained a conditional inference decision tree in R using library  with function  and saved the model in an  file.  I need to migrate this model from R to Java so that I can utilise the tree to make predictions in a Java environment. Can someone please point me in the right direction on how I can do this? ', 'Tags': {'java', 'machine-learning', 'r', 'decision-trees'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39800/how-to-implement-keras-lstm-time-series', 'post': \"       Closed. This question needs to be more focused. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Update the question so it focuses on one problem only by editing this post.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         I am learning how to implement Keras  on a simple time series data.  The dataset I'm using has  columns and  rows. Each group of  rows represents one time-series cycle. Then, the time starts again from zero and run for the next  rows. I want to make predictions for sixof the columns.  My questions are, how many (multiple of 200?) rows do I choose for train/test sets? Is my batch size ? What is the basis for choosing number of neurons?  \", 'Tags': {'neural-network', 'lstm', 'machine-learning', 'deep-learning', 'keras'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39793/what-is-the-relation-between-input-into-lstm-and-number-of-cells', 'post': \" I want to train an LSTM network for time-series predictions, and want to get to the bottom of LSTM's. In my understanding, the number of cells in a single LSTM layer can vary. However, since each cell takes an input at time-step t, wouldn't the number of cells need to be equal to t? For example (from TensorFlow tutorial):  Wouldn't the maximum number of cells in each layer be 5? since after the 5th input we no longer have any other information to input. However, I've seen many networks with a higher number of cells than that. Therefore, why is this a possibility? \", 'Tags': {'neural-network', 'lstm', 'machine-learning', 'deep-learning', 'rnn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39786/latent-space-of-vae', 'post': ' Say I have a dataset  on which I train a basic variational autoencoder (VAE) on (a couple of fully connected layers separated by nonlinearities). Does the latent space of the VAE have a feature coordinate for  and  (which are not part of dataset )?  ', 'Tags': {'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39783/opensource-speech-recognition-library-that-is-secure-and-trained-on-large-data', 'post': \"       Closed. This question needs to be more focused. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Update the question so it focuses on one problem only by editing this post.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         For all those who are working on developing a chatbot/assistant and care about the privacy of users consuming the speech recognition library, can you suggest an open souce library which is trained on a large data. Big concern is the privacy that's why not going for Google Diagflow or IBM Watson or Amazon Lexa or Wit. Would appreciate a lot if someone can suggest a good library. \", 'Tags': {'speech-to-text', 'machine-learning', 'nlp'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39759/how-to-find-the-most-important-attribute-for-each-class', 'post': \" I have a dataset with 28 attributes and 7 class values. I want to know if its possible to find out the most important attribute(s) for deciding the class value, for each class.  For example an answer could be: Attribute 2 is most important for class 1, Attribute 6 is most important for class 2 etc. Or an even more informed answer could be: Attribute 2 being below 0.5 is most important for class 1, Attribute 6 being above 0.75 is most important for class 2 etc My initial approach to this was to build a decision tree on the data and find the node that had the largest information gain/gain ratio for each class and that would be the most determining factor for that class. The problem with this is that the decision tree implementations I have found don't give the information gain/gain ratio for each node and as this is time bound I don't have the time to implement my own version. My current thought is to create multiple datasets which are all one class vs the rest and then perform attribute selection (eg. information gain) on them  to find the most important attribute. Is this the right direction to go down or is their a better option? \", 'Tags': {'neural-network', 'machine-learning', 'deep-learning', 'multiclass-classification', 'feature-selection'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39758/is-my-model-over-fitting-lstm-gru', 'post': \" I have small corpus max 150 text utterances, which is again distributed among 5 categories.  To test I started with basic deep learning model, where I used word2vec embedding, added 1D convolution layer followed by 150 GRU units:  Training & validation loss:  As per this it's fits perfect, but when I give as is utterances to predict it's going to come other class. Code for predicting the utterances:  So, I changed it to LSTM where I increased the LSTM units to 250:  Train & Validation loss:   I am not getting any clue where I am going wrong, even to avoid that I have hard coded the sentence tokens to predict. I wanted to understand is it over fitting or not cos at least by training & validation loss graph I don't think it's.  Please, let me know if you need any more information.   Epochs: 35 Batch size: 20 Shuffle = False Validation split is on 20%   Thanks in advance. \", 'Tags': {'lstm', 'machine-learning', 'nlp', 'keras', 'classification'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39742/creating-a-feature-by-combining-2-features-with-different-units', 'post': \" If I want to create a new feature by combining 2 features with the same unit, I can just subtract them (e.g. buy volume vs sell volume). If the units are different, it doesn't make sense to subtract them (e.g. trade volume vs orderbook size). I can divide them if one of the features is guaranteed to never be 0. If either features can be 0, what's the best way to compare them? I don't want to use an arbitrary high number to represent infinity if the denominator is 0. \", 'Tags': {'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39741/how-to-prepare-photo-data-for-training-model-to-recognize-bowling-ball-name-bra', 'post': ' I am asked to do this. The client can only prepare a only one photo for each ball from the product page of bowling manufacturer.  However, I need huge amount of image data for each ball.  Here is what I am thinking. To fake thousands pieces of photo data, turn the ball into 3D model and take thousands of photo of that ball from different perspective in software like Maya or 3DS Max. I am not sure if this would work for detecting balls in reality since the source of data is not real.  Would my approach of preparation of image data from virtual environment work for training model for detecting objects in reality?   BTW, I do not have specialty in data science. I just have very fundamental concepts of ML. What algorithm would you suggest for my case? Thanks! ', 'Tags': {'dataset', 'machine-learning', 'object-recognition', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39727/using-gridsearchcv-and-a-random-forest-regressor-with-the-same-parameters-gives', 'post': \" As the huge title says I'm trying to use GridSearchCV to find the best parameters for a Random Forest Regressor and I'm measuring my results with mse.  This is the gist of the code (nothing too complex I know, just getting started with it all) When I print the result of grid.best_estimator_ I get this  The problem is if I try to create a regressor with these parameters (without using grid search at all) and train it the same way I get a waaaay bigger MSE on the testing set (5.483837301587303 vs 43.801520165079467)  Does this have to do with the cross validation GridSearchCV performs ? What am I missing here ? \", 'Tags': {'random-forest', 'scikit-learn', 'gridsearchcv', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39700/what-is-the-quantity-sold-for-a-specific-fruit-country-combination', 'post': ' What is the algorithm that generates these potential quantities that meet the given criteria? Essentially - there are number of quantities for a fruit and country combination. E.g:  For complete dataset click here.  Each Country has to be as close as possible to the following values for all fruits sold:  And total fruit quantity across all countries has to be as close as possible to the following values:  An example combination that comes close to meeting the above criteria (having 5 countries and 8 fruits) is:  What is the algorithm that generates these potential quantities that meet this criterion? Problem 1: do we have to do a brute force method of generating all possible combinations or is there a more efficient way? Problem 2: how do we define \"closeness\" - the exact match is the best - what if there is no exact match - then what is the next best option? ', 'Tags': {'statistics', 'probabilistic-programming', 'machine-learning', 'data-mining'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39697/how-can-we-use-machine-learning-to-distnguish-between-similarly-looking-images', 'post': \" How can I build a model which can distinguish between Milk and Phenyl? I want to predict whether a given item is edible to eat or not. If I train a model with thousands of photos of Milk and Phenyl which are labelled, Won't the model get confused between them because they look very very similar? No matter how much I train, I will be getting poor results, won't I? Another such scenario is in the case of salt and sugar.  I want to know if there is a way to tackle this situation. Please let me know if I can achieve better results with any other approach. Also Please let me know If I'm unclear. \", 'Tags': {'accuracy', 'machine-learning', 'image-classification', 'deep-learning', 'image-recognition'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39694/why-does-the-votingclassifier-in-sklearn-ensemble-gives-higher-accuracy-than-eac', 'post': ' I am running an ML classifier on my data. I used SVM, RF and KNN. I used GScv for each of them and then used votingclassifier.The accuracy i got in each classifier independently was low, but from the hard and soft vote of the voting classifier is much higher! Why is that? Here is my code Fitting Kernel SVM to training set  Checking accurancy, Best score of GV  Applying RandomForest (RF) Classification  Applying KNN  Predicting the test set results  ', 'Tags': {'classification', 'machine-learning', 'machine-learning-model'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39693/do-i-use-actual-data-or-data-difference-to-train-machine-learning-model', 'post': \" I would like to predict tomorrows temperature :-). But I'm unsure of the best approach. Do I simply drop data from the last x days, or do I try do drop data from the last x days in difference?  Last three days could look like: 21.3C, 22C, 21.9C Where the difference is whatever previous, 0.7C, -0.1C You would need more than 3 days, I'm just setting an data example. I think I also would need to insert season among other variables to complete the experiment. But I'm just asking for temperature here. Have anyone checked this? While the temperature can be anything from -20 to 40 C with decimals (1, 0.x) makes up a lot of numbers! But with difference the range is maybe -5C to +5C, also with decimals (1, 0.x) but it beats the other range making the range smaller and therefor easier for ML to predict? If difference is best, should I also do percentage difference since then I would take to account different climate has different swings? \", 'Tags': {'machine-learning', 'machine-learning-model'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39664/calculation-of-distance-between-samples-in-data-mining', 'post': ' I am confused about a little issue related to distance calculation. What I want to know is, while calculating the distance between samples in classification or regression, is the label or output class also used or the distance is calculated using all other attributes excluding the label attribute?  ', 'Tags': {'distance', 'machine-learning', 'data-mining'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39624/how-do-we-ensure-that-a-model-generalize-well-in-the-real-world', 'post': '       Closed. This question needs to be more focused. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Update the question so it focuses on one problem only by editing this post.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         When we have trained a model on a training data set as well as the test set, how can we ensure that a model will generalize in the real world.I mean both the test set and the training set are a very small subset compared to what exist in the real world and there might be inherent bias in the datasets as well. As a beginner in machine learning , it will help me and others a lot if you share the techniques to avoid the common pitfalls ', 'Tags': {'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39613/regularization-in-simple-math-explained', 'post': ' I read a lot of articles online about how regularization works and most of them just show the equations with regularization terms but did not use example numbers to explain how the coefficient values change as lambda increase.  For example:  L1 Regularization theory states that When Lambda is large, coefficients tend to 0.  X  =  How is the coefficients(beta i assume) value changing when lambda changes? Is it because X a fixed value? Example, does beta have to decrease when lambda increase to ensure that X is the same? ', 'Tags': {'regularization', 'machine-learning', 'data-mining'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39588/how-to-build-a-classifier-with-a-rejection-class', 'post': \" Let's say I need to build a food classifier, and I want a rejection class for the inputs that are not food.  What is the best way to do that?  Should I just add a new class label that includes pictures of anything but food? Is there a best way to chose which pictures I should use? \", 'Tags': {'neural-network', 'machine-learning', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39582/how-to-detect-if-a-person-is-interacting-as-in-touching-some-object-in-an-imag', 'post': ' I am currently working on a human interaction problem, which tries to identify if a person touched a predefined object.  Current Approach: I am using open-pose to estimate the pose of the person then manipulating the keypoints returned into some logic (simple if else statements) which tells whether person interacted or not. However it seems to be a dirty workaround to me and is not a full proof solution. So is there any better approach I can go for? Is there any predefined model or white paper I can refer for this problem? I am struggling on how to detect the pose of a person in an image? Is there a way to classify the pose of a person in three categories like front, back and side?  ', 'Tags': {'machine-learning', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39544/is-rl-applicable-to-environments-that-are-totally-random', 'post': \" I have a fundamental question on the applicability of reinforcement learning (RL) on a problem we are trying to solve. We are trying to use RL for inventory management - where the demand is entirely random (it probably has a pattern in real life but for now let us assume that we have been forced to treat as purely random). As I understand, RL can help learn how to play a game (say chess) or help a robot learn to walk. But all games have rules and so does the ‘cart-pole’ (of OpenAI Gym) – there are rules of ‘physics’ that govern when the cart-pole will tip and fall over. For our problem there are no rules – the environment changes randomly (demand made for the product). Is RL really applicable to such situations? If it does - then what will improve the performance? Further details: - The only two stimuli available from the ‘environment’ are the currently available level of product 'X' and the current demand 'Y' - And the ‘action’ is binary - do I order a quantity 'Q' to refill or do I not (discrete action space). - We are using DQN and an Adam optimizer. Our results are poor - I admit I have trained only for about 5,000 or 10,000 - should I let it train on for days because it is a random environment? thank you Rajesh \", 'Tags': {'reinforcement-learning', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39541/training-a-model-for-object-detection', 'post': '       Closed. This question needs to be more focused. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Update the question so it focuses on one problem only by editing this post.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         I am new to Machine Learning.  Need a little direction on how to proceed with training a model for object detection.  I have a complete training dataset and test dataset of cars with color images, annotations and bounding boxes.  This data set has 196 classes. All google search mention use of pre-trained models.  However I want to train the model on this dataset to detect and predict bounding box values.  Did not find any on StackExchange either.  Appreciate any pointers. Thanks ', 'Tags': {'training', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39528/how-i-can-interpret-the-attached-tree', 'post': ' Is the total sample size = 30.891 and the overall percentage of “yes” = 11,3%? How I can describe the leaves predicting \"yes\" outcome in term of explicative variables and values? What are the variables appearing in the second tree level?  Thank you in advance! ', 'Tags': {'machine-learning-model', 'machine-learning', 'decision-trees'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39519/do-c-4-5-or-c-5-0-perform-multi-way-split-or-binary-split', 'post': \" Suppose that I am using a continuous variable as an independent variable (although a bad choice) for C 4.5 or C 5.0 (tree based classifier). I am having a hard time figuring out does C 4.5 or C 5.0 does a binary split or a multi way split by calculating entropy? If it does a multi way split how does it decides on optimal number of splits?  If it does a binary split for continuous variable using Entropy or Gini Index, then it's alright to regard it's splitting criteria the same as of CART?  \", 'Tags': {'classification', 'machine-learning', 'r', 'decision-trees'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39504/what-does-it-mean-if-a-high-or-low-number-of-my-componenets-describe-a-percentag', 'post': ' In the following code run after PCA i can see that X number of components explain Y % of cumulative explained variance (CEV).  I would like to know 1- What percentage of the CEV is typically acceptable e.g. 95% or 99%? (Or is it a case by case basis?) 2- If 20 out of 200 components explain 95% of the CEV, what does this say about my data, what about when 200 out of 200 explain 95% ?  ', 'Tags': {'pca', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39495/data-model-and-algorithm-for-recommending-related-interests', 'post': ' On my app, when a user selects an interest (example: ios), I\\'d like to show related interests (swift, xcode, apple, etc). I have a list of around 700 interests/tags (about 300 of them can be classified as tech). How do I design the system so that when a user selects one of the 700 items, the system can show other relevant items? Note that there is no \"learning\" involved in this. I\\'d just go and classify each one myself, and this list will be mostly static.  This is how I have been thinking about it:  I could assign a list of categories to each interest. These categories go from narrow to broad. The broadest category is just \"tech\", so if a user selects \"ios\", I could right away eliminate the 400 non-tech interests. Now I can fine-tune by looking at narrower categories (example: \"mobile-apps\", so I could recommend things that are categorized as \"mobile-apps\". I could go fine-tune further by looking at a narrower category like \"ios-apps\". So when a user selects an interest, I start by searching for items with the narrowest categories, and if I need more results, and I need to search for more, then, I go broader. To get started with this, I could simply have up to three categories (narrow, medium, broad) for each interest. Once I identify some categories, I can assign them as narrow, medium, or broad to each one. Now for any given interest, I can find out what other interests share the same narrow, medium, or broad categories. The other approach I was thinking was to maintain a list of related interests for each interest. This requires more initial work on my part, so for each of the 700 interests, I\\'d find the 5-10 other interests that relate to it best, so for example, \"ios\" could have related interests like [swift, xcode, iphone, ipad], and so when a user enters ios, I just have to look up this list. I could maintain this list in a relevancy order. This approach is tedious as when I want to add a new interest to my list of 700 interests, I\\'d have to go through each and see if this new interest needs to be added as a related interest.   My question is--am I trying to reinvent the wheel? Are there any standard data models and algorithms to do this type of thing? I have been trying to Google this and the results seem to indicate things like interests graphs, cluster algorithms. I have no background in data science or machine learning. Any input is appreciated! ', 'Tags': {'machine-learning', 'deep-learning', 'clustering', 'recommender-system', 'classification'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39482/balance-data-using-different-criteria', 'post': \" I have a dataset of audio and text files that I want to balance using different criteria to train a neural network. The text and matching audio file are grouped under one ID.  For each ID, I have a number of words and some metadata. Let's say my data have three metadata/criteria that need to be balanced (well... kind of, I want to be able to choose the percentage for some criteria values) :  The name of the speaker, each speaker should have approximately the same number of words. The status of the speaker (active if the speaker is still in activity or inactive if not), where I want approximately 70 % active speakers and 30 % inactive.  The gender of the speaker, where I want approximately 50 % male and 50 % female.  Example : ID12345 with 10,000 words. Metadata : John, male and active  If I balance one of the criteria, then another, I will surely unbalance the previously balanced one. Moreover, if I balance using only one criterion at a time, I could throw away data that could be useful to the balance of the following criteria, which mean I will end up with less data that I could have had optimally. Is there an algorithm that could balance the data using all criteria with their percentage target, and maximise the number of words remaining ? I'm looking for a general algorithm, with a variable number of criteria. EDIT : I use C++ and MongoDB (but I'll accept other languages/tools if it can help me, as well as algorithms). \", 'Tags': {'data', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39472/reading-strategy-for-ml-books', 'post': \"       Closed. This question is opinion-based. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         I am trying to read the following list of books on statistical learning. I have a BSCS and about 4 yrs of experience working in image processing and parallel programming. I won’t be an expert in the field by any means, however my aim is:  not to be a script kiddie, using tools and algorithms without understanding the hows and whys. be able to read and digest the latest research in statistical learning, specially w.r.t computer vision.  Prerequisites I have studied in preparation:  Matrix Algebra by James E. Gentle Statistical and Mathematical Methods lectures by Carlos Fernandez-Granda  Books to read:  An Introduction to Statistical Learning with Applications in R by Robert Tibshirani et al. The Elements of Statistical Learning: Data Mining, Inference, and Prediction by Robert Tibshirani et al. Understanding Machine Learning From Theory To Algorithms by Shai Shalev-Shwartz et al. Pattern Recognition and Machine Learning by Christopher M. Bishops Information Theory, Inference, and Learning Algorithms by David J.C. MacKay Deep Learning by Ian Goodfellow et al. Convex Optimization by Stephen Boyd et al.  I am looking for a reading strategy. I'd specially appreciate input from users who’ve read the majority of the books. \", 'Tags': {'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39467/logistic-regression-cost-function-gives-mathematical-error-since-its-attempting', 'post': \" I am learning machine learning and after reading through materials on logistic regression i attempted to implement logistic regression with gradient descent in python from scratch. It works well for some cases but for some cases it results in mathematical error, which is understandable if we see the case below. the cost function in logistic regression is -( ylog(predicted) + (1-y)log(1-predicted)) what happens when predicted is 1? code fails because it attempts to calculate log(1-1) = log(0) which is undefined. Explicitly we get this error in python ValueError('math domain error') Please help me in understanding how can i prevent this case. Code is given below:  client code:  \", 'Tags': {'logistic-regression', 'machine-learning', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39465/it-is-possible-to-use-features-maps-of-cnn-to-localised-important-areas-in-image', 'post': \" I'm new in deep learning and CNN, I understand how convolutional and pooling layers work, I understand how and why feature maps are created.  How I can localize from the feature maps important area in the original image? is that possible?  I.e. I use my own  data  ( medical image )  Feeding those images to my neural network I can receive maps features and calculate from them the probability of each class.  But how from that feature maps I can get know where exactly those areas are?  how can I backtrack the weights?  any idea?  \", 'Tags': {'computer-vision', 'machine-learning', 'image-classification', 'deep-learning', 'cnn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39456/machine-learning-algorithm-which-gives-multiple-outputs-from-single-input', 'post': \" I need some help, i am working on a problem where i have the OCR of an image of an invoice and i want to extract certain data from it like invoice number, amount, date etc which is all present within the OCR. I tried with the classification model where i was individually passing each sentence from the OCR to the model and to predict it the invoice number or date or anything else, but this approach takes a lot of time and i don't think this is the right approach. So, i was thinking whether there is an algorithm where i can have an input string and have outputs mapped from that string like, invoice number, date and amount are present within the string. E.g: Inp string: the invoice #1234 is due on 12 oct 2018 with amount of 287 Output: Invoice Number: #1234, Date: 12 oct 2018, Amount 287 So, my question is, is there an algorithm which i can train on several invoices and then make predictions? \", 'Tags': {'ocr', 'machine-learning', 'python', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39451/predicting-a-cyclic-target', 'post': \" I'm familiar with using trigonometric functions to transform cyclic variables for use as features in training a model (most commonly hour of the day or month of the year); I'm now trying to figure out the best way of doing this for using these types of variables as the target for my model. (Imagine a model predicting the month when a particular event is most likely to occur, for instance). Neither using a strictly-increasing representation (so that January, 2018 is close to December, 2017 but very far from January, 2017) nor treating the month as a categorical variable is ideal, but the trigonometric encoding done for features requires both  and  parts to have a unique representation of each month, so using one of the two isn't a workable approach either.  Is there a better option that I'm missing? \", 'Tags': {'feature-engineering', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39448/different-types-of-analysis-which-can-be-done-with-webdata-of-users', 'post': ' I have rich customer data for site usage, particularly the web analytics data. Some of the data fields I have are:  Product Usage details Geographic details Session data: Capture what all they did during a session onsite, unique to timestamp User details Device used to login Product looked at/ downloaded Campaign data for promotion of a product Search queries made on the on-site search bar Results of those search queries  and so on. I wanted to know what are the different types of analysis I can perform using this data, more inclined towards the Data science aspect of it. I have already thought of the following:  Churn Analysis Campaign effectiveness Find out which product is being sold the most and which at which geography User behavior analysis and such.  Could the community help me come up with more ideas which i can use to perform analysis on this data? Thanks ', 'Tags': {'data-analysis', 'data', 'machine-learning', 'clustering'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39422/are-there-any-meta-knowledge-bank-available', 'post': ' What resources do you use to learn meta knowledge ? By meta knowledge, I mean generalized information that will help us take more informed decisions when working on a problem later. Example of meta knowledge:  Lots of time series data ? Build a CNN Limited in time and want to get quick insights from a dataset ? Try Random forests. Continuous data and supervised learning ? Do a regression.  I thought people would have build quiz-like applications to help determine good algorithm candidates to test depending on a set of known conditions. I found \"Algorithm selection\" on google and the Algorithm selection literature summary, but this doesn\\'t summarize and dumb down the information for the larger public. The goal is for me to learn useful information that\\'ll allow me to pick algorithms and study them in details only when I really need them, instead of randomly trying many algorithms in hope one will give me better results. ', 'Tags': {'algorithms', 'meta-learning', 'machine-learning', 'model-selection'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39408/what-are-the-2-classes-of-categories-to-help-define-a-problem', 'post': \" While studying about machine learning, I've learnt the importance of defining your problem before getting started trying to model it. I can see 2 types of problem categorification:  Supervised / unsupervised / reinforcement algorithms Classification / clustering / regression / ranking  Example definitions found on the net: First type:  Supervised Algorithms: The training data set has inputs as well as the desired output. During the training session, the model will adjust its variables to map inputs to the corresponding output. Unsupervised Algorithms: In this category, there is not a target outcome. The algorithms will cluster the data set for different groups. Reinforcement Algorithms: These algorithms are trained on taking decisions. Therefore based on those decisions, the algorithm will train itself based on the success/error of output. Eventually by experience algorithm will able to give good predictions.  Second type:  Classification: You want an algorithm to answer binary yes-or-no questions (cats or dogs, good or bad, sheep or goats, you get the idea) or you want to make a multiclass classification (grass, trees, or bushes; cats, dogs, or birds etc.) You also need the right answers labeled, so an algorithm can learn from them. Clustering: You want an algorithm to find the rules of classification and the number of classes. The main difference from classification tasks is that you don’t actually know what the groups and the principles of their division are. For instance, this usually happens when you need to segment your customers and tailor a specific approach to each segment depending on its qualities. Regression: You want an algorithm to yield some numeric value. For example, if you spend too much time coming up with the right price for your product since it depends on many factors, regression algorithms can aid in estimating this value. Ranking: Some machine learning algorithms just rank objects by a number of features. Ranking is actively used to recommend movies in video streaming services or show the products that a customer might purchase with a high probability based on his or her previous search and purchase activities.  Do each type of categories have a name ?  And are these types correlated or independent ? \", 'Tags': {'algorithms', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39404/feature-engineering-decrease-my-cross-validation', 'post': \" I'm currently working on a fraud detection data set. I'm evaluating my training data with a 10-skfold roc auc and an estimator of default param LightGBM. But, the problem is every time I try to create a new column by calculating a ratio of 2 column, my CV always drop even though the new column is theoretically will highlight a difference between a fraud or not. The other problem is when I find 2 good (increasing CV) separate new feature by feature engineering when evaluating them independently, my CV actually decrease when I combine both of the feature on the same evaluation. Is there anything wrong with my way of feature engineering? Right now my step is: 1. Create a new column based on other column 2. Evaluate CV with 10-skfold default param 3. If the CV is increased (relative to original data) then it's good feature else no. Any help is appreciated. \", 'Tags': {'feature-engineering', 'dataset', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39376/why-do-we-have-to-remove-most-common-words-for-text-analysis', 'post': ' I am trying to do sentiment analysis the task is to classify racist tweets from other tweets. And I have read many articles and many have mentioned to remove the most common 10 words from the column because their presence will not of any use in classification of our text data. So these are my top 10 most common words on my dataset.  If I remove these will my classification model be more accurate? Similarly they are also recommending to remove the top 10 rare words from the column. I want to know why? Any help ', 'Tags': {'sentiment-analysis', 'machine-learning', 'python', 'nltk'}}\n",
            "page number ------------------------------------87\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39365/prediction-interval-for-very-small-dataset', 'post': \" Does it make sense to calculate prediction interval for very small dataset (about 60 samples)? I know it's easy for linear regression. But it seems like for tree-based methods or non-linear methods it doesn't make any sense. Am I wrong?  \", 'Tags': {'dataset', 'machine-learning', 'decision-trees'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39357/clustering-customer-data', 'post': \" I dont know if this kind of question is allowed but i kinda hit a wall. I know about some clustering algorithms. I already implemented Fuzzy C-Means and Gaussian Mixture Model, but I dont really know what's the efficient way to cluster customer data and there is no label at all.  Since it's company data I can't say the detail, but if this helps here is the columns of the data:   First of all, I group by using panda's for each customer data so there's no duplicate. And then i just soft cluster to 15 clusters. Why 15 ? I tried to cluster it with number of product categories as number of clusters (since this what my supervisor asked me unless i can propose better method). Is this the right way to do it ? or there is some papers that explain better methods?  what I make will be used for real marketing on e-commerce so I'm scared if my method screws the company up or something.  \", 'Tags': {'machine-learning', 'clustering'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39351/check-overfitting-in-cnn', 'post': ' I am kind of new to NLP and text classification with Convolutional Neural Nets, and I have trained my first models quite recently. I am a little bit concerned with overfitting. I am doing multilabel classification, so my output is a list of 9 numbers (one per label) containing probabilities. I have to set a threshold to the output to get a list of 0s and 1s and assign labels to sentences. When I train the models, I draw a couple of plots to check if there is overfitting or not.  I draw the variation of the F1 score, precision and recall depending on the threshold I set on the output from the model. I see that if I try the trained model with the training data the plots show a perfect F1 score, precision and recall but it shows something much poorer with the test data. In the plots, the orange line corresponds to the training data and the blue one to the test data. Shouldn’t both curves be closer? It seems to me that the training data is perfectly fit whereas the test data is further behind. Is this a good way of checking overfitting? Or is there any other way of checking it? ', 'Tags': {'neural-network', 'overfitting', 'machine-learning', 'convnet'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39349/string-values-using-python-with-pandas', 'post': '  above data and code i just want to know whether i can assign multiple values r not(for example, i want assign to cancer new values like , , ) if it possible can any suggest me code r condition to be applied ', 'Tags': {'machine-learning', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39336/dataset-with-features-as-snowmed-codes-and-outputs-as-names-of-medications-presc', 'post': ' So, i am constructing a dataset (from doctor prescription notes and diagnosis) where  X (input features) represent the various snowmed codes for illnesses/surgeries performed/chronic conditions etc.  Y is a set of pills prescribed for X.  Presently I do not have this data in a matrix format. For ex. i have the snowmed codes for an illness that reads as follows: \"Diabetes, Hypertension, Cardiac arrest, Left MCA stroke\". Y is the set of pills that the doctor noted down for the above.  As one can note, the above data is not in a matrix format. There are many snowmed codes and various different pills overall. I\\'d like to apply ML algorithms to predict the pills for a given set of snowmed codes about the illness.  How to organize this data in a format convenient for ML? I am open to other analytics approaches as well. Any discussion in this matter will be helpful.  ', 'Tags': {'data', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39331/right-regression-model-to-use', 'post': ' I am trying to predict reservation count from a dataset with few features. Features are both categorical and continuous.  The dependent variable reservations looks like below: My dataset size is around 917 obs.   When I plot the histogram of dependent variable I get this   So I used a log transform to remove some of the skewness.  as  Now the plot of distribution looks below:   Some of features.   Since actual_price and Recommended_price have huge correlation, I created a difference price of these two and dropped actual_price and recommended price.  But after running Linear regression or Random Forest Regression I get very poor results with R2 as 0.12 for both.  This shows the model is clearly not predicting and fitting well.  My dependent variable is clearly a positive variable. Is Linear Regression still right? Should I use Poisson regression? Log transformation makes sense? EDIT:  Tried Poisson from Statsmodels. Gives worse results  ', 'Tags': {'statistics', 'machine-learning', 'regression'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39327/exploratory-data-analysis', 'post': ' I am working on this dataset. Dataset has missing values. What would be the best method to impute the missing values. Also values are missing from target feature as well. So far I have dropped those observations from the dataset. There are multiple instances of the same ID for which some variables will not change i.e. age, weight, height etc., but some values are missing. I am trying to impute the missing values by other given values for the same ID. How can I go about it in Python? Thanks in advance. ', 'Tags': {'machine-learning', 'predictive-modeling', 'data-mining', 'data-analysis', 'data-science-model'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39320/in-supervised-learning-how-to-get-info-from-correlation', 'post': \"       This question already has an answer here:\\r                         \\r                         How to get correlation between two categorical variable and a categorical variable and continuous variable? \\r                                 (1 answer)\\r                               Closed 2 years ago.    I am trying to build a classification model so I have tried to check the correlation between the features.  Here Loan_Status is my target variable.  I just don't know how to extract information from this? Please help. I have questions like. Is -0.0047 corelation of ApplicantIncome with Loan Status useful? \", 'Tags': {'machine-learning', 'python', 'correlation'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39317/difference-between-ordinalencoder-and-labelencoder', 'post': ' I was going through the official documentation of scikit-learn learn after going through a book on ML and came across the following thing: In the Documentation it is given about  whereas in the book it was given about , when I checked their functionality it looked same to me. Can Someone please tell me the difference between the two please? ', 'Tags': {'machine-learning', 'encoding', 'python', 'scikit-learn', 'preprocessing'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39307/collaborative-filtering-with-human-adjusted-latent-factors', 'post': ' Having tried some of movie recommendation engines available on the web I have the feeling they are not satisfactory. I just fail to get movies similar to those I like based on traits interested for me personally. My guess is that the lack of precision can be overcome with extending the number of latent factors in CF SVD algorithm. Another idea is that latent space can be initially hand-crafted.  E.g. we can take 50 well-studied movies and asses them by 200 traits manually (partially fill item-latent factors matrix). Accept this values as fixed. Then having some of the user ratings we can perform matrix factorization and inherit user-factors and utility matrices as usual, while keeping fixed values. Any present work on this? ', 'Tags': {'machine-learning', 'recommender-system'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39290/how-to-build-classifier-if-only-1-image-per-class-available', 'post': ' If I have only 1 image for each of 10 classes, what is the best way to build an image classifier? The images themselves are large (1200x1600) and of good quality. For example:   Or similar images from https://www.pexels.com/search/flowers/ From what I have read, neural networks need large amount of data for training. Which other machine learning methods can be used in such a situation? Basically, image similarity needs to be assessed. Can support vector machines be used here?   ', 'Tags': {'neural-network', 'machine-learning', 'image-classification'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39269/which-ml-method-would-be-best-for-deriving-a-rough-formula-for-prediction-based', 'post': ' Which ML method would you say is the easiest to derive a mathematical formula from based on already existing data of predictor stats and outcome? I have this data: Opponent 1:  Strength: x Battle Score: y  I also have a model that I put against the opponent: Opponent 2:  Strength: z Battle Score: k  Finally, all outcomes of fights are written into a database (which currently has around 2800 outcomes) and look something like this model: Fight:  Strength: x - z Battle Score: y - k Outcome: win/lose  I would want to get proper weights for Strength and Battle Score, so I can derive a simple formula from it and thus somewhat predict whether the next fight will be won or lost. ', 'Tags': {'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39266/how-to-analyze-cnn-model-summary-and-improve-it', 'post': ' I am using a CNN (adapted from a few links on the net) for an image classification task. There are about 8000 images of size 128x128 each. They are of 13 different classes. Following is output of ):   How does one analyze this model summary and how can this model be improved? ', 'Tags': {'neural-network', 'machine-learning', 'image-classification', 'deep-learning', 'cnn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39264/how-does-sigmoid-activation-work-in-multi-class-classification-problems', 'post': \" I know that for a problem with multiple classes we usually use softmax, but can we also use sigmoid? I have tried to implement digit classification with sigmoid at the output layer, it works. What I don't understand is how does it work? \", 'Tags': {'neural-network', 'machine-learning', 'multiclass-classification', 'deep-learning', 'activation-function'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39261/how-to-fetch-text-from-pdf-to-further-proceed-with-question-answer-based-model-f', 'post': ' To illustrate the above title. Suppose you have a pdf document, which is basically scanned from hardcopy, now there are set of fixed questions to answer from the document itself.  For an example a document contains a contract of land, now the set of fixed questions be \"who is the seller?\" \"what is price of the asset? \", document has referred to this answers probably 2-3 times, as a human it\\'s a simple task. How to automate this? ', 'Tags': {'computer-vision', 'machine-learning', 'nlp', 'deep-learning', 'cnn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39260/hands-on-machine-learning-with-scikit-learn-and-tensorflow-confusion-matrix-with', 'post': '       Closed. This question needs details or clarity. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Add details and clarify the problem by editing this post.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         I followed the steps EXACTLY in the Hands on Machine Learning with Scikit Learn and TensorFlow ch. 3. But the confusion matrix for the multinomial classifier is very very bad. Even though the book expects the matrix score to be very good. The matrix looks like this:   But the matrix should ACTUALLY look like this:   Here is the link to the notebook: https://colab.research.google.com/drive/1x1vhgajdDYovFop59MoT5asQ9qyyP1ka My question is, what mistake did I make in my steps? And should I adapt my program to future updates in scikit-learn, numpy, and pandas? (The book is stated as outdated in some parts) ', 'Tags': {'accuracy', 'confusion-matrix', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39246/why-gradient-methods-work-in-finding-the-parameters-in-neural-networks', 'post': ' After reading quite a lot of papers (20-30 or so), I feel that I am quite not understanding things. Let us focus on the supervised learnings (for example).  Given a set of data  and  where we assume  are unknown,  the goal is to find a function   To do this, we need a model for . Typically, neural networks are frequently employed. Thus we have   where .  Then  is a neural network of  hidden layers.  In order to find , typically, one define a loss function .  One popular choice is   In order to find  which minimizes the loss function , a typical (or it seems the only approach) is to apply the gradient method. As far as I know, the gradient method does not guarantee the convergence to the minimizer. However, it seems that a lot of research papers simply mention something like   We apply the standard gradient method (e.g., Adam, Adadelta, Adagrad, etc.) to find the parameters.   It seems that we don\\'t know those methods can return the minimizer. This makes me think that it could be possible that all the papers rely on this argument (utilizing the parameters found by gradient methods) might be wrong. Typically, their justifications are heavily on their examples saying it works well. In addition to that, sometimes, they mentioned that they tuned some parameters to run gradient methods. What does that mean ``tune\"? The gradient method high depends on the initialization of the parameter . If the initial choice were already close enought to the minimizer, i.e., , it is not surprising that it works well. But it seems that a lot of trials and errors are necessary to find a proper (good and working well) initialization. It sounds to me that they already found the good solution via trials and errors, not gradient methods. Thus tuning sounds to me that they already found a parameter which already closes to . I start to think that there may be something I am not aware of as the volume of such researches is huge. Did I miss something? Or can we just do research in this manner? I am so confused... I am not trying to attack or critize a specific paper or research. I am trying to understand. Any comments/answers will be very appreciated.  ', 'Tags': {'neural-network', 'loss-function', 'machine-learning', 'gradient-descent', 'reference-request'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39245/to-which-category-does-this-algorithm-belongs', 'post': ' I have came across the Catboost package. Among the classes in categories in Sklearn, Catboost seems to belong to Ensemble methods.  What are then the advantages of Catboost over AdaBoost, Bagging etc.? ', 'Tags': {'algorithms', 'boosting', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39235/what-kind-of-algorithm-should-i-use-to-build-ml-model-that-can-predict-just-next', 'post': \" I'm quite new to machine learning and statistics.  I've a dataset from some ecommerce sale's history. It's almost 2k instances, and features include personId (string), productCategory (string/discrete), amountPaid (float/continuous), purchaseTime (string/Time(DD/MM/YYYY)). Person can purchase product at any time (irregular time interval so I can't use time series analysis, I guess). I want to know when will the same person (attr with person Id) make just next purchase in a category (attr with productCategory). What ML model should I use for this problem? Should I use Time Series forcasting or Survival Analysis or maybe some other function? How should I approach this problem? \", 'Tags': {'survival-analysis', 'machine-learning', 'classification', 'time-series', 'forecasting'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39232/how-to-make-multiple-regression-perform-better-for-outliers-without-reducing-e', 'post': \" I have a small dataset(about 60 samples) and I need it to predict well for high target values. There are only a few high values and all models I tried perform poorly for these high values.  So I wonder what technics exist to make algorithm perform better for high values that can't be dismissed as outliers. You see, these few high values make MSE very large, because the models tends to underestimate these high values, predicting them to be 2 or more times smaller. I have an idea to generate fake data for outliers, but I haven't found how to do it the right way for regression. Is it right to generate fake data for high values based on their proximity by mixing their features? So it would be smth like SMOTE, but instead of classes we have nearby values? Or, perhaps, the other idea is to cluster targets by density and then to generate balanced clusters by generating fake data with SMOTE or ADASYN? P.S. Note that I don't want to reduce effect of outliers. On the opposite, these high values are extremely important, so I want model to perform well for them. \", 'Tags': {'machine-learning', 'regression', 'smote', 'outlier', 'statistics'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39230/how-to-visualize-results-errors-of-multilabel-classifiers', 'post': ' For multiclass classification you would normally choose a confusion matrix to plot the error of predicted classes against the target classes. What is the best way to visualize errors of multilabel classifiers? As multiple classes are predicted at once a mapping of prediction against target is not always possible so confusion matrices are generelly not suitable. My first idea is to plot a bar chart where each class has a bar for missed and one for false predictions. But is there any standard for visualizing the errors which includes more information just like a confusion matrix for multiclass problems? ', 'Tags': {'machine-learning', 'python', 'visualization', 'multilabel-classification', 'classification'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39199/why-does-id3-decision-tree-algorithm-not-give-the-best-decision-tree', 'post': ' I was going through ID3 algorithm, and what I believe is it incorporates Greedy Search rule to get come up with the decision tree. If it gives the best split possible at every stage, how does it not give the best decision tree possible? ', 'Tags': {'supervised-learning', 'machine-learning', 'decision-trees'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39178/detect-non-digitized-university-degrees-authenticity-by-deep-learning', 'post': \"       Closed. This question needs to be more focused. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Update the question so it focuses on one problem only by editing this post.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         How can we detect by the look and feel of non-digitized university certificates’ authenticity through machine learning? I am working in this project right now, where I have to detect the authenticity of non-digitized university certificate, I know how can we do it with a digital certificate. The main problem here is that I don't have the dataset. I am thinking to follow the following approaches:  Extract the embossed seal on degree certificate and compare with the real one. Extract the signature and feed it into forgery detection neural network. Watermark detection.  Anyone can suggest me any other approach then that would be very helpful. TIA.  \", 'Tags': {'cnn', 'machine-learning', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39170/difference-between-gradient-descent-and-normal-equation-in-linear-regression', 'post': ' Hi I am new to Linear Regression. I want to know  what is the difference b/w Gradient Descent and Mean Square Error in   Linear Regression using machine learning?  And  When to use Gradient Descent and Mean Square Error in Linear   Regression using machine learning?   Or  When to use which algorithm in Linear Regression.?  Can anyone explain.? ', 'Tags': {'linear-regression', 'gradient-descent', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39158/which-are-the-latest-optimization-techniques-in-artificial-intelligence', 'post': ' My project work is optimization in power system using artificial intelligence (like fault location and classification,load forecasting and context awareness and IoT etc ) and I have used PSO (particle swarm optimization, supervised fine tuning strategy etc). Please can anyone suggest a new technique which will give best result in comparison to other techniques? ', 'Tags': {'optimization', 'machine-learning', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39157/date-and-time-format-in-r', 'post': ' I have date and time in one column of the form  2004/12/15 9:30 , how can I format it  to date, so that I can plot it on x axis. Also posxlt is not working for me.  ', 'Tags': {'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39155/relationship-between-train-and-test-error', 'post': \" I have some specific questions for which I could not extract answers from books. Therefore, I ask for help here and shall be extremely grateful for an intuitive explanation if possible.   In general, neural networks have a bias/variance tradeoff and thus we need to have a regularizer. Higher bias --> underfitting; Higher Variance--->overfitting.  To solve overfitting, we use regularization for contraining the weight. This is a hyperparameter and should be learned during training based on my understanding using cross-validation. Thus, the dataset is split into a train, validation and test set. The test set is independent and is unseen by the model during learning, but we have the labels available for it. We usually report the statistics such as false positives, confusion matrix, misclassification based on this test set. Q1) Is this bias/variance problem encountered in other algorithms such as SVM, LSTM etc as well?  In convolutional neural network (Matlab toolbox) I have not seen any option for specifying the regularization constant. So, does this mean that CNN's don't need a regularizer? Q2) What is the condition if training error and test error are both zero? Is this the ideal best situation? Q3) What is the condition if training error > test error? Q4) What is the condition if training error > validation error? Please correct me where wrong. Thank you very much. \", 'Tags': {'variance', 'machine-learning', 'svm', 'bias', 'cnn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39138/conditional-probabilities-on-store-data', 'post': ' I have data on store level purchases, Panel-level purchases and demographic information of loyalty cards. In the store purchases information, the data consists of a product code which can be assigned to brands/products (Pepsi, Coke, Budweiser etc.) for many different products. Along with how many items of that product was sold that week and the price of each product. In the Panel information, I have data on the product code, the number of items purchased in each store by that Panel ID and total number of $ spent on that product in that store.  In the demographic information I have data on each Panel ID:- how many people live in the household, panel_ID income, number of children, education etc. I want to apply conditional probabilities on the product information, such as given that panel_ID bought product  what is the probability that they will buy product . However for just beer data I have 250 different brands and 300,000+ Panel_ID purchases. I am thinking of applying Naive Bayes as a starting point, however what other Machine Learning methods would you suggest for such a problem? ', 'Tags': {'classification', 'naive-bayes-classifier', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39137/how-can-i-check-the-correlation-between-features-and-target-variable', 'post': \" I am trying to build a  model and I am looking for a way to check whether there's any correlation between features and target variables? This is my sample   I am trying to predict  column based on the features available above. I just want to see if there's a correlation between the features and target variable. I tried ,  and I'm hardly getting a accuracy of around .  Any suggestions on algorithms, params etc that I should use for better prediction?  \", 'Tags': {'linear-regression', 'scikit-learn', 'machine-learning', 'regression'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39136/calculate-number-of-parameters-for-convlstm2d-layer', 'post': ' time_distributed_24 (TimeDis (None, 16, 64, 64, 512)   0           conv_lst_m2d_2 (ConvLSTM2D)  (None, 16, 64, 64, 128)   2949632     time_distributed_25 (TimeDis (None, 16, 64, 64, 128)   512         time_distributed_26 (TimeDis (None, 16, 128, 128, 128) 0          For example, why does this ConvLSTM2D layer have 2949632 parameters? A standard LSTM layer has 4(n*m + n^2 + n) parameters where m = input dim, n = output dim. A standard 3x3 conv layer with 128 kernels over a 64x64x512 tensor would have 3x3x512x128 parameters What would be the n and m for the LSTM part?  ', 'Tags': {'neural-network', 'machine-learning', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39128/should-i-transform-a-multiple-regression-with-outliers-into-ordinal-regression', 'post': \" I have small dataset of about 60 samples that performs poorly in regression. So I wonder how can I transform this task into predicting intervals instead of values. Is it possible to make it perform better for intervals instead of values? The dataset is imbalanced. There are just few high values. The algorithm constantly underestimates these values. So I think if I make prediction in intervals, it'll perform better for values, say, more than 50. But, overall, the problem is predicting for values near the limits of intervals. Is it better to mark this values as belonging to both intervals? \", 'Tags': {'statistics', 'machine-learning', 'regression'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39124/hidden-layer-weights-calculation', 'post': \" I've been working on neural network for a while and I built simple network from scratch with python but before using TensorFlow, I would like to have a complete understanding of it. Here is my question : Lets say you have 3 layers you have 3 weights to update : 1) --> the weight between the outputlayer and the hiddenLayer2 2) -->  the weight between the hiddenLayer2 and the hiddenLayer1 3) -->  the weight between the hiddenLayer1 and the inputLayer For the 1) the calculation is quite simple we got :  For the 2) the calculation is more complicated  and we got :  I need help for the 3rd part, I tried to calculate and find on internet but not a lot of people uses 2 hidden layer when they work from scratch.   I also tried to resolve the chain rule but its too long and I can't resolve. Does someone know the formula to get the weight between the hiddenLayer1 and the inputLayer ? Thank you so much in advance \", 'Tags': {'neural-network', 'machine-learning', 'deep-learning', 'python', 'numpy'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39107/how-to-interpret-sum-of-squared-error', 'post': ' I am working on ANN. I have 2497 training examples and each of them is a vector of 128. So the input size is 128. Number of neurons in hidden layer is 64 and number of output neurons is 6 (since classes are six). My Target vector looks something like this : [0 1 0 0 0 0]. This means that the example belongs to class 2. I have used sigmoid as an activation at all layers and sum of squared error is loss. SSE is computed over one epoch. Total epochs are 10k. my loss starts from around 700 and reduces to 450. Should I say that loss is 18% per example since 450 is the loss for all the 2497 examples. How do I interpret this. Is my model good enough? I know that I should test it on unseen data to be sure of its accuracy but still does this tell anything about the performance at all or not. ps: I am implementing it in C ', 'Tags': {'neural-network', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39100/equivalent-of-numeric-encoding-when-rows-can-contain-multiple-values', 'post': \" If we have a column like:  then, after numeric encoding, it becomes:  What if, however, we have a column like this:  One way to encode it would be like this:  However, this creates a lot of extra columns. Is there a way to encode such a column in a way that doesn't end up with loads of extra columns, just like using numeric encoding instead of one-hot-encoding prevents too many columns from appearing in a column like the first one I showed? If you're using Python, here's some code to reproduce my DataFrame:  Edit: the ultimate purpose of this is to then pass this through a tree-based classifier. \", 'Tags': {'feature-engineering', 'machine-learning', 'encoding'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39099/what-is-an-intuitive-explanation-for-the-importance-weighted-autoencoder', 'post': \" I have been reading a paper by Burda et al. on Importance Weighted Autoencoders(IWAE) but I can't quite grasp what they mean by sampling the terms h1...hk. Do they mean you have separate models from which you sample and then average over? \", 'Tags': {'autoencoder', 'generative-models', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39087/using-by-statement-in-proc-esm', 'post': ' I was using proc een in SAS, for time series forecasting. I want to include two variables in  statement. My data contains four columns:  Shop_id Item_id Item_price Date  The item_price column is the one which should be forecasted. The date is in months. I want to include both shop_id and item_id in the  statement Is it possible ?  ', 'Tags': {'time-series', 'sas', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39085/structure-of-a-multilayered-lstm-neural-network', 'post': \" I implemented a LSTM neural network model in Keras. However, how the codes worked under the hood was not quite clear. I want to know if it worked the way I guessed how it worked? For example: Say there's a 2-layer LSTM network with 10 units in each layer. The inputs are a sequence data . So when the inputs are entered into the network,  will be thrown into the network first and be connected to every unit in the first layer. And it will generate 10 hidden states/10 memory cell values/10 outputs. Then the 10 hidden states, 10 memory cell values and  will be connected to the 10 units again, and generate another 10 hidden states/10 memory cell values/10 outputs and so on? After all 5 's are entered into the network, the 10 outputs from  from the first layer are then used as the inputs for the second layer. The other outputs from  to  are not used. And the the 10 outputs will be entered into the second layer one by one again. So the first from the 10 will be connected to every unit in the second layer and generate 10 hidden states/10 memory cell values/10 outputs. The 10 memory cell values/10 hidden states and then the second value from the 10 will be connected and so forth? After all these are done, only the final 10 outputs from the layer 2 will be used? Is this how the LSTM network works? Thanks! \", 'Tags': {'lstm', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39077/creating-a-single-number-from-a-numpy-array-python', 'post': '       Closed. This question needs details or clarity. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Add details and clarify the problem by editing this post.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         I am working on a gender classification project. I am extracting the pixels of an image using a Numpy array in Python, similar to the one below:  How can I use these values to extract a single meaningful number to be used in a csv file for K-Nearest Neighbor Algorithm. For example single number like   In summary, how can I turn  into a single number like  ', 'Tags': {'machine-learning', 'dataset', 'python', 'csv', 'k-nn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39064/why-the-vc-dimension-to-this-linear-hypothesis-equal-to-3', 'post': ' I am trying hard to understand this. Here is the scenario:  I need to calculate the VC dimension for the above linear separator. Somehow, the VC dimension for this linear separator is 3. I just cannot understand how. According to what I understand, VC dimension is the size of the largest finite subset of  which can be shattered by . So if there exists a subset of size  where all points are shattered by  and there exists a subset of size  where at least one point is not shattered by , then the VC dimension will be .  Is the VC dimension >= 1?   Yes. We just need to pretend there is a single point on the line and by keeping the line (X-axis) steady we can flip which side is positive/negative   Is the VC dimension >= 2?   Yes, because we could separate all 4 combinations { ++, --, +-, -+ } using a single line   Is the VC dimension >= 3?   This should be NO, according to what I understand. How could we separate this case ?  But I was going through a video, that explains VC dimensions which says it is three and I do not understand how.  ', 'Tags': {'vc-theory', 'model-selection', 'machine-learning', 'machine-learning-model'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39060/does-min-max-algorithm-leads-to-information-loss', 'post': \" Simplifying the question with an example. Lets say I have a time series data and the variables are time, temperature, growth_of_microbe. Now I have observed usually with a higher high of temperature there is an effect on growth of microbe. Sample of the data is as follows   With respect to the above table, Min-Max uses the lowest/highest data  within the total available time rather than the lowest/highest till the time the value is calculated  Doesn't this lead to information loss? \", 'Tags': {'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39058/dealing-with-nan-missing-values-for-logistic-regression-best-practices', 'post': \" I am working with a data-set of patient information and trying to calculate the Propensity Score from the data using MATLAB. After removing features with many missing values, I am still left with several missing (NaN) values. I get errors due to these missing values, as the values of my cost-function and gradient vector become NaN, when I try to perform logistic regression using the following Matlab code (from Andrew Ng's Coursera Machine Learning class) :  Note: sigmoid and costfunction are working functions I created for overall ease of use. The calculations can be performed smoothly if I replace all NaN values with 1 or 0. However I am not sure if that is the best way to deal with this issue, and I was also wondering what replacement value I should pick (in general) to get the best results for performing logistic regression with missing data. Are there any benefits/drawbacks to using a particular number (0 or 1 or something else) for replacing the said missing values in my data? Note: I have also normalized all feature values to be in the range of 0-1. Any insight on this issue will be highly appreciated. Thank you \", 'Tags': {'logistic-regression', 'machine-learning', 'data-cleaning', 'missing-data'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39039/model-joint-probability-of-n-words-appearing-together-in-a-sentence', 'post': ' Assume that we have a large corpus of texts to train with. Given N words as input, I want to model the joint probability  of these words appearing together in a sentence. More specifically, the N words are not required to be ordered or contiguous, and words other than given words can appear in the sentence. There is no restriction on the number of times each of N words can appear in the sentence. I did some research and below are some possible directions.   Popular RNN models like LSTM offer conditional probability , which may shed light on the joint probability I am modeling. However, RNN models take sequential inputs and require the N given words to be ordered. Also, if I am to use RNN models, I need to use Markov chain rule to calculate the joint probability, the N words would need to be continuous, and no words other than given words would be allowed.   Topic models like LSA(Latent Semantic Analysis) or LDA(Latent Dirichlet Allocation) may help me find topics from corpus, and model joint probability based on topics that each of N words belong to. Words from same topic are assigned higher joint probability. These models require me to manually set the number of topics. As I am not really familiar with these models, I am not sure how good they will perform.  A neural network alternative to LDA is a Restricted Boltzmann Machine, where topics are learnt as hidden neurons. The input to this RBM would be a vector the size of my whole dictionary. Each entry is either 1(present), 0(not present), or -1(don’t know). During training the probability  is learnt, where  is my learnt topics and  are training data. At test time, the test input  will have all N given words’ value equal to 1, while all other words equal to -1.  In this way we can learn the probability .   I feel like this is a really fundamental problem in language modeling, but have not yet been able to find recent research on this. Could anyone please give any pointers on this problem? Does any of the above proposed solutions look feasible? Any help is appreciated!  ', 'Tags': {'rbm', 'machine-learning', 'nlp', 'recurrent-neural-net', 'lda'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39034/how-to-train-ml-model-with-multiple-variables', 'post': \" I am trying to learn Machine Learning concepts these days. I understand in a traditional ML data, we will have features and labels. I have following toy data in my mind where I have features like 'units_sold' and 'num_employees' and a label of 'cost_$'. I would like to train the model to learn these features and label for a particular 'city' and 'store'. For example if I perform Linear Regression, the model learn intercept and coefficient for the city and store it relates to. When I input units_sold and num_employees for next year, I get the prediction.  I'm trying to brainstorm about it and would like to know how to approach this problem? \", 'Tags': {'scikit-learn', 'machine-learning', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39025/beginner-question-related-to-data-science-course', 'post': '       Closed. This question needs to be more focused. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Update the question so it focuses on one problem only by editing this post.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         I am Third Year B.Tech student from 3-tier college of India ,Here is no one fellow or collegous who has somebit knowledge about ML or Data science and I am purely sure that I have strong background of math. What will be my first move to be conceptual in data science ? Because my plan to do one project based on data science concept in my next semester and carry the same stream as career ! ', 'Tags': {'machine-learning', 'feature-engineering', 'data', 'python', 'statistics'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/39002/perform-pearsons-correlation-and-chi-squared-test-for-feature-selection-in-a-da', 'post': \" I've a dataset of about 200 features and 5000 instances. These features comprise of different data types like percent (string like 4.50%), dollar amount (value between 1,000,000), discrete distribution as string (colors like red, blue), etc.  There are lots of different format in the data. I can't use a regex pattern of a feature in another feature. So how can I convert these features into float (for correlation feature selection), or into some categories (for chi-squared test and one way ANOVA test)? \", 'Tags': {'statistics', 'feature-selection', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38992/cross-entropy-loss-increase-but-precision-get-better', 'post': ' i am working on classification model. my test result shows precision is getting better despite loss is increase.   is it just the nature of my data or is there some kind of theoretical explanation? ', 'Tags': {'neural-network', 'classification', 'machine-learning', 'data-mining'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38991/detecting-anomaly-from-cdr-data', 'post': '  I need to find anomalies from this. The term anomaly in my case is in which square id and in which time the total activity goes higher, how can I detect anomalies with the help of machine learning algorithms. ', 'Tags': {'machine-learning', 'r', 'dataset', 'ipython', 'data-science-model'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38989/validating-performance-of-panel-data-based-models', 'post': \" I'm wondering from a theoretical/general practice perspective, what is the best way to evaluate performance of regression models derived from panel data (i.e. a time series of cross sectional data). Specifically, I'm curious whether the final accuracy measure used (e.g. explained variance) should be an average of such measure for individual cross sections (perhaps to accounting for distribution of the accuracy measure itself) or of the full data sample? To give an example, when dealing with factor models in quantitative finance, you have cross sections of variable number of stocks with multiple attributes, which you use to forecast the return of the stock for some lagged period. To evaluate the in- and out- sample performance, I have seen both methods applied, for example:  IC Score (cross-sectional correlation averaged over the sample period) R2/Explained Variance on the full sample  For fixed-effect models (e.g. Linear Regression), I understand the logic behind using the full sample, but I also find that it masks the presence of shocks/reversals in the system that may be occurring in specific periods that is unaccounted for in the model? Also, when it come to tuning parameters for learning algos, should I adjust generic evaluations when it comes to cross-validation so that cross-sections are evaluated together? \", 'Tags': {'evaluation', 'accuracy', 'cross-validation', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38964/why-not-higher-accuracy-in-otto-data', 'post': ' On this site of Otto Group Product Classification Challenge, it is shown that best accuracy was possible with RandomForest method, but it was relatively low at 0.83. Accuracy with ANN and with Naive Bayes was even lower (0.72 and 0.65, respectively).  What is the cause of low accuracy achievable in some data? Are there any other methods with which accuracy can be increased here?  Edit: On this site: https://www.kaggle.com/c/otto-group-product-classification-challenge/leaderboard best model seems to have a score of 0.38. What will be corresponding accuracy for that? ', 'Tags': {'data', 'accuracy', 'machine-learning', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38952/neural-network-options-for-simple-data-classification', 'post': ' I want to clear about keras neural network options for classification of simple data where there are a number of features and one target column, as in iris flower dataset (Species is target):   I am finding that in almost all examples various combinations of  layers  are the only options:   What other keras layers can be used in such situations, especially if the data is large, say with 50K rows and 100 features? Edit:  My specific question is whether Dense and Dropout are the only kind of layers for this purpose and such data?  ', 'Tags': {'neural-network', 'keras', 'machine-learning', 'deep-learning'}}\n",
            "page number ------------------------------------88\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38930/egan-paper-with-confusing-notation', 'post': \" I am reading a research paper that tries to fix some issues with GAN,  but for one of the equations, the paper does not fully explain where it comes from and why it works. Although the overall algorithm for EGAN is straight forward, I wish to understand it a little better.   The paper uses an evolutionary approach to fix stuff like instability and mode collapsing. A fitness function for evaluating the different generators is give as:  Where  is:   and  is:   What I'm focusing on is   In , what does  represent. It was not defined in the paper so I figured maybe it was very typical notation for something so I looked around but could not find anything. The only things I know is from what it says in the paper, and the paper that the writers referenced to create the equation (proofs >.<)   What I know right now is the entirety of  represents what the paper calls the diversity fitness score. I also know that the second part of the equation is the same as the loss function for a discriminator so it could be re written as . But what is ? and how does this equation properly evaluate the diversity of the generated samples? \", 'Tags': {'neural-network', 'generative-models', 'machine-learning', 'gan'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38905/huge-doubt-on-anomaly-detection', 'post': '   from the naked eye itself, we can tell in the region 5161 the network usage is high so that is the anomaly in my case, then why do we want to apply k-means and other machine learning algorithms to find anomalies in our data ', 'Tags': {'machine-learning', 'data-mining', 'bigdata', 'anomaly-detection', 'data-science-model'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38900/too-little-or-too-much-maxpooling', 'post': \" I am creating a  in Keras where  shows:   The inputs are images of size . How can I find out if there is too much, too little or just right max-pooling layers? This page explain it but I am not able to get it from Kera's output here. \", 'Tags': {'neural-network', 'machine-learning', 'deep-learning', 'convnet', 'cnn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38883/data-cleaning-for-discrete-features-data-list-that-contains-null-or-n-a-data', 'post': \" I ran into this kind of problem in my projects and I want to see if there more ways to solve it.  EXAMPLE There is some data about apples and pears and the features are dominant color (red, green) and weight in grams. Some apples and pears in the data are missing the color. I know that I can just ignore and remove those apples and pears but is there another way to solve this problem? Since the data for the color is discrete, one or the other, and not continuous, I can't just plug in the median or average color of the other apples and pears. Let's also say that I can't assume or make educated guesses from the data. How should I solve this problem without removing data? \", 'Tags': {'classification', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38874/dropout-in-deep-neural-networks', 'post': ' I was reading a paper published on Dropout. What I find difficulty in understanding that, In the training phase, a unit is present with a probability  and not present with a probability . In the test phase, all units are present, but we multiply each of them with the probability.  Now, is it like, let we have 4 input units originally named a,b,c,d. In the training stage, after applying dropout, with a dropout rate of , we are left with units a and c. So, As in the test stage, all the units are present, so, is it like, we multiply each of the units with ? Also, Is  defined for each of the units in the network, or for the entire Neural Network? Also, In doing so, how is the result same for training and test stage? ', 'Tags': {'neural-network', 'machine-learning', 'deep-learning', 'keras', 'dropout'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38861/convert-a-csv-file-so-that-all-attributes-will-become-columns-with-corresponding', 'post': \"       Closed. This question is off-topic. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Update the question so it's on-topic for Data Science Stack Exchange.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         so I have this dataset in a csv file. I would like to convert it into a matrix form. The data currently looks like this:  I would like to convert it so that the Attributes become columns and the elements on each corresponding row. Like in a excel sheet. Example:  Thank you very mcuh for your help! \", 'Tags': {'data', 'machine-learning', 'python', 'data-mining'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38854/algorithm-suggestions-for-a-self-project', 'post': \"       Closed. This question needs to be more focused. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Update the question so it focuses on one problem only by editing this post.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         So, I am doing a small self project on data analytics. I am collecting the android apps data from the play store sites by web scraping. I am basically trying to collect all possible information related to a particular app, like it's type/genre and sub-genre, ratings, size, number of downloads, if it's paid/unpaid, and other possible information regarding the apps, wherever possible. I was wondering after I'm done with the data extraction and cleaning work, what data analytics related algorithms I might be able to apply on my play store apps data to analyze it and find patterns in it, predict the links amongst the data and do other predictions and data analysis on it?  A few, I came across were :- TFID, Clustering, Decision trees, Sequence Analysis and maybe Regression. So, please review the above ones and suggest any new algorithms, which might be useful to me for my project. Some specific algorithms under the above mentioned broad algorithm(if there are, which might be useful for this project ) please suggest them.  Details on them will very much helpful.  Thanks!  \", 'Tags': {'machine-learning', 'predictive-modeling', 'clustering', 'algorithms', 'classification'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38853/how-do-i-implement-a-backpropagation-neural-network-with-particle-swarm-optimiza', 'post': ' I was trying to implement pso-bpnn. Can any one help with this? I am unable to understand how a bpnn computes distance from rssi values. And how to optimize bpnn weights using pso. ', 'Tags': {'neural-network', 'machine-learning', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38833/using-sensor-data-and-a-know-reference-point-infer-the-position-of-a-moving-robo', 'post': \" Say, the robot is starting at a known position and I've data coming off of the robot as it traverses the grid layout. Exploiting the nuances captured in the data - like the implication of unequal rpm in the wheels indicating the robot is turning off to the side of slower wheels- I would like to dynamically infer the location of the robot relative to its starting position.  The ideal solution would be as the data is streaming in my model would dynamically infer its location from the data. I'd like to know if there are any existing solutions for this problem or any ideas that can help me get started are appreciated.   \", 'Tags': {'time-series', 'machine-learning', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38830/how-standardizing-and-or-log-transformation-affect-prediction-result-in-machine', 'post': ' I recently ran an elastic net model on my data. My predictors are mostly skewed. I found my model perform slightly better when I standardize on log-transformed data than standardizing on original data. I have some general questions about how standardizing and/or log transform predictors affect prediction result in machine learning.  Standardizing (center&scale):  I understand many models require standardizing predictors. For those models that do NOT require standardization, theoretically how does standardization affect the prediction result? I know it changes model interpretation. But my primary goal here is prediction. If it does not matter, then can I always scale my predictor before modeling, at least making training data more consistent when exploring many different models?   log transformation: When predictors are skewed, we often do log transformation to make them normal. How do log-transformation affect the prediction result? For models like random forest, I thought it should not matter. But I did see some difference in prediction performance to my prior experience. Again, prediction is the goal.  in addition, even for simple linear regression, the assumption is residual is normally distributed. It does not require normality of predictors. Do log-transform (normalizing) predictors help make residual normal? standardization on log-transformed predictors:  For models like elastic net, when predictors are skewed, should I do standardization on log-transformed predictors? As I mentioned earlier, I did see some difference in prediction performance.   I think these are some pretty common data preprocess questions. Some online sources are pretty confusing to me. I look forward to some experts to clarify these importance questions. I believe a clear answer will benefit a lot of people in this community. Thanks a lot.    ', 'Tags': {'statistics', 'machine-learning', 'preprocessing', 'feature-scaling'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38827/find-matching-text-from-a-text-column', 'post': ' This is my first time to use Data Analytics tool to figure out a solution to a problem. I have a table with following columns . I have been given list of  to be identified from the  column. These key phrases are in a sentence format. Example: I have to find exact or a similar matching phrase from those notes. The format of my final report would look something like this  I looked into couple of articles related to text matching which suggested options such as  fuzzywuzzy, Doc2vec, Difflib, python-levenshtein  It is all so confusing. Even if I get a starter to pick the best suitable option, I can may be take it from there. Any suggestions? Thank you so much! ', 'Tags': {'text-mining', 'machine-learning', 'python', 'nlp'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38820/normalizing-standardizing-training-and-validation-data', 'post': \" Say I split my data to 80% training and 20% test/validation and I want to standardize it, I think I'm right in saying I shouldn't standardize across 100% of the data, and then do the split, because then the validation has some insight into the training data? I'm not sure if I should either 1) Generate the mean and standard deviation stats on the 80% of training data and then apply the same mean/standard deviation to standardize the validation data. Or 2) Standardize the training data, and then standardize the validation data, i.e mean and SD is derived from the 80% for the training data, and then mean/SD is derived separately on the 20% of validation data? Many thanks \", 'Tags': {'neural-network', 'machine-learning', 'normalization'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38819/how-can-cognitive-neuroscience-enhance-machine-learning', 'post': \" There have been many recent papers on using cognitive neuroscience as inspiration for the improvement of machine learning.   For example, Hassabis et al. (2017) have written an article on Neuroscience-inspired Artificial Intelligence  Similarly, Lake et al. (2017) have published 'Building machines that learn and think like people' Marblestone et al. (2016) have their 'Toward an integration of deep learning and neuroscience' Finally, a while ago Gershman et al. (2015) released their article on Computational Rationality I'm looking for what other research has highlighted how neuroscience can enhance machine learning.  \", 'Tags': {'machine-learning', 'predictive-modeling', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38817/credit-scoring-using-scorecardpy-with-xgboost', 'post': \" I used XGBoost for scoring creditworthiness. At first I thought I could use predict_proba for scoring but then I saw that there was a module scorecardpy based on WOE to claculate code scoring. I tried to use it with my XGBoost like in an exemple but my ROC AUC fell to 0.5 and I don't see what I am doing wrong. Thanks for your help.  \", 'Tags': {'machine-learning', 'decision-trees', 'python', 'xgboost', 'scoring'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38794/reduce-number-of-vectors-in-dataset-to-achieve-the-same-average-dimensions-resu', 'post': \" I have many tests (rows), each with a large set of 3D vectors (features/cols). Each vector complies:  Simply averaging all components  gives me some  for each test (row)   What I want now is to reduce the number vectors to get a similar result to some defined extent.  So, how could I select best features? Some PYTHON EXAMPLE would be amazing Sorry if I'm asking using the wrong words. I'm new to machine learning. UPDATE: Some sample data  Each Tn averages all Vn of each dimension X, Y or Z. I want to reduce the number or Vn to calculate the average, selecting the most relevant Vn according to my data to get similar average of each T. Each vector V comes from a sensor and I want to reduce the number of sensors to get the same average to some approximation or to some limited number of sensors and calculate the difference. I have like +2000 tests T each with +100 vectors V. So the Target of the Feature Selection is the average of all Features \", 'Tags': {'machine-learning', 'feature-selection', 'python', 'ranking', 'feature-reduction'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38771/how-to-create-user-and-item-profile-in-an-item-to-item-collaborative-filtering', 'post': ' I want to build a recommender system for a coupons website which should do the following: Given the past purchase behaviour of a user, recommend coupons which the user is likely to buy. The data does not have any ratings for coupons by the user. It tells if a user bought a certain coupon or not, which category did it belong to, what is the gender and location of the user who bought these coupons etc.? I am implementing an item to item collaborative filtering. Based on what I have learnt through videos available on this topic, the following is my approach:  Build an item profile of all the coupons with attributes such as category of the coupon, location the coupon is available in and the days on which the coupon can be redeemed. Build a user profile of all the registered users with attributes such as age, gender, and location.  Now I am making an item and user matrix, where I will have Coupons on the rows and Users in the columns. I will fill 0 and 1 depending on if a coupon C1 was bought by a user U1 in the past or not. In the same dataset, for the coupons for which I want to predict, I will calculate a score based on cosine similarity between items.  My question is if I am using just 0s and 1s (coupon bought or not) to recommend coupons, what is the use of the item and user profile? How I can incorporate the intelligence in the recommendation system that if a user has bought more coupons from X category in the past than recommend him new Y coupons from the same category?  I read online about using random forests etc. to calculate \"weights\" of the attributes in recommendation systems, where do they fit in this scenario?  P.S. I am doing everything in R.  Many thanks in advance for any advice/suggestions.  ', 'Tags': {'machine-learning', 'predictive-modeling', 'recommender-system'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38767/coefficient-of-determination-is-close-to-1-but-the-value-of-rmse-is-large-what', 'post': ' I am working with the  and trying to understand how well the data fits the model. I calculated both  and . At a certain depth,   has the value equal to  but the value of RMSE is equal to . I cannot understand this. The value of RMSE is quite large but the value of the coefficient of determination is close to 1.0. What does it really mean? Is the model/fit good enough?   ', 'Tags': {'machine-learning', 'python', 'decision-trees'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38766/a-way-to-identify-tuning-parameters-and-their-possible-range', 'post': \" I am a novice in Machine Learning. But when I started learning, I figure out that all the methods have some tuning parameters and those parameters take a range of possible values. By grid searching, we identify a set of these parameters that optimize some function. But is there any way to find the possible domain of the tuning parameters? This would definitely save my time and the computer's job. In addition, some methods such as  have loads of tuning parameters. Is there any way to know which one to tune and which one to leave as it is. I have been using  python library. \", 'Tags': {'xgboost', 'hyperparameter-tuning', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38758/should-i-pad-zero-or-mean-value-in-a-convolution-neural-network-what-about-refl', 'post': ' In convolution layers, sometimes you need to pad some (usually 1 or 2 pixel) 0s at the edges of the original image before applying your convolution kernel (filter). However, 0 has different meanings. In the case of 0 - 255 gray scale images, 0 means black. In the case of normalized images ( minus mean then divided by std), 0 is the mean value. Should I always pad 0, or should I sometimes pad mean value ? If in some cases I should pad mean value, what are those cases?  Edit: And when should I use reflective padding? Further more, is reflective padding always better than zero padding? ', 'Tags': {'neural-network', 'machine-learning', 'cnn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38756/error-in-using-fit-on-randomforest-classifier-where-x-was-a-pandas-dataframe-o', 'post': ' On using fit() method on sklearn.ensemble.RandomForestClassifier I am getting a value error that says.  The data-set used is the one in Titanic:Machine Learning from Disaster competition on Kaggle.  Here is the link- https://www.kaggle.com/c/titanic Can someone please help me how to deal with this, why is it occurring and how to prevent it in future. Note-There are no NaN in my DataFrame for train_X, i.e I have replaced all NaN with df.fillna(df.mean()), also I cross-checked that no NaN values exist by using  where, train_X is the training data for features. Please Help!! ', 'Tags': {'random-forest', 'machine-learning', 'pandas', 'scikit-learn', 'classification'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38754/face-recognition-using-hogsvm', 'post': \" This is my approach  where I get encodings using HOG and names from:  but when I train    'samples is not a numpy array, neither a scalar'  this error appears in the line:  I think I need to convert the names to a numpy array.  How can I do that?  \", 'Tags': {'machine-learning', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38752/are-there-cases-where-tree-based-algorithms-can-do-better-than-neural-networks', 'post': ' I was experimenting with different modelling methods including KNN, Decision Trees, Neural Networks and SVN and trying to fit my data to see which works the best. To my surprise, the decision tree works the best with training accuracy of 1.0 and test accuracy of 0.5. The neural networks, which I believed would always perform the best no matter what has a training accuracy of 0.92 and test accuracy of 0.42 which is 8% less than the decision tree classifier.  Could someone please explain the circumstances/cases where neural networks could have low accuracy when compared to a modelling technique like the decision tree. I had tried my neural network with different configurations like:  but not in a single case, I found the neural network to beat the decision tree test accuracy of 50%. ', 'Tags': {'neural-network', 'machine-learning', 'python', 'decision-trees'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38743/sensitivity-analysis-of-a-machine-learning-model', 'post': ' Let’s say I have a set of input variables (, ,  and ) and I predict a target () using a machine learning model ( in my case) with a reasonable performance (5% relative error on test set).  Now, I want to do some kind of sensitivity analysis on this model by answering two questions:  What is the impact of a 5% independent increase in variables ,  and  (not ) on the target variable? From variables , ,  and ; which combination of values of ,  and  (without touching ) increases the target  value by 10, minimizing the sum of ,  and .  I have already answered question one (see this gist). However, how can question 2 be coded? I imagine that this implies an optimization problem. ', 'Tags': {'scikit-learn', 'xgboost', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38742/reinforcement-learning-how-to-deal-with-varying-number-of-actions-which-do-num', 'post': ' I am a new to Reinforcement learning, but I am trying to use RL in this task: Given a function definition in written e.g. in C with 1 to 10s of input arguments (only numerical ones - integer, float, etc.) and the body of the function (represented as a Abstract Syntax Tree/ Abstract Decision Tree with data dependencies - how the internal variable values change) I would like to approximate the values of these input parameters so for e.g. a certain decision block is executed. For this I thought of a recurrent network with LSTM cells. Now, to achieve this, I would traverse one path in the tree leading to the block and take note of any data changes and decision blocks in the path. These steps would influence my parameter input predictions - what values to insert into/change in the input parameters if I wish to have a certain decision block executed.  Action: Changing the value of one chosen input parameter of the function OR Changing the value of all input parameters individually (with mathematical different operation). After action execution, moving onto the next node in the tree. Reward: How close I am to executing the given decision block (thus satisfying the condition) with given input parameter values. Goal: Have a condition in the code satisfied and a decision block thus executed (e.g. an if condition is met).   State: Current position in the AST/ADT with data dependencies. Assuming that I already have a way to evaluate, how far I am from executing the wanted decision block given current parameter input values, I came across two problems:  How would I deal with varying number of function input parameters in RL? If I want to change their values to be closer to the execution of the wanted decision block, the number of given actions changes with the number of parameters defined for the given function. If I already did chose one parameter, what is the best way to do number approximation using RL? In the function body there could be numerous very complex mathematical operations happening, so should there be defined action as logarithm, exponentiation, division, multiplying, etc. or is there a better way with maybe just adding/subtracting from the current value?  ', 'Tags': {'neural-network', 'lstm', 'machine-learning', 'reinforcement-learning', 'rnn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38739/confusion-matrix', 'post': ' Here is my question in my assignment:  You have built a classification model with 90% accuracy but your client is not happy   because False Positive rate was very high then what will you do?  This is the question..nothing is given in the background ', 'Tags': {'confusion-matrix', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38729/statistical-machine-translation-word-alignment-for-fr-eng-and-eng-fr-what-is-p', 'post': \" I'm currently trying to implement this paper, but am struggling to understand some of the math here. I'm pretty sure I understand how to implement the E-step, but for the M-step, I'm confused on how to compute the M-step. It says just before section 3.1 that , and then the same for  but with  and  swapped. The second part of this makes sense to me, but what is  or ? From my understanding,  are sentences in the bi-text. So how would we compute the probability of a sentence? It says earlier that  and  are arbitrary distributions that don't affect the optimization problem, but then how do we compute ? Thanks! \", 'Tags': {'markov-hidden-model', 'machine-learning', 'probability', 'expectation-maximization', 'machine-translation'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38721/detecting-abnormal-cat-behaviour-via-supervised-learning', 'post': \" A few work colleagues and I were looking through a recently replaced 'cat', we had in the workplace. For those of you which are curious, the 'cat' in this context, refers to a specific type of pump known as a cat pump. It's affectionally called the 'kitty' as we only have one of these pumps.  After one routine inspection, I noticed a number of periods where kitty wasn't performing well. This is labelled in the dataset as Cat Anomaly.  (Accessible from here if anyone wishes to analyse further:  https://drive.google.com/open?id=1e0OhzhaSZP9_A3QdaK9-BvWXbRPZacED) This is what it looks like when it is abnormal: Essentially, it looks similar to a sinusoidal pattern and is slightly curved. I also noted that the cluster of points seem to be quite tight when it is an abnormal state.   Using Python's Sklearn Libraries and some good ol fashioned signal processing, I decided to remove some of the noisy clusters so I can see what the signal looks like 'without' the noise. To do this, I have performed a Savitzy-Golay Filter on the data, which has smoothed this out as per below.   I have then used a feature to subtract the values from the 'normal' cat values - 'smoothed' cat values which gives me a rough indication of when the 'clusters' appear.  This has worked acceptably (~90% accuracy) and I can pick up abnormalities to an extent when I feed this feature into my random forest model, but I feel the 'cluster' approach I have used is very clunky and there are much better ways to capture the 'noisiness' of the cat signal as a variable.   So now I'm looking for a better way of recognizing and separating 'abnormal' cat behaviour. So, as a better feature or variable to capture 'noisy' clusters, does anyone else have any ideas on improvements on how to capture this?  \", 'Tags': {'supervised-learning', 'machine-learning', 'image-recognition'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38717/can-i-retrain-my-keras-model-on-small-dataset-with-some-empty-folders', 'post': \" I am working on a vehicle classification problem.  I trained my model with 605 labels(folders) and 300k photos. After testing my trained model, I checked my wrong answers' training and validation photos. I realized some of my labels training or validation photos are wrong or not enough different from another.  I can prepare a small dataset that only contains these wrong learnt labels photos and keep other labels folders empty.  If i retrain my model, will I get better accuracy? Or should I train from scratch? \", 'Tags': {'keras', 'machine-learning', 'python', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38708/can-grid-based-clustering-method-be-use-for-customer-segmentation', 'post': \"       Closed. This question needs details or clarity. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Add details and clarify the problem by editing this post.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         I am trying some clustering methods for customer segmentation and I stumbled upon grid based methods like: STING, MAFIA, WAVE CLUSTER, and CLIQUE. However, from what i've read, most of them are for image segmentation. So before I invest my time in implementing these algorithms, I would like to know if anyone has tried using grid based clustering for clustering customer data before or on something that is not image based? \", 'Tags': {'machine-learning', 'clustering'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38706/is-there-a-site-like-fast-ai-or-deeplearning-ai-but-for-machine-learning-instead', 'post': \"       Closed. This question needs to be more focused. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Update the question so it focuses on one problem only by editing this post.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         I'm looking for a summary of the pros and cons of different machine learning models in practise. Something that includes: - How the model works - What kinds of outliers will be misrepresented by the model - Effect of different parameters in each model on speed/performance \", 'Tags': {'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38682/data-augmentation-for-the-inputs-of-cnns-to-identify-flowers', 'post': ' I want to make a neural network to identify flowers from images like this:   or similar images e.g as on https://www.pexels.com/search/flowers/ I want to use a CNN for this as in https://stackoverflow.com/questions/52463538/reducing-memory-requirements-for-convolutional-neural-network My questions are:  Which will be better: to put whole images into training set or to divide each image into 4 parts (cutting in midline horizontally and vertically)? Will it help if I rotate/tilt these images and put them also in training set? Will it help if I blur these images and put them also in training set?  The main aim of network is to identify which kind of flower it is. For this, it will be trained with images of about 20 flowers. Hence, classification is the objective. Edit: we can assume there will be only one kind of flowers in one image. ', 'Tags': {'neural-network', 'machine-learning', 'image-classification', 'deep-learning', 'cnn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38680/what-does-model-recursive-loss-convergence-mean', 'post': ' I trained a very simple neural network to figure out some features from an image. it\\'s something like a reverse graphic. The neural Network will receive an image and try to estimate its features. The features can completely describe the image(like inverse graphic-see p.s.2). So we can use them to reconstruct the image using a \"rendering function\". Obviously, the network has some error and the image cannot be reconstructed exactly. I fed the \\'Reconstructed Image\\' to the model and got some new \\'Estimated Features\\'. I repeated this and after some time the \\'Estimated Features\\' has converged to some value. Not the \\'True Value\\' for sure but something. What does this repeated estimation convergence and also the loss convergence of this repeated process mean? can it be useful for anything? for example, for training the Neural Network model in a more efficient way. This is the plot of the convergence of the estimated parameters and the real value of each of them. Colors represent each Param. lines are estimated values in each feedback loop iteration and \\'+\\' is the real value.   p.s.1: the loss is too high cause I wanted to show how it converges to something. with more training, the Loss decrease and the distance get\\'s really small. p.s.2: For the reverse graphic, you can imagine a set of features like (x, y, width, height, thickness) which can be used to draw a rectangle. The rendered image then can feed into a model to find the estimated features of (_x, _y, _width, _height, _thickness) p.s.3: The 5th feature\\'s True and estimated value is 0 for all samples  ', 'Tags': {'neural-network', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38669/architecture-advice-for-training-a-gan', 'post': \" I'm trying to create a model that generates worlds for a game. The game is 2-dimensional and small worlds sizes. I've sampled about 15,000 worlds that are about 200x200 blocks. Each block has its own id ranging from 0 to around 1500, although not all ids are used. Right now I'm training with 64x64 samples. Here's where I'm seeking advice. Right now I'm normalizing all the blocks into a sequential hash before normalizing them from -1 to 1. If I'm training with ids 0, 2, 3 I would first hash them into 0->0, 2->1, 3->2, then into 0->-1, 2->0, 3->1. I don't know if that's right, because my generator is getting completely weak after a few epochs and the discriminator loss is converging to 0 usually pretty quickly. I know I haven't given much information, but I'm just wondering if another approach might be easier to train. My other idea would be to have a generator that outputs a 64x64x300 where each 64x64 dimension corresponds to a list of probabilities for each x, y for that block (like a softmax?) Any sort of advice would be appreciated, as I'm very much an amateur with machine learning. Thanks, and let me know if you'd like more information for context (I'd thought I would spare the potentially unnecessary details) \", 'Tags': {'machine-learning', 'gan'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38666/is-max-depth-in-scikit-the-equivalent-of-pruning-in-decision-trees', 'post': \" I was analyzing the classifier created using a decision tree. There is a tuning parameter called max_depth in scikit's decision tree. Is this equivalent of pruning a decision tree? If not, how could I prune a decision tree using scikit?  \", 'Tags': {'supervised-learning', 'machine-learning', 'decision-trees', 'python', 'scikit-learn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38662/what-approach-to-use-to-detect-violations-of-media-ethics-in-news', 'post': \" I've been trying to come up with a solution to detect violations of media ethics in news articles. For example, I need to detect if an article has mentioned the name of a victim of rape. So far, I've tried to perform a binary classification using a Multinomial Naive Bayes classifier with sklearn. However, it does not produce very accurate results and I believe this is because there are no clear-cut features to be used for classifying. I really appreciate if someone can point me in the right direction or give me some tips on how to proceed. \", 'Tags': {'classification', 'machine-learning', 'natural-language-process'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38656/where-to-find-state-of-the-art-time-series-data-deep-learning-approaches', 'post': '       Closed. This question needs details or clarity. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Add details and clarify the problem by editing this post.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         Just like this (which stopped updating) that shows the newest deep learning methods utilized in image recognition, is there some similar lists/summaries (in blog posts/paper/competitions etc.) about newest deep learning methods used in time series analysis? ', 'Tags': {'machine-learning', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38643/1x1-convolution-how-does-the-math-work', 'post': \" So I stumbled upon Andrew Ng's course on  convolutions.  There, he explains that you can use a  convolution to shrink it. But when I do:  I obviously get  matrix. So how can I shrink it? Just add the result of every  kernel result? So I'd get a  matrix? \", 'Tags': {'machine-learning', 'convnet', 'convolution'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38636/getting-rid-of-maxpooling-layer-causes-running-cuda-out-memory-error-pytorch', 'post': ' Video card: gtx1070ti 8Gb, batchsize 64. I had such UNET with resnet152 as encoder wich worket pretty fine:  Then I edited my class looks to make it work without pooling layer:  is_deconv - in both cases True. After changing it stop to work with batchsize 64, only with with size of 16 or with batchsize 64 but with resnet16 only - otherwise out of cuda memory. What am I doing wrong? Full stack of error:  ', 'Tags': {'computer-vision', 'machine-learning', 'deep-learning', 'python', 'pytorch'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38634/what-does-the-co-ordinate-output-in-the-yolo-algorithm-represent', 'post': \" My question is similar to this topic. I was watching this lecture on bounding box prediction by Andrew Ng when I started thinking about output of yolo algorithm.  Let's consider this example, We use 19x19 grids and only one receptive field with 2 classes, so our output will be => 19x19x1x5. The last dimension(array of size 5) represents the following:  I don't understand whether X,Y coordinates represent the bounding box with respect to the size of entire image or just and receptive field(filter). In the video the bounding box is represented as a part of receptive field but logically receptive field is much smaller than bounding box and also people might tinker with filter size, so positioning bounding boxes with respect to filter makes no sense. So, basically what does the coordinates of bounding boxes of an image represent ? \", 'Tags': {'machine-learning', 'image-classification', 'object-recognition', 'deep-learning', 'image-recognition'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38631/can-not-run-auto-weka', 'post': \"       Closed. This question is off-topic. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Update the question so it's on-topic for Data Science Stack Exchange.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         I have downloaded Auto WEKA into my computer and installed Java Runtime Environment and I have WIndows 10 I am trying to run the Auto WEKA by double clicking on  file after assign jar files to Java but that does not work I also tried using creating  file same as this link suggested yet nothing works https://stackoverflow.com/questions/394616/running-jar-file-on-windows how can I get the GUI running? \", 'Tags': {'machine-learning', 'weka'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38616/typeerror-not-supported-between-instances-of-int-and-str', 'post': ' I have the following code  and I get the following error:  TypeError                                    Traceback (most recent   call last) ipython-input-109-c48c3ffd74e2> in () 2 rf.fit(X_train, Y_train) 3 print (\"Features sorted by their score:\") ----> 4 print (sorted(zip(map(lambda x: round(x, 2),   rf.feature_importances_), X_train), reverse=True)) TypeError: \\'<\\' not supported between instances of \\'int\\' and \\'str\\'  I am not sure what I am doing wrong. I only have int and float in my dataframe. ', 'Tags': {'scikit-learn', 'error-handling', 'machine-learning', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38614/fully-connected-layer-in-deep-learning', 'post': ' How to determine the best number of the fully connected layers in ? Can I use only one fully connected layer in CNN? How to determine the dimension of the fully connected layer output? ', 'Tags': {'neural-network', 'tensorflow', 'machine-learning', 'deep-learning', 'cnn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38612/feature-importance-python', 'post': ' My dataset has around 1000 features and 30k rows.  All the feautres have value either 1 or 0. My target variable is Size which 3 classes : Small, Medium and Large. I have around 5k \"small\" data points, 9k \"Medium\" data points and 20K \"Large\" data points. I want to find feature importance. I am not sure which algorithm will work best for features with only 1s and 0s. I tried Chi2 but accuracy is very low:  My accuracy is only 58% and the model is only good in predicting \"Large\" data points. Below is the Confusion Matrix  I mostly have 2 questions:  I am not sure which algorithm will work for best features with only 1s and 0s How do I get higher accuracy using chi2 (predict Small and Medium class better)  Thanks! I would really appreciate any input on this. ', 'Tags': {'classification', 'feature-selection', 'machine-learning', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38570/how-can-i-improve-the-accuracy-of-my-neural-network-on-a-very-unbalanced-dataset', 'post': ' I have a dataset which contains data for about accidents. The dataset consists of about 15.000 entries and I can\\'t get more. The Distribution is as follows:  88.6% of the data are class 1 accidents 10.6% of the data are class 2 accidents 0.8% of the data are class 3 accidents  As you can see, the biggest part of the training data belongs to one class. I have only very few examples for class 3 accidents (about 100 rows out of 15.000), but it would be most important to classify class 3 accidents correctly. I trained a pretty standard deep neural network on the data and got an accuracy of ~93% on the validation set. I used a custom Tensorflow estimator with an AdamOptimizer and tuned the parameters as good as possible.  The Problem is, the network still classifies most of the accidents as class 1 accidents. So if I have for example 25 class 3 accidents in the validation set, the network misclassifies 10 of them as class 1. I want to improve that. Are there any methods to improve the performance in this case? The obvious choice would be to get more data of class 3 accidents, but sadly that is not possible.  Does it make sense to show the existing class 3 data multiple times? So for example, train 5 Epochs with all data and then 3 additional Epochs with just the class 3 accidents? Or could I do something during the data preprocessing? Right now I\\'m MinMax-Scaling the input data to get to the [0, 1] interval. Is there maybe any other way to emphasize outliers more? (If you assume outliers mostly belong to class 3) I hope someone knows some methods to increase the accuracy in this case. EDIT: The Dataset has mostly categorical columns like:  Street Class (e.g. highway or country road) Light (e.g. \"good\") Weather (e.g. \"rainy\" or \"sunny\") ...  Additional it has these columns:  Accident Date (just month and day) Age  Time of the day  Number of injured persons Number of Vehicles  So an entry might look like this:  ', 'Tags': {'machine-learning', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38563/how-to-do-good-keyword-extraction', 'post': ' I tried the sketch engine (no ad!) and I wonder what might be the underlying algorithms to do such a good keyword extraction. I have a document consitsing of rows of sentences and from each I want the proper NERs or keywords. I tried a lot. I need them to write my own NER based on CRF. Is this just statistics used to extract the keywords or what else do you need for that? I ask in general and not how they did it, but it would be nice to find out if someone knows... ', 'Tags': {'machine-learning', 'nlp', 'named-entity-recognition'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38562/can-we-compile-coreml-on-server', 'post': \" I am working with CoreML and ARKit for Face recognition. But i don't want to build CoreML model with app. I have make a coreML model with python Turicate. I want this model to be put on server and it give us a API then i will sent face pictures to this model through this API and then server get back a response in form of JSON.I want this because CoreML size is very high and if we want to more train this model then this user has to download this high size model and compile on device.I have research and found that apple adds a feature to compile CoreML model on device but every time for small training upgradation in model user has to download this high size model again. \", 'Tags': {'machine-learning', 'image-recognition'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38546/branching-decision-tree-exit-node-and-visitation-duration-classfication', 'post': \" I have a dataset where every record in each data cluster corresponds to a step in a branched tree (from an IVR system). I am still learning some of the more machine learning concepts of data science and was wondering for pure detection of exit nodes and detecting time in system, would a flat file analysis be a better option, or is there a better approach that would be faster considering I am dealing with a large (for me) dataset of 2-4 Mil records?  I apologize if this is asked and answered, as I am definitely in a don't know what I don't know scenario here, and would appreciate any help.  I cannot post any of the dataset as it is sensitive info. \", 'Tags': {'bigdata', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38535/xgboost-performs-significantly-worse-than-random-forest', 'post': ' I have a dataset of 3500 observations x 70 features which is my training set and I also have a dataset of 600 observations x 70 features which is the test set. The target is to classify observations correctly either as 0 or 1. 2000 observations of the training set are 0 and the rest 1600 of them are 1. I aim at the highest possible recall for precision>=90%. I did grid search for ensemble algorithms only in relation to number of trees (from 50 to 650 trees). Analytically the best recall results for precision >= 90% for each of the algorithms are the following: Random Forest (375 trees)   Precision: 90% Recall: 24%  Xgboost (550 trees)   Precision: 90% Recall: 15%  Why Xgboost is performing so much worse than the Random Forest? ', 'Tags': {'random-forest', 'classification', 'xgboost', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38526/boundaries-of-reinforcement-learning', 'post': ' I finally developed a Game Bot that learns how to play the videogame Snake with Deep Q-Learning. I tried with different neural networks and hyper-parameters, and I found a working set-up, for a specific set of rewards.  The problem is is: when I reward the agent for going in the right direction - positive rewards in case the coordinates of the agent increase or decrease accordingly to the coordinates of the food - the agent learns pretty fast, obtaining really high scores. When I don\\'t reward the agent for that, but only negative rewards for dying and positive for eating the food, the agent does not learn. The state takes into account if there\\'s any danger in proximity, if the food is up, down, right or left and if the agent is moving up, down, right or left.  Here\\'s the question: is rewarding the agent for going into the right direction a \"correct approach\" in Reinforcement Learning? Or it\\'s seen as cheating, cause the system needs to learn that by itself? Is passing the coordinates of the food as state an other way of \"cheating\"?  ', 'Tags': {'machine-learning', 'deep-learning', 'reinforcement-learning', 'keras', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38516/i-have-limited-samples-for-one-class-unlimited-samples-for-the-other-class-nee', 'post': ' I want my machine learning algorithm to learn the difference between two classes, actually  or . My sample data is:  500  (I know it is low, unfortunately I can\\'t do much about it) 1,000,000   Question: Should I do the training with all of the 1,000,000 ? Or would such an imbalance have negative effects? For instance would it kind of \"drown\" the other data? Notes:  Computing power and time is not an issue. In the real world,  make up 5% to 10% of the data, so I don\\'t think I have a class imbalance problem. I think the classification is an easy one, machine learning will probably be able to understand quickly. I am OK with a reasonable amount of misclassifications. In case that matters, I am planning to use Keras and Tensorflow with Flatten/Dense relu/Dense softmax/AdamOptimizer/sparse_categorical_crossentropy.  ', 'Tags': {'machine-learning', 'sampling'}}\n",
            "page number ------------------------------------89\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38507/why-should-we-use-or-not-dropout-on-the-input-layer', 'post': \" People generally avoid using dropout at the input layer itself. But wouldn't it be better to use it? Adding dropout (given that it's randomized it will probably end up acting like another regularizer) should make the model more robust. It will make it more independent of a given set of features, which matter always, and let the NN find other patterns too, and then the model generalizes better even though we might be missing some important features, but that's randomly decided per epoch. Is this an incorrect interpretation? What am I missing? Isn't this equivalent to what we generally do by removing features one by one and then rebuilding the non-NN-based model to see the importance of it? \", 'Tags': {'machine-learning', 'deep-learning', 'machine-learning-model', 'deep-network', 'dropout'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38474/what-does-the-one-function-mathbf1-i-yt-exactly-mean-in-backward-pro', 'post': ' It confused me for a long time what is  exactly mean in (10.18) below.  It is in the Chapter 10 on RNN of the book LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. \"Deep learning.\" nature 521.7553 (2015): 436. Because the  doesn\\'t seem to be a condition while the author has introduced a condition function as  if condition is true or  if the condition is false.   It is computing the gradient of the log-likelihood with backward propagation in recurrent neural network. Instead of , I found my solution might be   It is based on the prior assumption of Gaussian distribution with  Below is the equation I am asking about:  The gradient  on the outputs at time step , for all , , is as   follows:     While, the corresponding notes are:    And I wonder which version of softmax function is used here?  or  ? Thank you very much! ', 'Tags': {'neural-network', 'machine-learning', 'deep-learning', 'recurrent-neural-net', 'rnn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38464/continuously-train-coreml-model-after-deploy-on-server', 'post': \" I am working on face recognition with CoreML.I make a CoreML model with python Turi Create and put it on Server. User download it and compile on device but there is one problem if we want to do any updation like add a new user's face then we have to make this model again and then put it on Server.I want that if i deploy a model on Server then for every new user i train this model on the server.Is there any possible ways to do this. \", 'Tags': {'machine-learning', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38448/candidate-elimination-algorithm-simple-problem', 'post': \" I'm trying to understand version space learning and the Candidate Elimination algorithm. Define the set of most general and the set of most specific hypotheses. Take these training examples with the simple binary features:  I searched a lot of it but I didn't find any helpful tutorial (apart from this tutorial: link). I tried myself to find a solution and I got that the set of most general hypotheses is  and the set of most specific hypotheses is .  I know that this problem is not that hard, but could you help me with some instructions on how to solve it? \", 'Tags': {'classification', 'theory', 'machine-learning', 'binary'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38447/multiple-output-vs-single-output-nns', 'post': \" I'm trying to build a 5 input-5 output model using LSTM, where all the outputs are the same features as the inputs, predicted in the future. My question is: is it better to build 5 models, each with the same 5 inputs, but predicting just 1 of the 5 sequences at a time, or is it the same as building 1 model predicting all 5 sequences? In other words, is the accuracy per predicted sequence going to be higher with 5 separate models or will it be the same as 1 model with 5 outputs.  The reason for my confusion is that, in the case of the multiple output model, the hidden layer will be the same; so how would the algorithm go about optimizing the weights so as to minimize error for all output sequences? \", 'Tags': {'neural-network', 'lstm', 'machine-learning', 'time-series', 'rnn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38440/boosting-an-xgboost-classifier-with-another-xgboost-classifier-using-different-s', 'post': ' What I would like to do, is train a first model $f_{1}(\\\\underline{x})$, where $\\\\underline{x}$ is a set of features, fix what model 1 has learned, and then train a second model $f_{2}(\\\\underline{y})$ where $\\\\underline{y}$ is a second set of features.  (it\\'s not really the emphasis of this post, but in case you\\'re curious as to why I want to do this, see the bottom of the post) My target variable is binary, and I want to maximise (binary) cross-entropy rather than accuracy.  While there is nothing intrinsic to this problem that dictates I should use XGBoost, XGBoost is performing favourably compared to other models on the problem of predicting the target when using only the external variables, so I would like to find a way of getting XGBoost to do this. It seems to me, that this will require using a custom cost function, which requires using the generic booster class rather than  . When using the booster class, it outputs a real number, with no constraint of being between $[0,1]$, so one needs to define $P(z_{i}=1|x_{i})=\\\\frac{1}{1+e^{-f_{1}(x_{i})}}$  and then implement binary cross-entropy accordingly (because I\\'ve got two sets of features denoted by x and y, I\\'ve somewhat criminally used z to refer to the target) I then train a second custom booster, $f_{2}(\\\\underline{y})$ in which $P(z_{i}=1|x_{i}, y_{i})=\\\\frac{1}{1+e^{-(f_{1}(x_{i})+f_{2}(y_{i}))}}$ This requires a new custom implemented cost function (it\\'s a bit hacky, as I don\\'t think xgboost allows the custom cost function to be passed any arguments other than  and , so after training the first classifier, I save the train predictions in a global variable which I then call in the custom cost function of the second classifier. Not the main point of this post, but if anybody knows a way around this, I\\'d be super grateful) What happens when I do this, is a little odd (I\\'m using a validation set and verbose progress printing, as well as early stopping, so I can watch my classifier \"get better\" as it iterates). Note that for debugging purposes, I am not using a set of external and controllable features, I\\'m just using a set of features, X, which I artificially subdivide into (x,y), in which I know that x is a suboptimal set of features (as in, a classifier trained on only x performs somewhat worse than a classifier trained on the full X). I thus would expect the combination of classifiers to perform better than the first one (although not necessarily as well as a single classifier trained on the full X) Let\\'s say Classifier 1 finishes and has a cross-entropy of K. Classifier 2 starts training, and its validation cross-entropy does actually drop for a substantial number of iterations before exiting. But, the problem is that when classifier 2 starts training, on the zeroth iteration, the validation loss is substantially lower than it was when classifier 1 exited.  I have a hypothesis as to why this happens, which is that in xgboost, the way the first tree is generated is special. For example one possible way to do xgboost regression, is for the first learner in the ensemble to simply output the mean of the target variable, and then each subsequent learner to learn a correction to this. In general, the theory behind xgboost assumes that corrections to the output will be small, and thus a second order taylor expansion is valid, so for this to be true, the first learner needs to be relatively good. Subsequent trees are multiplied by a \"learning rate\" to ensure that they only make small corrections, but the first tree is not.  This hypothesis is backed up by the fact, that changing the learning rate to something ridiculously small basically doesn\\'t change the amount by which the loss decreases between the final iteration of classifier 1 and the zeroth iteration of classifier 2.  My Actual Question(s) Is there a good way around this? I have two ideas but do not know how to implement them/whether it\\'s possible. 1: Can I force XGBoost to also multiply the first tree in the ensemble by the learning rate? 2: The XGBoost booster class can take  as an argument, and boost an already existing classifier (allegedly, this doesn\\'t have to be an XGB model, so I hear, but I\\'ve only played around with doing this with an xgb model). The problem here is, that I think the features the two models take need to be the same, as under the hood, XGBoost will be calling  and then using that same dmatrix to train model_2, but of course I want to do this with different dmatrices.  If you\\'ve made it this far, thanks for reading, and an even bigger thanks if you can help. Below I\\'ll provide the actual maths/code details Cost Functions The first classifier $f_{1}(x)$ takes a set of features $\\\\underline{x}$ and maps to a real number. We associate this with a probability as discussed above. The corresponding cost function is: $C = \\\\sum_{i}z_{i}\\\\ln\\\\frac{1}{1+e^{-f_{1}(x_{i})}}+(1-z_{i})\\\\ln \\\\left(1-\\\\frac{1}{1+e^{-f_{1}(x_{i})}}\\\\right)$ which rearranges to  $\\\\sum_{i}f_{1}(x_{i})(z_{i}-1)-\\\\ln (1+e^{-f_{1}(x_{i})})$ Similarly, the second classifier takes a set of features $\\\\underline{y}$ and maps them to a real number $f_{2}(\\\\underline{y})$. The cost associated with the output of this second classifier is given by: $\\\\sum_{i}(f_{1}(x_{i})+f_{2}(y_{i}))(z_{i}-1)-\\\\ln(1+e^{-(f_{1}(x_{i})+f_{2}(y_{i}))})$ XGboost doesn\\'t need to be passed the actual cost functions, it needs to be passed the first and second derivatives, and as vectors, i.e. if $C=\\\\sum_{i}c_{i}$, XGBoost requires $\\\\frac{\\\\partial c_{i}}{\\\\partial f_{1}(x_{i})}$ and $\\\\frac{\\\\partial ^{2} c_{i}}{\\\\partial ^{2}f_{1}(x_{i})}$ for the first cost function and $\\\\frac{\\\\partial c_{i}}{\\\\partial f_{2}(y_{i})}$ and $\\\\frac{\\\\partial ^{2} c_{i}}{\\\\partial ^{2}f_{2}(y_{i})}$ for the second cost function. I calculate that  $\\\\frac{\\\\partial c_{i}}{\\\\partial f_{1}(x_{i})}=z_{i} -1 +\\\\frac{1}{1+e^{f_{1}(x_{i})}}$ and $\\\\frac{\\\\partial ^{2}c_{i}}{\\\\partial ^{2}f_{1}(x_{i})}=-\\\\frac{e^{f_{1}(x_{i})}}{(1+e^{f_{1}(x_{i})})^{2}}$ and similar expressions for the second cost function. Code The code for my first cost function looks like this:  (note, grad and hess have been multiplied by -1, as I think XGBoost is trying to minimise loss rather than maximise) The cost function for the second classifier looks like:  (note the hack of requiring the global variable). Similarly, in order to watch your classifier\\'s performance on an evaluation set, a rather odd quirk of xgboost is that you need to re-implement the cost function so that it outputs the cost as a scalar (rather than its gradients as a vector). I\\'ll provide these here for completeness (another weird quirk is that you need to give the metric a name, which is what the strings are about):   and   Then the code which actually trains the first model (there are three decision matrices, dtrain, deval and dtest):  After training the first model, I assign  and , as required by  and  respectively:  Next, I re-assign ,  and  (I won\\'t include this code, it\\'s just data manipluation), maybe this is sloppy I should call them  etc, but I haven\\'t... and the code which trains the second model  I think that\\'s about all of the (useful) code I can provide. Why am I interested in this I\\'m looking to learn how a set of actions in the past have changed the outcome (e.g. marketing attribution). One way of doing this which is what I\\'m investigating here, is to train a model to learn how the target variable is related to external factors which I have no control over, denoted by $\\\\underline{x}$, and only when all of this dependence has been learned, do I train a second model which learns how factors which I can control/interventions, $\\\\underline{y}$ have affected the outcome.  Implicit is the assumption that the external factors have a (much) larger effect than the ones I have control over, and that in the historical data, $\\\\underline{x}$ and $\\\\underline{y}$ might be inter-correlated (interventions have not been independent of environmental factors).  If you train one model with all the features at once, you can\\'t control to what extent the model chooses to learn information contained within both $\\\\underline{x}$ and $\\\\underline{y}$ (because they are correlated) from each. Of course you want the model to learn as much from $\\\\underline{x}$  as possible, as you don\\'t have any influence on external factors.  ', 'Tags': {'cost-function', 'machine-learning', 'python', 'boosting', 'xgboost'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38418/how-to-generate-data-if-algo-itself-is-involved-in-the-process-with-a-feedback-l', 'post': ' I have an algorithm which would be a rather easy classification task with a set of features and a class output which I would like to solve with a machine learning algo. But I am having issues and doubts about the data generation. The features my algo uses as inputs are processed beforehand by other algorithms and more importantly, also have a feedback loop to my the algorithm I want to change. Basically, the better my algo is getting, the less false positives there should be. But with less false positives, I have more and more imbalanced data to work with, which would mean, it is harder to train the algorithm. I could reduce the performance of my algo on purpose and generate data, but then I am not sure if the data I am getting is any meaningful as there is a feedback loop. To me this seems like a chicken, egg problem. ', 'Tags': {'data', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38413/statistical-test-for-machine-learning', 'post': \" I want to prove that my proposed machine learning algorithm (prop_ml) is better than other baseline algorithms (ml_1, ml_2, ml_3) when given a small number of data for training. What I've done is to split a dataset into train and test sets. Then, I've randomly selected small k samples (10, 20, 30, ... 100) from the train set and used them to train the classifiers and used the test set for testing. I've replicated this 5 times to make sure I got some reliable results. Now, I want to evaluate the results. Any suggestions on a statistical test that I can use to prove that the proposed ml is better or not? Thanks. \", 'Tags': {'statistics', 'machine-learning', 'evaluation'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38407/predicition-for-a-specific-month', 'post': \" I am attempting to build a predictive model based on the past historical data. I have details of specific machine failure based on the past year data. I have data from some months of 2016 and from 2017 January to November. I am attempting to predict if the machine fails in December. I have attached the past historical data based on the data transformation I performed. I am stuck at a point on how to build a model to identify if the machine throws up any repair or replace in next one month.  I need to build a classification model which will identify if the machine fails. I don't understand on how to convert this multiple rows to single row record for each machine. Do I need to create categorical for day and month. How would I represent each sensor data column. Essentially look at the right way of representing data so that it is easier for classification to predict for next one month.  Any sort of guidance would be of great help dataset.csv train.csv train.csv contains the 1 month data of December for the machines which are already predicted. The target column is to predict in the month of December, if the machine requires a repair or replacement or no issue.  \", 'Tags': {'classification', 'machine-learning', 'prediction', 'time-series'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38404/which-ml-algorithm-to-use-if-we-have-categorical-data-numeric-data-derived-dat', 'post': '       Closed. This question needs to be more focused. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Update the question so it focuses on one problem only by editing this post.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         I am a beginner in Data Science. I have a data set which contains numerical data, categorical data and derived data (derived from other columns). The target column (dependent) is binary. Which Machine Learning algorithm will predict my target column most accurate?   ', 'Tags': {'supervised-learning', 'machine-learning-model', 'machine-learning', 'model-selection'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38400/sagemaker-deepar-access-data', 'post': ' Can SageMaker DeepAR algorithm only deal with training data that resides in Amazon S3 buckets? ', 'Tags': {'amazon-ml', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38395/standardscaler-before-and-after-splitting-data', 'post': ' When I was reading about using , most of the recommendations were saying that you should use  before splitting the data into train/test, but when i was checking some of the codes posted online (using sklearn) there were two major uses. 1- Using  on all the data. E.g.   Or  Or simply  2- Using  on split data.  I would like to standardize my data, but I am confused which method is best! ', 'Tags': {'scikit-learn', 'machine-learning', 'preprocessing'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38392/why-neural-networks-do-not-perform-well-on-structured-data', 'post': ' I was recently working on some classification problem where decision trees performed better than neural networks. I had tried various combinations with neural networks altering the number of neurons / hidden layers with an objective to beat the decision tree classifier accuracy on the test set. But the best accuracy I could achieve with neural networks was that of 0.42 and decision tree was at 0.50. I had asked a question here, as what could be the case and someone pointed out that neural networks do not work very well with the structured data (data in tabular format) as compared to the unstructured data (like representing each pixel in an image). In the comment linked to the same answer, it was pointed out that :  Well you can take a look at kaggle competition winners. In competitions containing structured data by far the most popular algorithm is xgboost (along with other similar algorithms lightgbm, catboost, etc.). On the other hand Neural Networks are rarely used in these competitions because they are not so strong with these types of data. This is also evident by the near 20-year disappearance of neural networks, until deep learning made them relevant again. During these years trees and SVMs on top.  That could generally be true but I do not have the intuition/reason as to why neural networks do not work well with the structured data? Could someone help me reason that? It will also be great if you could point me to some paper/post that explains this. One feeling that I have is, it could be because of the less volume of data. Neural networks might not generalize well with fewer data points as compared to other classifiers like decision trees, svms etc. But then I am not really sure about this. ', 'Tags': {'neural-network', 'supervised-learning', 'machine-learning', 'decision-trees'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38363/amazon-sagemaker-input-data', 'post': ' I am exploring Amazon SageMaker as a scalable machine learning solution. My question is; is it required that the training data first be uploaded in Amazon S3?  ', 'Tags': {'amazon-ml', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38355/which-model-may-be-best-for-outcome-of-a-surgery', 'post': \" New to data science and am trying to be a self-starter and implement advanced data analytics in my subspecialty of surgery. Below is a description of my data set. I know that I will have to explore multiple methods, but wanted to get your take on which you think may be best. I will most likely be using R to achieve this analysis.  Have a data set with about 200 patients (rows) Each patient has about 10-15 variables (preoperative and intraoperative) Each patient has undergone either nonoperative or operative management Success in nonoperative or operative management is determined by a questionnaire that patients fill out 1 year after they are seen. This questionnaire gives a binary outcome on whether they (1) Benefited or (2) Did not Benefit from the surgery.   My questions for the study are as follows:  In the surgery group, I am trying to find out which variables lead to patients (1) Benefit vs (2) Do not benefit from surgery, and create a model which can better help predict which patients we can operate on (I have left out some details such as patient population, type of surgery, etc). In the second study, I would like to determine which patients we should operate on. In other words, I would like to find out which preoperative characteristics make some patients more likely to benefit from (1) Operative treatment vs (2) Nonoperative treatment and in this case the outcome will also be binary from the questionnaire.  I have tried linear and logistic regressions for this which have not been very good, hence why I am trying to learn more advanced models.  Models which are easier to comprehend by clinicians are more valuable which is why I haven't delved into neural nets. I appreciate any and all advice that can be provided. In addition, if I expand this data set to 600 people, would you use another model? I don't have access to large servers so most of this will be done on my laptop though I can use online resources if necessary (Azure etc). Thank you all for your help and input.  \", 'Tags': {'machine-learning', 'predictive-modeling', 'model-selection'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38348/select-more-or-less-features-if-results-are-almost-the-same', 'post': ' I am having a dataset of 3500 observations with 70 features each with binary labels/targets for classifications purposes. My aim is to score more than 90% precision and the highest recall possible for this precision. I have tested many algorithms and thus far Random Forest performs the best.  Specifically, with 70 features I am getting:  Auroc: 0.71 Precision: 0.94 Recall: 0.18  However, I had a look at the importance of my features by calling the built-in function  of the  of  and by also using the Boruta algorithm with the  module.  I removed the 20 least important features. The number 20 was quite arbitrary because even the next 10 least important features were relatively close in terms of importance to these 20 least important ones. Only the top 10 most important features had a significant difference in importance in comparison with the others.  Then the results with 50 features were the following:  Auroc: 0.7 Precision: 0.91 Recall: 0.17  Since I can get almost the same results with less features should I remove these features? Please also into take into account that in the future my dataset will be expanded to 5000 observations. ', 'Tags': {'feature-selection', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38336/why-do-we-need-a-gain-ratio', 'post': ' I\\'m learning about decision trees, and I feel like up till now I\\'ve understood them and the math behind them pretty well except for one thing: the gain ratio. As I understand, the gain ratio is implemented in order to punish for features that may take on A LOT of possible values.  If a feature takes on a lot of possible values, it becomes plausible that if we split on that feature there may be values that only point to a single class, but simply because there are only 1 or 2 data points with that value for that feature anyways.  In other words, the only reason that we would get low entropy for splitting on that feature is because the feature could take on a lot of values, and therefore a lot of those values pointed specifically to a single label. So our decision tree algorithm would end up splitting up on something like \"ID #\", and wrongly calculate that we just had a HUGE information gain.  However, this only seems like a problem because \"ID#\" is a feature we shouldn\\'t be splitting on to begin with. I mean, if we had another feature that also took on a lot of possible values, but each of those values actually DID imply some label for that datapoint, then wouldn\\'t applying the gain ratio mean that we are actually messing up our decision tree by punishing what was actually a very good split with tons of information gain?  Isn\\'t it better to just identify which feature will have nothing to do with our labeling BEFORE we feed in the training data to the algorithm? IDK, I just don\\'t see why the gain ratio would really be useful... ', 'Tags': {'algorithms', 'machine-learning', 'decision-trees'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38328/when-does-decision-tree-perform-better-than-the-neural-network', 'post': ' I was experimenting with different modelling methods including KNN, Decision Trees, Neural Networks and SVN and trying to fit my data to see which works the best. To my surprise, the decision tree works the best with training accuracy of 1.0 and test accuracy of 0.5. The neural networks, which I believed would always perform the best no matter what has a training accuracy of 0.92 and test accuracy of 0.42 which is 8% less than the decision tree classifier.  Could someone please explain the circumstances/cases where neural networks could have low accuracy when compared to a modelling technique like the decision tree. I had tried my neural network with different configurations like:  but not in a single case, I found the neural network to beat the decision tree test accuracy of 50%. ', 'Tags': {'neural-network', 'machine-learning', 'python', 'decision-trees'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38305/how-to-chose-a-machine-learning-algorithm', 'post': \"       Closed. This question needs to be more focused. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Update the question so it focuses on one problem only by editing this post.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         I was wondering, are their any guidelines or any rules of the thumb as to which algorithms perform best for each task? What I'm looking for is something along the lines of:  NLP tasks are usually accompanied highly-dimensional and sparse data. Algorithm X performs well under these circumstances.  etc. \", 'Tags': {'machine-learning', 'machine-learning-model'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38291/why-is-the-logistic-regression-decision-boundary-linear-in-x', 'post': ' The logistic regression model, \\\\begin{equation} \\\\operatorname{p}(X) = \\\\frac{\\\\operatorname{e}^{\\\\beta_0 + \\\\beta_1 X}}{1 + \\\\operatorname{e}^{\\\\beta_0 + \\\\beta_1 X}} \\\\end{equation} is said to create a decision boundary linear in $X$. As far as I understand, only the logit is linear in $X$. Is this the reason the decision boundary is linear in X? If so, why? And if this is not the case, what is the reason for this phenomenon? I am confused about this because the decision boundary can be expressed as: \\\\begin{equation} \\\\operatorname{p}(X) = a, a \\\\in [0,\\\\, 1] \\\\end{equation} And $\\\\operatorname{p}(X)$ is not linear in $X$. ', 'Tags': {'logistic-regression', 'machine-learning', 'regression'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38286/neural-network-architecture-for-identifying-image-copies', 'post': ' I have a large image collection and wish to identify the images within that collection that appear to copy other images from the collection. To give you a sense of the kinds of image pairs that I wish to classify as matches, please consider these examples:  I have hand classified roughly .25M pairs of matching images, and now wish to use those hand labelled matches to train a neural network model. I am just not sure which architecture would be ideally suited for this task. I originally thought a Siamese Network might be appropriate, as they have been used for similar tasks, but the output from those classifiers seems more ideally suited to finding different figurations of the same object (which is not what I want), rather than different printings of the same figuration (which is what I want). If anyone can help recommend papers or architectures ideally suited to identifying images given the training data I have prepared, I would be tremendously grateful for any insights you can offer. ', 'Tags': {'neural-network', 'computer-vision', 'machine-learning', 'image-classification', 'convnet'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38284/what-is-the-one-hot-encoding-for-cancer-data-classification', 'post': \" I am working on a project to classify lung CT dataset using CNN and tensorflow, I know that the order for the category is cancer/no-cancer (only 2 classes), in more than one Github repository I see that they did one hot encoding like the code below:  what makes me confused is: 1 means cancer and 0 means no-cancer, as I understand it should be:  but why they did one hot encoding like this, I don't know I am wrong or there is another thing that I did not understand, can anyone explain it for? or give me a better way to do encoding for my data, but with code? \", 'Tags': {'tensorflow', 'machine-learning', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38267/binary-text-classification-with-tfidfvectorizer-gives-valueerror-setting-an-arr', 'post': \" I am using pandas and scikti-learn to do binary text classification using text features encoded using TfidfVectorizer on a DataFrame. Here is some dummy code that illustrates what I'm doing:  This gives the following error:  I have found numerous posts (e.g. here, here) mentioning that this error can indicate non-uniformity of the data. This post for the same error suggests it can also be due to a data typing issue. However, I can't see how my very simple example could be due to either of these. There is surely something simple I am missing. Help! \", 'Tags': {'scikit-learn', 'pandas', 'machine-learning', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38260/feed-forward-neural-network-not-training-with-keras-function-generators-deep-lea', 'post': '       Closed. This question needs details or clarity. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Add details and clarify the problem by editing this post.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         I am using feed-forward neural network for a classification task. My data is 1 million examples for 9 classes (imbalanced). Due to memory constraint in Keras, I used function generator to generate batches automatically with a batch size of 200. I trained a simple model with 3 hidden layers with ReLU activation function. Input layer is 39 dimensional MFCCs and output is 9 classes. This model worked fine when I used a subset of this huge data (Sure!) but now, while using function generator i.e model.fit_generator, I see that training accuracy is just wandering around and validation accuracy is just too low. Looks like the model is not learning at all. What might be the possible reasons for this behaviour? about data Data : Speech subset of huge data mentioned was totally clean but I generated 1 million examples from 1300 examples using data augmentation techniques like equalization, time stretch, time compression, noises, reverb etc ', 'Tags': {'neural-network', 'keras', 'machine-learning', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38258/what-are-the-differences-between-logistic-and-linear-regression', 'post': ' I know that linear regression does \"regression\" and logistic regression does \"classification\". When we implement these two methods, the only difference I could notice is the loss function: linear regression uses a loss function like mean square error and logistic uses cross-entropy. Is there any other difference that I am not aware of?  ', 'Tags': {'neural-network', 'loss-function', 'machine-learning', 'linear-regression', 'logistic-regression'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38253/a-classification-machine-learning-flow-chart-implimenting-dimentionality-reducti', 'post': '       Closed. This question is opinion-based. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         I would like to make a flow chart for an ML classifier and make sure that my thinking is correct. Here is a little about my sample: I have 3 classes and about 160 features. I suspect that some of those features are redundant. My classes are imbalanced. Please let me know if I should provide more info about the sample. This is my first try on tackling a machine learning problem. Based on checking the functionality of each of the options I use below, I chose it in that specific order, but i might be wrong. Could someone just let me know if this flow chart is logical, and if there was a problem in any of the step, could you possible tell me why it is not supposed to be there? Flow chart  Get the data Make it in a readable format and import it Split the data into train/test Standardize with StandardScalar Perform PCA on train data Perform K-fold cross-validation on train data Apply SMOTE Predict (Using SVC) Make a confusion matrix Perform GridSearchCV Perform PermutationImportance  ', 'Tags': {'cross-validation', 'machine-learning', 'smote', 'pca'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38234/rnn-based-predictions-of-sine-waves-with-frequency-different-from-training-data', 'post': ' I am wondering if I can generate a sine wave with a frequency different from training data using RNN. For example, Using two training data of two time series, say 0[sec] ~ 10[sec] each: sin(t) and sin(2t) If we feed these two data to RNN, can we possibly predict a sine wave of sin(1.5t) with some amount of initial data? I would be very appreciated if you could tell me the possibility of this, and perhaps some examples of implementation. Thank you in advance ', 'Tags': {'time-series', 'rnn', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38233/why-isnt-local-averaging-including-knn-used-often-for-regression', 'post': ' My professor said that the \"holy grail of regression\" is the function E(Y|X=x) i.e. the conditional expectation of Y on X. In practice, you\\'d take a small window of X and take the average value of Y for all observations that lie in the window. The professor said that this is basically the best prediction you can make, but we don\\'t usually do it because the curse of dimensionality reduces its effectiveness in when # of predictors is large. So it seems that local averaging (KNN regression is a type of this) is good with few predictors. However, in most articles and stats classes, I always see linear regression being used even in low dimensions. Why isn\\'t local averaging used more often? ', 'Tags': {'linear-regression', 'statistics', 'machine-learning', 'k-nn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38227/classification-regression-problem-where-response-variable-is-ordinal', 'post': \" Data Science novice here! I'm trying to work on the white/red wine quality data set, where I 'm trying to predict the quality of the wine. All the features are numerical.  The response variable however, is ordinal with a quality score of integers 1 through 10 . I have seen tutorials try to group the scores ie: (0-4: Bad, 5-7: Good, 8-10: Great), but what if I wanted to predict the score as it is? Should I use a regression approach where I try and minimize the error of my predicted scores versus actual scores?  Or should I use a classification model anyways and instead of calculating a F-score to evaluate the model, find the model that minimizes a cost function? Or perhaps there is another approach that works best? \", 'Tags': {'classification', 'machine-learning', 'regression'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38217/whats-the-best-metric-for-evaluate-an-estimator-for-a-multi-class-problem-with', 'post': ' accuracy, precision, f1, ROC are good for binary single class problem. but for more complex problem (imbalance multi-class problem), what should i use? Do you have any recommendation? ', 'Tags': {'metric', 'machine-learning', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38213/pass-2-different-kinds-of-x-training-data-to-ml-model-simultaneously', 'post': \" I'm trying to classify if a book is fiction/nonfiction based on title and summary. This is 2 distinct types of information - is there a way to segment  and  before feeding it to a model, rather than concatenating the information? For example: Title:  Summary:  Label:  (where fiction =1) Current procedure: What I've been doing until now is concatenating the information, so the above becomes,  Then the usual setup, something like,  But feeding the data concatenated feels intuitively wrong. Is there a better way to do this? If for some reason its possible with a library other than sklearn (keras, tensorflow) I'd be also open to hearing about that.  UPDATE Going from,  to,  causes errors to be thrown.  is X is a list, and  if X is an array. The error is thrown when I call,   Is it possible to pass in a vector of features? \", 'Tags': {'scikit-learn', 'machine-learning', 'python', 'nlp'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38203/var-model-valueerror-x-already-contains-a-constant', 'post': \" I'm using VAR model for multivariate time series. The structure is that although each variable is a linear function of past lags of itself and past lags of the other variables, one and/or two of the variables MAY NOT alter within the period under investigation. Out of 10 variables.  Below is a similar dataframe to the one I'm working on. The actual dataset has 190 rows.   Is there a way to fix this? \", 'Tags': {'machine-learning', 'statistics', 'python', 'time-series', 'forecasting'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38198/how-to-decide-the-shape-of-input-features-when-each-data-file-is-of-different-l', 'post': ' To help me understand the benefits and shortcomings of , , , I wanted to build a simple classifier that classifies into 2 classes ( and ) using all of the above 3 methods.  So I downloaded a sound dataset from kaggle and was exploring pysoundfile as a module to read the sound file. So the following stub:  returns a numpy ndarray. The  of data varies with each file, some being of  and some being . Since each file is of different length the shape of  varies with each file. What are some ways, I can make the shape of  equal? I was planning to have the shape of data set to the smallest length from all the files. But it definitely does not do justice to the sound file dataset. I could lose so many features and so the model accuracy at the end. ', 'Tags': {'neural-network', 'machine-learning', 'decision-trees', 'python', 'k-nn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38192/help-with-creating-dimensions-features', 'post': ' It is quite hard to name the title properly as I just started to learn ML, will try to explain here. I want to practice ML by creating Movie suggestion algorithm. I came up with the following list of dimensions/features:  Rating Number of votes Genre Actors Directors Writers Year Combination of Actors/Directors/Writers working in the team Combination of Year and Actor or Director or Writer Actor or Director or Writer working in some genre  There is no problem for me for numeric fields, but in case of actors I have multiple values. How to create feature for the actors? ', 'Tags': {'feature-selection', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38189/image-recognition-of-selfie-images', 'post': \" I developed an Android app that lets anyone upload pictures of encyclopedic things (bridges, museums, dishes, landscapes, paintings, etc) to Wikimedia Commons. Unfortunately, 5% of the users find it funny to upload their own selfie. So I want to programmatically check whether the picture is a selfie or not, and if it probably is, warn them that selfies are off-topic. As a data set, I have:  1000 pictures that I consider as undesirable selfies. It is in part subjective, but usually such pictures show one or two human faces taken from an arm's distance and random backgrounds. 1000 pictures that are not selfies (bridges, museums, dishes, etc, anything really). Tricky: this also includes pictures of famous people, usually they are easy to distinguish from a selfie because the persons are at a further distance. If you see an extended arm then you can be sure it is a selfie.  All pictures are taken with smartphones (hundreds of different models), they are JPG files of 2MB to 5MB in various sizes and ratios, in portrait or landscape mode. I must use only open source, and the resulting detection code must run in less than a second on low-end Android phones. What approach and steps does this task call for? \", 'Tags': {'computer-vision', 'machine-learning', 'image-classification'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38186/how-to-use-tensorflow-graphs-in-multithread-valueerror-tensor-a-must-be-from-the', 'post': ' I am doing instance detect and image retrieval task by Keras and Tensorflow as backend. I plan to use multi thread to load two model, I load maskrcnn in a thread and load mobile net in another one. I load the maskrcnn in a thread successfully, but I failed to load mobile net in another thread, and:  show:ValueError: tensor a must be from the same graph as tensor b.  The code is as below: Merge.py  Thread1.py  Thread2.py  Where am I going wrong? ', 'Tags': {'tensorflow', 'machine-learning', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38185/how-can-there-be-more-true-positive-than-positive', 'post': ' Currently reading  Learning from Little: Comparison of Classifiers Given Little Training In 3 Experiment Results, the following graph is shared:  The experiment is described as follow  We begin by examining an example set of results for the average TP10   performance over all benchmark tasks, where the training set has P=5   positives and N=200 negatives.We vary the number of features selected   along the logarithmic x-axis.  I understand this as \"we use a training set of 205 elements, 5 being positives and the 200 remaining being negatives\".  But looking at the results, Naive Bayes using Information Gain with a few features, and Multinomial Naive Bayes using Bi-normal separation with several hundred features  both end up with 6.5 true positives in the top 10 (The TP10 metric is the number of true positives found in the 10 test cases that are predicted most strongly by the classifier to be positive). I would have assumed that the models being closest to 5/10 would be the most accurate, but reading their results, it looks like the higher the better. Thus, it feels like I overlooked and misunderstood something. Can somebody enlighten me on this issue ? Thank you ', 'Tags': {'research', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38181/how-to-choose-negative-examples-for-recommendation-system', 'post': \" I am building a search recommendation system for e-commerce which generates most relevant results given an input query. I have framed it as a classification problem (learning to rank) and using pointwise ranking to compute relevance scores. Let us say for every input query Q1, we have to generate 30 most relevant results. Suppose, a user initiates a search query and chooses the 3rd result as a relevant result. My question is how should this interaction be used to generate training dataset for the model :  Q1 - Result 3 : Positive Label Q1 - Result 1,2 : Negative Label (User obviously saw result 1,2 and didn't like it. Also mentioned as skip-highs in some papers/blogs). Q1 - Random : Negative Label (just add some random results to add random negatives to the dataset).  What about ranks 4-30. It's possible user saw ranks 4-8, but after those ranks user didn't even see the results. Should these lower ranks be used to generate implicit negatives to feed to the model ? Pros  Random sampling from implicit negatives in lower ranks will ensure that the model is trained on the entire feature distribution that it would have to score during predictions. If we just use the explicit positives/negatives from Step 1/2, model would have been trained only on very relevant results. Sampling from lower ranks will ensure that the model will be trained on entire spectrum of relevant results too.   Cons  We are adding noise to the training dataset, since the user didn't even interact with these results.   So, the main question is should we include the results from rank 4-30, which were probably not even seen by the user as negative examples in the training dataset or not? \", 'Tags': {'machine-learning', 'deep-learning', 'recommender-system'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38178/using-ontology-to-infer-labels-for-process-model', 'post': ' I\\'m trying to implement a specific type of process mining, that has been presented in this thesis [link]. It is based on HMMs and generates a process model in form of a directed graph, where:   Nodes are called intentions and correspond to hidden states Edges are called strategies and consist of different activities These activities correspond to the HMM\\'s observable emissions Intentions can be fulfilled using different strategies   A user event log consisting of user IDs, timestamps and activities is used as input. The image below is an example of such a process model. The highlighted nodes and edges resemble the path that has been predicted using the Viterbi algorithm.  You can see that the graph\\'s nodes and edges only carry numeric labels, which allow to distinguish between the different strategies and intentions. In order to make these labels more meaningful to the human reader, I\\'d like to infer some suitable labels.  My idea is to use an ontology to obtain those labels. After some research I figured out that I probably needed to do something that is generally referred to as \"ontology learning\". For this I would need to create some axioms in RDF/OWL format and then use these as input for a reasoner, that would infer an ontology. Is this approach correct and reasonable to achieve my goal? If this is the way to go, I will need some tool to generate axioms in an automated way. So far I couldn\\'t find any tool that would do that completely out-of-the-box. Based on what I\\'ve seen so far I conclude that I would need to define some kind of mapping between the original data and the desired axioms. I took a closer look at protégé, which offers a plugin for spreadsheets. It seems to be based on the MappingMasterDSL project [link]. I\\'ve also found an interesting paper [link] on ontology learning where an RNN-based model is trained in a end-to-end fashion to translate definitory sentences into OWL formulae. BUT: My user event log data does not contain any natural sentences. Its activities are defined by tokens derived from HTML elements of the user interface. Therefore the RNN-based approach does not seem to be applicable here. (For the interested reader, the related project can be found here [link]) Isn\\'t there really any easier way than hand-crafting the axioms\\' schema(ta)? Assuming that I have created my axioms and inferred an ontology, I would like to use the strategies\\' (edges\\') observable activities (emissions) to infer a suitable label. I guess I would need to query my ontology somehow. I could use the activity names as parameters for my query and look for some related entities that reveal the desired label. I\\'m expecting something like:   \"I have a strategy with , that strategy can be executed with   actions ,  and , give me all entities of the ontology, that   have these actions as property value and show and give me all related   labels for those entities\"  But where would the data for the labels actually come from? I think I\\'m missing some important step during the process of ontology learning. Where do I find an additional data source for the labels and how do I relate this data to my ontology\\'s entities?  Also I\\'m wondering if there is a way to incorporate the inherent knowledge of the process model\\'s topology into my ontology. ', 'Tags': {'information-retrieval', 'machine-learning', 'labels'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38174/training-on-accurate-data-versus-noisy-data', 'post': \" I have data currently available that is very accurate and I would like to train my classification methods on this set of clean data to learn the important markers for distinguishing between classes. But in the future, my trained classifiers will not be seeing and performing decisions on this cleaned data; instead, it will likely have a lot more noise following some unknown distribution(s). Thus I am wondering, is it 'better' to train on noisy data if I'm going to likely see noisy data in the future, or train on good data since the noisy data should (ideally) correspond to the cleaned data if noise was removed?  Intuitively, if my goal is to simply perform classifications, then training on noisy data seems the 'better' approach since this is better representative of my expected future inputs. But if my goal is to learn about the data and what constitutes a particular decision, then training on cleaned data appears the 'better' approach.  But am I overlooking anything? Would training on the clean and/or noisy data be preferable for different reasons (e.g. generalization, overfitting, reducing dimensionality)?  \", 'Tags': {'error-handling', 'generalization', 'machine-learning', 'noise'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38164/normalize-data-with-uneven-groups', 'post': \" I have a dataset with 3 independent variables [city, industry, amount] and wish to normalize the amount. But I wish to do it with respect to industry and city. Simply grouping by the city and industry gives me a lot of very sparse groups on which normalizing (min-max, etc.) wouldn't be very meaningful. Is there any better way to normalize it? \", 'Tags': {'machine-learning', 'preprocessing', 'feature-scaling'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38145/in-machine-learning-how-to-find-feature-interdepencies', 'post': '       Closed. This question needs details or clarity. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Add details and clarify the problem by editing this post.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         Given a data set of N features, wherein some the features in this set were derived from other features from the same set, I am trying to discover inter dependencies between features (something like this Input feature(s) -> output feature(s)). Note that,there can be multiple dependencies in the same feature set. Can someone suggest some technique to approach this problem. ', 'Tags': {'statistics', 'feature-selection', 'feature-extraction', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38138/what-is-the-difference-between-missing-at-random-and-missing-not-at-random-data', 'post': \" I have been working with a dataset where the missing data seem to following a few particular patterns. I have gone through a lot websites and articles related to missing data but I haven't been able to understand the difference between MAR and MNAR. First I would like to give the description of the dataset and the patterns of the missing data. So it would be easier for you to explain the difference between the two and also it would enable to identify the what are those patterns in my dataset. Description of the dataset: It is a transaction history dataset of a cycle company(Imaginary company) which contains descriptions about customers like their name, DOB, gender, geographic location, Income, social status and the transaction details like the cycle brand name bought, size of the cycle(small,M,L), type of cycle like (Mountain, Road and Standard), Product manufacturing cost, List Price of the product and finally the transaction date. Pattern in the missing data: There are basically four different patterns of missing data that I have identified in the dataset: Pattern 1: This is related to product details in the transaction history: Whenever there is some missing data about say the brand name then the details regarding Product line, Product class, Product size, standard cost and Product_first_sold_date are also missing but the list price alone is available but then these list prices seem unique as they cannot be found repeating in the same column. This could blamed on the company for not recording the details properly but there is actually no such company that exists and it is upto me to deal with. Pattern 2: Whenever there is missing data about a customer say first name then all the other columns about the customer are also missing like Gender, Past_3_years_bike_related_purchases,Job_Industry_Category, Wealth_Segment,Deceased_Indicator and Owns_Car. Only the customer ID in this case is available. Probably this could be taken as the customer didn't want to disclose the details but then again it is upto me to deal with. Pattern 3: Whenever there is some missing data about the geographic location of the customer like say the Address then the Post code, state,country and property evaluation column data are also missing but the general customer details like name, gender, DOB are all available. Probably in this case the customer didn't want to reveal their geographic details and I have got to deal with it. Pattern 4: Most interesting one: The gender column has three categories M, F and U. U can be taken as not disclosed. Whenever the gender is U then their age and tenure are also missing. This could be taken as those not ready to disclose their Gender are not ready to disclose their age and tenure. Sometimes some patterns occur together.  What category of missing date like(MAR, MCAR and MNAR) do these fall into? And how do I deal with it. Any suggestion would be extremely helpful. Thanks. \", 'Tags': {'machine-learning', 'r', 'missing-data', 'data-imputation', 'data-mining'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38137/multiple-output-from-a-model', 'post': ' I have dataset where input is userId and day and Output is project activity and hour, each day some users are reporting for two projects, two activities and 4 hour each and some users only report for single project and activity. I want output  for those users who reports for more than 1 project and activity a day,, i am not getting how to do and which algorithm to apply Any help? ', 'Tags': {'machine-learning', 'multiclass-classification', 'machine-learning-model'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38117/spatial-transformer-networks-how-is-theta-differentiable', 'post': \" In the paper , the localization network's output, theta, is differentiable, given the current input feature map. How is this theta differentiable? \", 'Tags': {'neural-network', 'machine-learning', 'deep-learning', 'spatial-transformer', 'cnn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38111/classification-problem-with-many-images-per-instance', 'post': ' I am working in the following kind of classification problem: I have to classify every instance as class A or class B using many images of the instance. That is, every training example has not one image (which is the usual thing in image classification), but many images, and the number of images for every training instance is not fixed. That is, instance 1 can have 3 images, and according to these images we have to classify it as A or B, and instance 2 can have instead 5 images. As any machine learning problem, I am provided with many labelled images and I have to build a classifier. Although ideas are also welcome, I am looking for a documented way to attack this kind of problem (Kaggles, papers or books, mainly). My main idea was the following: train a model $f$ that given one image gives a probability of that image being of class A. Then, for every training instance, evaluate $f$ in every image of the instance and compute statistics (aggregate) of the distribution of these probabilities, as the mean, median, maximum and minimum. Then, train a model $g$ that has as inputs these aggregates and use the composition of $f$, aggregates and $g$ as the final model. This idea is a bit simple so I am looking for something better. ', 'Tags': {'computer-vision', 'machine-learning', 'cnn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38108/what-to-do-if-the-missing-data-in-one-column-is-based-on-some-value-condition-in', 'post': \" I have a dataset with 20,000 observations and 19 variables. To start off with I have a gender column which has three levels namely 'M', 'F' and 'U' where U can be taken as not disclosed. Whenever there is a 'U' in the gender column, there is an NA in two of the other columns namely Age and Tenure. This could basically be interpreted as a person who is not ready to disclose their Gender is not ready to disclose their age and tenure. How do I deal with such a situation? Apart from these three columns there are other 16 columns in the dataset that have got meaningful data in them. Would the normal imputation techniques out there like a KNN Imputation help me out in such a case? Here is my reproducible example that I have tried my best with:  The Dataframe:  As you can see from the example above whenever the gender is undefined, there are missing values in both age and tenure and this is the case overall in the entire dataset. What would be the best way to deal with such a situation? And this what is called a Missing at Random data, is that right? Any suggestions would be extremely helpful. Thanks a lot. \", 'Tags': {'machine-learning', 'r', 'missing-data', 'data-imputation', 'data-mining'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38098/can-we-change-the-structure-of-the-feature-extraction-layers-of-deep-networks-ar', 'post': ' I want to know if we can change the structure of the feature-extraction layers in the deep networks architectures, for example can we add more Rectified Linear Unit (ReLU) activation function or is it the same sequence that we should respect ? ', 'Tags': {'machine-learning', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38096/backpropagation', 'post': \" I use chain rule when doing backpropagation and then I do Gradient Descent with weighting coefficient and I am updating the weight, so I do not understand how the method works in the equations below. \\\\begin{align*} Z^{[1]}&=W^{[1]}X+b^{[1]} \\\\\\\\ A^{[1]}&=g^{[1]}\\\\left(Z^{[1]}\\\\right) \\\\\\\\ Z^{[2]}&=W^{[2]}A^{[1]}+b^{[2]} \\\\\\\\ A^{[2]}&=g^{[2]}\\\\left(Z^{[2]}\\\\right) \\\\\\\\  \\\\vdots & \\\\\\\\ A^{[L]}&=g^{[L]}\\\\left(Z^{[L]}\\\\right)=\\\\hat{Y} \\\\\\\\ \\\\\\\\ dZ^{[L]}&=A^{[L]}-Y \\\\\\\\ dW^{[L]}&=\\\\frac1m\\\\,dZ^{[L]}{A^{[L]}}^{T} \\\\\\\\ db^{[L]}&=\\\\frac1m \\\\,\\\\;\\\\operatorname{np.sum}\\\\left(dZ^{[L]},\\\\;\\\\operatorname{axis}=1,\\\\;\\\\operatorname{keepdims}=\\\\operatorname{True} \\\\right) \\\\\\\\ dZ^{[L-1]}&={dW^{[L]}}^{T}dZ^{[2]}g'^{[1]}\\\\left(Z^{[1]}\\\\right) \\\\\\\\ dW^{[1]}&=\\\\frac1m\\\\,dZ^{[1]}{A^{[1]}}^{T} \\\\\\\\ db^{[1]}&=\\\\frac1m \\\\,\\\\;\\\\operatorname{np.sum}\\\\left(dZ^{[1]},\\\\;\\\\operatorname{axis}=1,\\\\;\\\\operatorname{keepdims}=\\\\operatorname{True} \\\\right). \\\\end{align*} (Source Image) \", 'Tags': {'neural-network', 'machine-learning', 'deep-learning', 'gradient-descent', 'backpropagation'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38094/algortihm-for-making-predictions-from-minimal-data', 'post': \" I am working with classification problem.  I have a dataset with a lot of features. A lot of them can easily determine class. On production, I want to ask the user to provide me with only part of the information.  Which algorithm is able to make pretty accurate predictions from incomplete data?  I was thinking about decisions tree, but I don't want to determine the entry point to my algorithm. Suppose that my features are binary. I want to give choice to my user which features are most common for him and based on that information make pretty accurate predictions. \", 'Tags': {'data', 'dataset', 'machine-learning', 'machine-learning-model'}}\n",
            "page number ------------------------------------90\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38080/interactive-labeling-annotating-of-time-series-data', 'post': \" I have a data set of time series data. I'm looking for an annotation (or labeling) tool to visualize it and to be able to interactively add labels on it, in order to get annotated data that I can use for supervised ML. E.g. the input data is a csv-file and the output is another csv-file of the format timestamp,label. Therefore I need something like this:  to visualize data  to select a specific area output the labels with timestamps  As an example:  Building such a tool in python will not take too long, however I was just wondering how other people solve this problem and maybe there are already nice OS tools for doing this. Thank you! \", 'Tags': {'data', 'machine-learning', 'python', 'labels'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38070/given-youtube-videos-is-there-a-face-recognition-technology-to-detect-faces-in', 'post': ' Given Youtube Videos, is there a face recognition technology to detect faces in all new videos and categorize them in a database for future use? My research showed that especially rate limits were a problem with the api. Apart from that, given the huge amount of videos youtube uploads, processing power is another. ', 'Tags': {'classification', 'bigdata', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38061/why-does-exploration-in-dqn-not-lead-to-instability', 'post': ' Why does action exploration in DQN not lead to instability? I see in DQN algorithms, that it selects random actions even after some iterations.  My question is how does this approach not lead to instability? Even the final value of epsilon (the probability of a random action) is non-zero! If we set the final epsilon=0.01, we will select a random the action one time in a hundred, which means that we are not going to get the same answer in different runs and probably our answers are not stable. Why does this still work? ', 'Tags': {'neural-network', 'dqn', 'training', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38054/is-there-a-dataset-with-news-articles-and-their-headlines', 'post': \"       Closed. This question is off-topic. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Update the question so it's on-topic for Data Science Stack Exchange.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         I need a set of news headlines and articles to help me in a project on automatic summarization. Is there such a dataset or something similar? \", 'Tags': {'machine-learning', 'nlp', 'automatic-summarization', 'machine-translation'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38048/how-design-a-autoencoder-architecture', 'post': ' I would like to build an autoencoder (CNN) to learn a representation of my data. I never built such a network and I have some experience in supervised learning (classification). I would like to know if some good practices in training a classifier is also right for an autoencoder:  Does reference architecture exists like ResNet/Inception or something? If not, should I design manually layers? Does transfer learning/fine tuning works for autoencoder (or is it better to train from scratch)?  ', 'Tags': {'neural-network', 'computer-vision', 'machine-learning', 'deep-learning', 'autoencoder'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38042/classifying-car-data-by-year', 'post': ' I have huge car photos. I want to predict car\\'s \"brand-model-body type and production year\"  First, I splitted data into train and validation, and I categorized them like this.  Every category has about 1000 train and 900 validation images. My plan was: I train my keras model with these categories after training, model can predict labels like below: audi a3 sedan 2008  => %25 audi a3 sedan 2009  => %25 audi a3 sedan 2010  => %25 audi a3 sedan 2011  => %25 And I can tell user that: \"This car is Audi A3 Sedan 2008-2011\" My problem is, some of these categories have very similar photos. For example: audi a3 2009 and audi a3 2010 have same body type and there is not much difference between photos (No difference in reality).  Because of that, train accuracy has improved to about 0.9 but validation accuracy hasn\\'t improved above 0.55 When I try some predictions, it usually gives same label, \"Ford Focus sedan 2009\" :) Here is my output:  My train code is here:   Am I doing something wrong? How can I achieve this result? Should I change validation accuracy calculation, or should I give more photos per category?  ', 'Tags': {'computer-vision', 'keras', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38041/having-the-target-and-data-row-how-to-find-the-correct-model', 'post': \" I am coming from a web developer background, and having no data science background.  I am looking to skill-up and get some answers for a current task: find a model that returns a predicted value same as the target.   I am looking for guidance how to get going and improve this. I loaded the training data into BigQuery and run the BQ ML proceess on it, but it yielded a bad accuracy, so that's why I am here to get some answers what to try next.  suggest me good tags \", 'Tags': {'data', 'machine-learning', 'predictive-modeling'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38028/check-if-answer-for-a-question-is-correct-by-similarity', 'post': ' Let\\'s say in an NLP problem, I have a question and some correct answers to that question (say, 10 correct answers).  Is there a way to get a new answer as input, and \"calculate\" whether it is correct?  ', 'Tags': {'ai', 'machine-learning', 'nlp'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38027/what-is-parts-of-speech-technique-in-sentiment-analysis', 'post': \" In an article, I saw Sentiment Analysis using Parts Of Speech(POS) technique. When I searched I got some paper on POS but I couldn't understand what POS basically is. Though I am new to sentiment analysis please help me to understand POS. \", 'Tags': {'sentiment-analysis', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38024/making-inferences-from-incomplete-data', 'post': \" I have data which have complete information. Each record has one class assigned. On production, I won't be able to get so many information from a user, so I want to create a model which will be able to make inferences from incomplete data and making more and more reliable predictions with getting more information from a user. For Example: Say,  Training data - 200 binary features, In rows in average 11 of them are marked as 1, another 189 as 0.     Production - We give our user     list of features to mark which of them is most common(Let's suppose     that user picked 3 of them), then I want to make some inferences and     ask good questions to get more accurate predictions.  My second problem is that my dataset assumed that user can belong only to one class, whereas in fact user on production can be multi label.  Say, in my dataset there is situation that: -class 1 has 5 features annotated as true at random(allways the same), rest always false -class 2 has 5 DIFFRENT features annotated as true at random(allways the same), rest always false Notice, I have a list of possible features for each class, so I can easily calculate conditional probability of class depends on features, so I tried to use Naive Bayes, but my features are not independent.  On production can occur situation that user will be in class 1 and 2 and will have some features from class 1 and some from class 2. I want to that my algorithm will figure it out that it is in class 1 and class 2. What algorithm should I use? I was thinking about decision trees, but in that, I cannot give user choice what is the most common for him and in that situation. Thanks in advance for your help! \", 'Tags': {'dataset', 'machine-learning', 'machine-learning-model'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38023/machine-learning-take-into-account-a-variable-if-a-condition-is-met-depending', 'post': \" I am working on a set of student data to train some models. I have the gender variable and I can also retrieve data about how many other girls go to this student's class. I would like to take into account in the modelling that if it is a girl, whether there are other girls in her class or not. This variable shouldn't have any interference in the cases where the student is a boy. Any idea how I should treat this? \", 'Tags': {'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38022/are-word-embeddings-further-updated-during-training-for-document-classification', 'post': ' I am relatively new to the area of using word embeddings in NLP tasks. From a large corpus of documents, I train word2vec word embedding vectors and afterwards I am going to use these for document classification, combined with RNN based classifiers (LSTM, GRU), which is a pretty standart pipeline nowadays. There is one issue; that should we update the word embeddings during the document classification training as well. I am used to tasks like image classification/object detection in the past. You get an image input and the convolution features extracted from that image are updated during the numerical optimization of a CNN. But the image itself is never updated, naturally, since it is the original data. How do we treat the embedding vectors in the world of text documents? They are not \"natural\" exactly like the images, we learn them from an unsupervised method first (word2vec, GloVe or any other tool) so I think they can be further fine-tuned during the supervised training. Is it the common practice to update the embedding vectors as well as the RNN parameters during the training of the sequence classifier or should we leave them as constant (in order to avoid overfitting) ? ', 'Tags': {'word2vec', 'rnn', 'machine-learning', 'nlp'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/38019/use-machine-learning-to-predict-a-next-day-of-visit-for-customer', 'post': ' I have a problem a need your suggestion , I am working in a retail data , and want to predict the behavior of the customer , the data contains information about the customer who visits the shopping center, it only contains the customer unique identifier and the customer visits for 143 weeks so for each record I have visitor Id and visits , the visits contains numbers such as 1 5 30 ..etc , 1 is the first day of the first week which is Monday then 5 means Friday , then 30 means Tuesday for the fourth week and etc , what I am trying to do is to predict when the first day of the visit for the next 144th week can you please help me in this This is the data format  ', 'Tags': {'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37999/why-does-a-machine-learning-algorithm-need-a-bias', 'post': ' The first line of section 2.7.3 in Mitchell\\'s Machine Learning is:  \"A Learner that makes no prior assumptions regarding the identity of the target concept has no rational basis for classifying any unseen instances.\"  Why is it that a machine learning algorithm needs a bias? Can someone please help me understand this, with examples perhaps?  ', 'Tags': {'classification', 'machine-learning', 'predictive-modeling', 'machine-learning-model'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37990/is-there-any-consensus-on-choosing-an-appropriate-ml-approach', 'post': ' I am studying data science at the moment and we are taught a dizzying variety of basic regression/classification techniques (linear, logistic, trees, splines, ANN, SVM, MARS, and so on....), along with a variety of extra tools (bootstrapping, boosting, bagging, ensemble, ridge/lasso, CV, etc etc). Sometimes the techniques are given context (eg. suitable for small/large datasets, suitable for a small/large number of predictors, etc) but for the most part, it seems like for any regression or classification problem there exist a dizzying array of options to choose from. If I started a job in data science right now and was given a modelling problem, I don\\'t think I could do any better than just try all the techniques I know with basic configurations, evaluate them using cross-validation and pick the best. But there must be more to it than this. I imagine an experienced data scientist knows the catalogue of techniques well and follows some mental flowchart to decide which techniques to try, instead of mindlessly trying them all. I imagine this flowchart is a function of a) number of predictors; b) variable types; c) domain knowledge about possible relationships (linear/non-linear); d) size of the dataset; e) constraints around computation time and so on. Is there any such agreed on, conventional flowchart to follow, to choose the techniques? Or does it really boil down to \"try lots of things and see what works best on the desired measure eg. cross-validation\"? ', 'Tags': {'neural-network', 'machine-learning', 'deep-learning', 'classification', 'data-science-model'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37988/how-to-shuffle-input-data-using-stochastic-gradient-decent', 'post': \" Let's say that I have input with size $n\\\\times n$ and after that I operated shuffled operations and get the output with the same dimension. But I don't know if we can do that or not. The idea is to permute all the configuration and find the minimum configuration when given loss. But the operation became very expensive with the growth of $O((n^2)!)$, and with each iteration we need to compute loss separately. So is there an optimization for such problem or it just not feasible? In other words, is there a gradient correlation between different configuration that will lead to the local minima of the loss function? \", 'Tags': {'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37987/why-imbalanced-data-set-will-bias-the-prediction-model-towards-the-more-common-c', 'post': \" As we know, an imbalanced data-set has a disadvantage of training a model for deep learning. However, I don't know how to explain it with mathematics? \", 'Tags': {'neural-network', 'optimization', 'machine-learning', 'deep-learning', 'bigdata'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37984/what-does-the-normalization-factor-mean-in-the-adaboost-algorithm', 'post': \" I am studying the AdaBoost algorithm. The update rule for a weak hypothesis is: $Dt+1(i) = Dt(i)exp(−αtyiht(xi))/zt $ where $zt$ is a normalization factor chosen so that $Dt+1$ is a distribution.  What does the 'normalization factor' mean? Could I have an explanation with an example, please? \", 'Tags': {'distribution', 'machine-learning', 'boosting', 'statistics', 'descriptive-statistics'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37979/check-if-answer-is-correct-by-similarity', 'post': ' I am new to data science and machine learning.  Let\\'s say that I have a question, and some correct answers for that question (for example, 10 correct answers). Is there a way to get a new answer as input, and \"calculate\" whether it is right? If you can recommend some readings, that would be great. ', 'Tags': {'ai', 'machine-learning', 'nlp', 'natural-language-process'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37975/what-is-the-intuition-behind-using-2-consecutive-convolutional-filters-in-a-conv', 'post': \" I understand the purpose of Convolutional filters (or kernels). I visualize them as learnable feature extractors. E.g. Extract vertical edges or horizontal edges, etc.  Could somebody kindly explain to me the intuition behind stacking 2 or more consecutive convolution filters? Why couldn't the 2 filters be merged into 1? Picture from AndrewNg's video   AndrewNg's video I have hyperlinked Andrew Ng's lectures on Machine learning as a reference https://youtu.be/bXJx7y51cl0?t=6m15s \", 'Tags': {'neural-network', 'machine-learning', 'deep-learning', 'feature-extraction', 'cnn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37962/what-is-the-difference-between-multi-layer-perceptron-and-generalized-feed-forwa', 'post': \" I'm reading this paper:An artificial neural network model for rainfall forecasting in Bangkok, Thailand. The author created 6 models, 2 of which have the following architecture:  model B:  with  activation function and 4  in which  are: 5-10-10-1, respectively. model C:  with  activation function and 4  in which  are: 5-10-10-1, respectively. In the Results and discussion section of the paper, the author concludes that :  enhanced the performance compared to  and . This suggests that the  performed better than the  in this study Is there a difference between these 2 architectures? \", 'Tags': {'neural-network', 'mlp', 'machine-learning', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37959/how-to-save-and-test-cnn-model-on-test-set-after-training', 'post': ' My CNN model is trained on the training set and validated on the validation set, now I want to test it on test set, here is my code:  I have a test dataset stored as  like  in the code above, tried to do it in more than one way, but I did not succeed, can anyone share a testing code with me, ofcourse based on my code? ', 'Tags': {'neural-network', 'tensorflow', 'machine-learning', 'deep-learning', 'cnn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37957/how-to-handle-large-number-of-features-in-machine-learning', 'post': \" I try to do normal classification on high dimensional traditional columnar data (several hundred columns). The features are of different type. In this case, it's clearly out of question to examine each features one by one to figure out what are they exactly and what optimization or feature engineering could be done with them. Still, I have to do all the necessary preprocessing steps like imputation, standardization etc. But even such basic steps like categorical feature encoding or imputation are problematic because R/Python-pandas are sometimes wrongly recognized the numeric/categorical nature of some variables (and as a consequence, wrongly try to encode or mean-impute the NAs), not to mention other very problematic issues that could be handled if one could oversee the features one by one. Of course, I could turn to models which are capable of handling non-standardized features with NAs but this limits the number of possible models on one hand and seems me very unprofessional on the other hand. What is the way to get over this issue? \", 'Tags': {'neural-network', 'machine-learning', 'deep-learning', 'feature-engineering', 'dimensionality-reduction'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37955/why-is-my-neural-net-getting-a-poor-accuracy', 'post': \"  Results went something like this-  I have referenced part of the code from  textbook on . EDIT- found this O' Reily guide online-https://github.com/ageron/handson-ml/blob/master/10_introduction_to_artificial_neural_networks.ipynb They have a code similar to mine but their's works just fine. \", 'Tags': {'neural-network', 'tensorflow', 'machine-learning', 'deep-learning', 'cnn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37952/how-to-implement-one-vs-rest-classifier-in-a-multiclass-classification-problem', 'post': ' I have a dataset which contains 750 data points with 8 classes in the target variable. I tried implementing simple machine learning models and also did hyperparameter tuning but they results were not much impressive. The best log loss that i could get was 1.52 with a misclassification rate of 53%. What are the other methods that i could apply to improve the models performance ? Also, I want to implement  with the hope that, it would improve the results, but implementation of the same is not clear to me although I looked in the internet for some clear codes.  ', 'Tags': {'data-science-model', 'machine-learning', 'multiclass-classification', 'machine-learning-model'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37950/robustness-of-ml-model-in-question', 'post': ' While trying to emulate a ML model similar to the one described in this paper, I seemed to eventually get good clustering results on some sample data after a bit of tweaking. By \"good\" results, I mean that  Each observation was put in a cluster with high probability (as opposed to having being 50/50 between two clusters or something similar) A high proportion of the observations were put in the correct cluster, indicating that the model actually did work.  For example, if we had observation $a$ that belonged to cluster $A$, and observation $b$ belonging to cluster $B$, then the model might output  for observation $a$ (where the  indicates a high probability for $a$ belonging to $A$ and  represents a low probability of belonging to $B$) and  for observation $b$. (These specific numbers are chose randomly, but generally the good results give probabilities close to 0 and 1.) However, every few times I train the model, I get a weird result; one that seems to still \\'cluster\\' the data in a sense, but is technically wrong. A bad result would give something of the nature  for observation $a$ (which is good), but then give something like  for observation $b$ . In this way, when I look at the data I can tell that it has been \"clustered\" as the observations that belong to $A$ give different cluster probability distributions than the observations of $B$, but the model has not accomplished either of the two goals above when actually assigning clusters.  I would like to make my model more robust so that I get \"good\" results more often, but I don\\'t know what I could do to do this. One thing that would definitely work to help avoid this problem would be to do the cluster training many times and take the average result; the problem with this is that each training takes somewhere on the order of hours, meaning that doing it multiple times might take on the order of days, which I want to avoid.  If there are any (hopefully quick) fixes I can do to avoid getting these odd results, I would love to hear any advice on the topic. I should also be able to answer questions if you need more information, but the paper I linked should have most of the pertinent info.  ', 'Tags': {'machine-learning', 'clustering'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37945/how-to-integrate-time-series-data-into-normal-features-for-machine-learning', 'post': ' I confront a problem where one data source is a \"normal\" DF with customers as rows (each customer occurs once) and static customer features as columns. The other DF other hand is a big pile of timestamped data of the aforementioned customers with event features and event timestamps (each customer occurs many times according their activity). The task is a simple classification by customer. My problem is how to integrate the time series data into the first DF as columns? Should I simply compute somekind of aggregations from the second DF? If so, what kind of? If not, what is the canonical way to merge these two kind of data sets? ', 'Tags': {'classification', 'machine-learning', 'feature-engineering'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37941/what-is-the-difference-between-sgd-classifier-and-the-logisitc-regression', 'post': \" To my understanding, the SGD classifier, and Logistic regression seems similar. An SGD classifier with loss = 'log' implements Logistic regression and loss = 'hinge' implements Linear SVM. I also understand that logistic regression uses gradient descent as the optimization function and SGD uses Stochastic gradient descent which converges much faster. But which of the two algorithms to use in which scenarios? Also, how are SGD and Logistic regression similar and how are they different? \", 'Tags': {'gradient-descent', 'loss-function', 'logistic-regression', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37936/small-dataset-in-time-series', 'post': ' I have soccer data with a time series index. 30 seconds interval. So, 194 rows for 90+ minutes per game. I have 1500 games. The dataframe has the following information. Home/Away: •   Goal Total. •   Shots Total. •   Dangerous Attack Total and. •   XG(Expected goal per game) given at the start of the match which needs to be adjusted per section.  I am required to build a time series model to predict time t+1 (say Goal score).  Given that for each variable the graph shows the highest spike in the later stage of the time, Is there any machine learning algorithms that can learn with such a small dataset? Sample Data  Questions Which dataframe should I use? I would really appreciate if someone starts off with a few python codes...  ', 'Tags': {'time-series', 'optimization', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37934/what-model-can-i-build-with-a-limited-dataset', 'post': ' I have a dataset consisting of purchasing history from an e-commerce website. The columns consist of , , , , . There are thousands of different customer ids, hundreds of product ids, and some million rows. I ran ARIMA modelling for forecasting the purchased quantities of a given product. Now I want to try other methods for analysing the dataset, but cannot find models which could fit the dataset well. What other models could I run in order to gain more insight on this data? ', 'Tags': {'machine-learning', 'predictive-modeling', 'recommender-system'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37928/how-to-identify-the-unknown-class-in-machine-learning', 'post': \"       Closed. This question needs details or clarity. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Add details and clarify the problem by editing this post.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         In the problem of multi-label classification how to identify the unknown class which is not in training labels or classes. In the prediction phase, the classifier puts the data in any of the class even though it's not related to any of the classes or labels? For binary, the prediction probability of classification is 50/50 so how to interpret these as the confusion or just anomaly but when the data appears in both of the class or the similar to both of the class we can understand this scenario but what about the data which should be in any of the classes. So, how to handle the same situation for multi-class or a general solution which can provide the proper classification for unknown and non-relevant data. \", 'Tags': {'classification', 'supervised-learning', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37923/navigating-the-jungle-of-choices-for-scalable-ml-deployment', 'post': ' I have prototyped a machine learning (ML) model on my local machine and would like to scale it to both train and serve on much larger datasets than could be feasible on a single machine. (The model was built with Python and Keras. It takes in a CSV table of inputs and spits out the corresponding CSV table of predicted outputs.) My naive \"vision\" is that I\\'d have the model reside on a single (master) machine, whereas the data would be equally distributed among several units (whatever unit means: nodes in a \"cluster\", CPUs, GPUs, ... ?) The model would be projected onto these units and the learned parameters would somehow all synchronize back to the master unit. Similarly, in the case of serving, the same model would be applied to the data that resides on the different units. Does this \"vision\" sound reasonable? (I have had some experience with parallel computing with MPI and I vaguely remember that\\'s how things work.) If I were to start from a blank slate, what architecture/infrastructure should I choose to deploy my model in a scalable fashion? Below are some of the confusingly many options I have read about. (I hardly master what each of these things do, so please forgive me if it looks like a laundry list of disparate technologies.)  Kubeflow (i.e., Kubernetes) PySpark Amazon SageMaker Distributed TensorFlow Dask Apache Mesos REST API Hadoop ... and dozens more  As a pure ML guy (read: Python, Keras, pandas guy coding on a laptop), I\\'m out of my depth with all the infrastructure jargon that comes with the above links. It\\'s therefore overwhelming to find a starting point, or some kind of \"Hello World\" example I could relate to. All I want is an architecture from which I can transpose my code and have it run in an efficient and scalable manner. Which one does the job? Despite all the hype around ML, there does not seem to be any comprehensive map or comparative review of all these solutions. I did find some comparisons, for example between Spark and SageMaker, or Spark and Dask, but given my illiteracy in these subjects, they only add to the confusion. ', 'Tags': {'tensorflow', 'machine-learning', 'keras', 'bigdata', 'apache-spark'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37919/label-encode-with-pre-defined-classes', 'post': \" I have trained a model (Random Forest) and now I want to use it to predict for a certain data on a particular day. I have a categorical column where there are some values(say a,b,c,d,e) over a period. Now on a particular day, only some of those values are there (say b,d). Now while making them to onehot encoding, I am using LabelEncoder and the Onehot encoder. But if I give that column for label encoding, it is labelling only 'b' and 'd' (say 1 & 2) and the one hot vector length is 2. What I need is say the actual model labelled (a,b,c,d,e) as (1,2,3,4,5), now i need (b,d) to be labelled as (2,4) and the one hot vector to be of size 5. What I am doing is saving the label encoder used in the training and using that one to label encode the column on that day. But I am not getting proper results, am I doing it the right way ? I have given the length of onehot_train as n_values for the one day data. I have used sklearn label encoder and onehot encoder. That is my main question and other one is, suppose I see new category which I haven't seen it during training, how to proceed with that, should I consider all the new categories as a 'unknown' category and encode all of them as same onehot or is there any better method? Thank you for any suggestions.   \", 'Tags': {'machine-learning', 'labels', 'feature-construction'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37889/adding-batch-normalization-layer-to-vgg16-network', 'post': ' I want to use batch normalization layer to decrease overfitting in . Where should the batch normalization layer be added to the network?  ', 'Tags': {'neural-network', 'machine-learning', 'deep-learning', 'keras', 'cnn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37885/gradient-descend-method', 'post': \"  Hello! I started learning the Gradient Descend Method in order to solve some regression problems, I can say I know what the algorithm does overall but I can't understand why in every cost function, the x^i is not taken as an argument! Lets suppose we have n sized input data (x1,x2,...,xn) , each input data has m attributes. In the cost fucntion we basically have an xi from (x1,x2,...,xn), but which one? We just take one at random? And that remains starndard throughout the algorithm? I did attach the image to provide an example of what I'm saying \", 'Tags': {'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37883/gps-route-matching', 'post': \" We have a mobile application which records many of the sensors on a users  mobile to a database (time,GPS location, activity (e.g. walking,still), network connectivity status) etc.  The user is allowed to select a contiguous time frame from these records and tag what they were doing e.g. gone for a run.   What we then wish to do, given this information, is next time this sequence of activities is seen we automatically tag what we believe it to be.  The 'problem' for us though is that the sequence of entries can be variable e.g. GPS coordinates not quite the same as last time. different number of GPS entries.  So my question is, given a SINGLE tagged sequence of activities how can we reasonably classify a future sequence as say 'gone for a run'. (We did think of mapping the GPS traces to roads, but not everyone follows roads/paths for runs).   Ideally we also want to be able to execute this on the users mobile. Any pointers gratefully received. \", 'Tags': {'classification', 'markov-hidden-model', 'machine-learning', 'svm'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37859/forecasting-revenue', 'post': ' Does anyone have any recommendations on how I would go about forecasting Microsofts revenue using python + time series or ML (recommended techniques e.g Random-forest). (I have past revenue and share price data).  Thanks.  ', 'Tags': {'forecasting', 'machine-learning', 'predictive-modeling'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37833/integration-of-sentiment-analysis-in-crm', 'post': ' What is the process for integrating sentiment analysis in a CRM? What I am searching for is a system which analyzes the customer comments or reviews using the CRM and finds out the customer sentiment on the services provided by the system or company or a product. I have done a sentiment analyzer which takes text and shows the sentiment of the text. Now I want to integrate the above-mentioned sentiment analyzer to a CRM, how can I do that? ', 'Tags': {'sentiment-analysis', 'machine-learning', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37827/how-to-check-two-imagesone-is-original-image-and-other-one-captured-by-mobile', 'post': \" My problem statement is - In my project, original image of product is stored in database. Now whenever any person uploading that product's camera pic(for internal audit process) then I need to verify whether this uploaded image is similar of original image or not. So can anyone help me on this, how can I achieve this verification automated using deep learning or tensor-flow ?  \", 'Tags': {'tensorflow', 'machine-learning', 'image-classification', 'deep-learning', 'image-recognition'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37819/ml-models-how-to-handle-categorical-feature-with-over-1000-unique-values', 'post': \" I am trying to build an ML Classification model on a data set that contains quite a few categorical columns. However, few of them have over 1000 unique values. I am concerned that if I run one-hot encoding or pandas get dummies on them, it will simply result in too many features to work with. So, I tried to find the top N unique values that account for, say, 90% of the underlying data and group the rest of them under say, 'Other' or 'miscellaneous'. But that's making the 'Other' or 'miscellaneous' value as the most prominent one. I am concerned that this might skew the model/results. Any pointers as to how I should handle such a scenario? \", 'Tags': {'supervised-learning', 'machine-learning', 'pandas', 'python', 'unsupervised-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37806/purpose-of-backpropagation-in-neural-networks', 'post': \" I've just finished conceptually studying linear and logistic regression functions and their optimization as preparation for neural networks.  For example, say we are performing binary classification with logistic regression, let's define variables:  - vector containing all inputs.  - vector containing all outputs.  - bias weight variable.  - vector containing all weight variables.  - summation of all weight variables.  - logistic activation function (sigmoid), representing conditional probability that  will be 1 given observed values in .  - binary cross entropy loss function (Kullback-Leibler divergence of Bernoulli random variables plus entropy of activation function representing probability)  is multi-dimensional function, so it must be differentiated with partial derivative, being:  Then, the chain rule gives:  After doing few calculations, derivative of the loss function is:   So we got derivative of the loss function, and all weights are trained separately with gradient descent. What does backpropagation have to do with this? To be more precise, what's the point of automatic differentiation when we could simply plug in variables and calculate gradient on every step, correct? In short We already have derivative calculated, so what's the point of calculating them on every step when we can just plug in the variables? Is backpropagation just fancy term for weights being optimized on every iteration? \", 'Tags': {'neural-network', 'loss-function', 'machine-learning', 'backpropagation', 'logistic-regression'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37785/classification-of-medical-dataset', 'post': '       Closed. This question needs details or clarity. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Add details and clarify the problem by editing this post.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         I have medical data set with different types of attributes and I have to do the classification based on attributes: there are no class labels in this data set.  Suggest to me which algorithm works better for this dataset. ', 'Tags': {'classification', 'machine-learning', 'clustering'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37784/how-to-automatically-verify-official-documents', 'post': \" I am new to machine learning and data science. I apologise if the question seems very basic. I have a requirement where I need to verify information submitted via a form with the corresponding official document. My approach till now has been to use google vision to extract text and use regex to extract necessary fields and compare with the form information. This is not always reliable because of the image quality and vision captures the noise as well.  Previously, I was thinking of just comparing each data file and search it in the text extracted and provide a metric of certainty. I talked to one of my colleagues and he suggested using some for of supervised learning algorithm to process the documents, so that they automatically extract the key fields.  I want to ask would this method be simpler than my current approach. I am also worried about the scalability of my approach in event of minor changes in document formats.  I am looking for links for some articles or books related to this and answer to why using my own model will be better than just searching each word in a text. Edit: The data can be imagined as a visiting card which contains the name of person, his office address, his contact number and the company's name. The problem here is different visiting cards can have different formats. Moreover the information is repeated twice: Once in English and the second in another language. For example , NAME : JANE DOE is followed by नाम : जेन डोए in next line. I only need the English name. The number of formats are constant but are high. Moreover, the address is not read together by Google Vision in most cases and using regex can become too complicated and case specific. I need to verify employee identity cards as proof that they are working for the company they claim to. \", 'Tags': {'neural-network', 'supervised-learning', 'regex', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37780/alternative-to-apache-spark', 'post': \"       Closed. This question is off-topic. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Update the question so it's on-topic for Data Science Stack Exchange.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         I have been looking for a comprehensive alternative to Apache Spark for Big Data analytics/machine learning and couldn't find one. The ones which I have come across are:  Apache Flink  Google DataFlow Apache Storm SAS  Can any of these can be considered as a complete alternative/better than Spark? Or Spark is the only comprehensive Big Data analytics/machine learning engine there is these days? \", 'Tags': {'apache-spark', 'bigdata', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37777/maximum-number-of-classes-yolo-net-can-recognize', 'post': \" I'm trying to make a mobile app on image recognition(Computer Vision Application) . Does anyone know whether modern day smartphones have enough processing power/memory to recognize, say about 1 million classes from their real-time camera feed (30 fps) ?  (On-device inference or YOLO neural net concepts needed.) What is the maximum number of classes it can reliably classify on a mobile device? Some insight into figuring out computational loads/times in general would be helpful. \", 'Tags': {'neural-network', 'computer-vision', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37767/how-do-machine-learning-models-work-and-remember', 'post': '       Closed. This question needs details or clarity. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Add details and clarify the problem by editing this post.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         I am new to machine learning. I am confused about how a machine learning model remembers what it learns. And how it learns.  I know the basic workflow of machine learning: first is data gathering, then data preprocessing, then creating training and test data sets, then training the ML learning algorithm. After training, we test the accuracy of our model with test data. But I am confused about how the algorithm is trained and how it remembers what it learns. ', 'Tags': {'machine-learning', 'machine-learning-model'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37756/how-can-generative-models-be-used-in-machine-learning-classification-application', 'post': ' My understanding of generative models is that they generate data to match certain statistical properties. Intuitively, I find it hard how generative models can be used for classification purposes. On the other hand, discriminative models being used for classification is self-explanatory. ', 'Tags': {'classification', 'generative-models', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37755/training-stateful-lstm-with-different-number-of-sequences', 'post': \" I'm using a stateful LSTM for stock market analysis, and I have varying amounts of data for each stock, ranging from 20 years to just a few weeks (i.e. for newly listed stocks). I use 3 years of data as a minimum for training as I want to create some state within the network. I set a year as my sequence length, so if I have 12 years of data then I will submit 4 batches with 3 sequences in each. Only after I've submitted all batches do I then reset the network state ready for the next stock. But is there any issue training with differing number of sequences? E.g. if I train with a company that has 20 years of data then the network will build up much more state than a company that I only have 3 years of data. \", 'Tags': {'neural-network', 'recurrent-neural-net', 'lstm', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37752/ner-extracting-entities-from-an-article', 'post': \" Description : I have dataset of categorised articles and to extract specific values from respective categorised article I have regex created for each category. Aiming for:  Nlp techniques which learns the context of the content and avoids/minimizes the use of regex If some new (similar) article comes up, depending on the learning (from 1) it tries to give the specific values.  Steps taken:   Created a dataframe with various features like : 'Name of the author', 'published date', etc. and got the values from the dataset by using regex  Research_work: I was considering these options ahead this stage :  Using CNN : it will classify new articles depending on the feature values it learnt on and then use regexs for entity extraction. (It wont achieve the first aim) Using CRF (medium_article): making use of POS+IOB tagging  Is there any other way around ? cons of above stated methods? \", 'Tags': {'data-science-model', 'machine-learning', 'nlp', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37739/how-to-choose-dimensionality-of-the-dense-layer-in-lstm', 'post': \" I have a task of multi-label text classification. My dataset has 1369 classes:  For this task, I've decided to use LSTM NN with the next parameters:  Question: Are there any scientific methods for determining  and  dimensionality (in my example, , , and )?  If there are no scientific methods, maybe there are some heuristics or tips on how to do this with data with similar dimension. I randomly chose these parameters. I would like to improve the accuracy of the model and correctly approach to solving similar problems. \", 'Tags': {'neural-network', 'lstm', 'machine-learning', 'deep-learning', 'keras'}}\n",
            "page number ------------------------------------91\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37738/how-to-know-for-sure-if-we-can-learn-from-a-given-data-or-not', 'post': ' I want to know that given a set of data and a target, how we can know for sure whether we can learn from that data to make any inference or not? ', 'Tags': {'neural-network', 'machine-learning', 'deep-learning', 'data', 'learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37734/what-are-some-situations-when-normalizing-input-data-to-zero-mean-unit-variance', 'post': ' I have seen normalization of input data to zero mean, unit variance many times in machine learning. Is this a good practice to be done all the time or are there times when it is not appropriate or not beneficial?   ', 'Tags': {'normalization', 'machine-learning', 'feature-scaling'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37730/how-to-use-scikit-learn-normalize-data-to-1-1', 'post': ' I am using scikit-learn  to normalize to $[0, 1]$, but I want to normalize to $[-1, 1].$ What function do I have to use to normalize this way? ', 'Tags': {'scikit-learn', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37722/how-to-interpret-predictions-from-model', 'post': \" I'm working on a multi-classification problem - Recognizing flowers.  I trained the mode and I achieved accuracy of 0.99. To predict, I did:  output:  How do I interpret this? How do I know get the label it predicted? I have five labels 0-4, which are assigned to 5 types of flowers. My notebook is here.  What am I doing wrong here? \", 'Tags': {'prediction', 'keras', 'machine-learning', 'image-classification'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37710/why-does-the-naive-bayes-algorithm-make-the-naive-assumption-that-features-are-i', 'post': '  is called naive because it makes the naive assumption that features have zero correlation with each other. They are independent of each other. Why does naive Bayes want to make such an assumption? ', 'Tags': {'naive-bayes-classifier', 'machine-learning', 'probability'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37706/how-to-handle-unknown-category-in-machine-learning-classification-problems', 'post': ' Tutorial problems come in the form of binary or mult-class classification where data are all properly labelled. In real-life applications, there are incoming data that do not belong to any category and cannot be classified. How can we handle these data which fall into \"unknown\" category? The universe of \"unknown\" can be far more than \"known\". So, the data for \"unknown\" can be too much and lead to class imbalance. How do we train the model to deal with \"unknown\" data? Or do we ignore it? ', 'Tags': {'class-imbalance', 'classification', 'machine-learning', 'classifier'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37705/face-recognition-takes-too-long', 'post': ' I am doing a small project with the basic idea of recognizing a face. I made a version that uses  library. And, although I am quite satisfied with the results I am not happy with the time it takes for the program to finish. I am quite sure that this is due to  does not use GPU/CUDA despite me having GPU on my machine. in particular the function  does all the computation on CPU. In the docs of this function it says that it will use CPU if it cannot use GPU. However, I used GPU with another library and it was fine. I would be happy for any help that you can give me. P.S. Probably you know other libraries that I probably should pay attention to. Edit: the link to the face_recognition library. ', 'Tags': {'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37685/feature-extraction-using-a-cnn', 'post': ' I want to use a CNN to extract features from a dataset. My questions are:   What is the stopping criteria when training a CNN to extract features? What is the difference between extracting features after training for 50 epochs or 100 epochs? How should I choose the best layer for feature extraction? Does it depend on the accuracy of the model?  ', 'Tags': {'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37674/what-should-be-y-train-in-keras-lstm', 'post': ' I refer to the example given at the Keras website here:  For a real-world example, what should be y_train and y_val? Should they be the same as x_train and x_val respectively, since they come from the same sequence? Also, how should I understand data_dim and num_classes? ', 'Tags': {'neural-network', 'lstm', 'machine-learning', 'keras', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37645/in-natural-language-processing-why-each-feature-requires-an-extra-dimension', 'post': \" I am reading Machine Learning by Example. I am trying to understand natural language processing. The book used Scikit-learn's fetch_20newsgroups data as an example. The book mentioned that the text data in the 20 newsgroups dataset that we downloaded from fetch_20newsgroups data is highly dimensional. I do not understand this statement.  It is my understanding that dimension is used to describe axies that an array has. For example,  How does no. of axies relates to feature in NLP? Why one feature equals to one dimension? Please explain. Thanks. Below is the code from the book that used to download the data for your reference.  \", 'Tags': {'feature-selection', 'machine-learning', 'nlp'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37632/machine-learning-perceptron-algorithm', 'post': \" I'm studying Machine Learning using Sebastian Raschka's book.   Wonder if someone could please help me to confirm if I have the following steps correct if I apply Perceptron Algorithm to Iris dataset as shown in the table below. What do I set $\\\\eta$ to? What do I set $θ$ to? I randomly set it to 2.  Perceptron rule - summarized by the following steps:  Initialize the weights to 0 or small random numbers.  For each training sample $x(i)$ perform the following steps:  Compute the output value $\\\\hat{y}$ . Update the weights.   \\\\begin{array}{c|c|c|}   & \\\\text{sepal_length, $X_1$ } & \\\\text{sepal_width, $X_2$} & \\\\text{petal_length, $X_3$} & \\\\text{petal_width, $X_4$} & \\\\text{species, $Y$}\\\\\\\\ \\\\hline \\\\text{Row 0} & 5 .1 & 3.5 &1.4 &0.2 & setosa \\\\\\\\ \\\\hline \\\\text{Row 1} & 4.9 & 3.0 & 1.4 & 0.2 & setosa \\\\\\\\\\\\hline \\\\text{Row 2} & 4.7 & 3.0 & 1.4 & 0.2 & setosa \\\\\\\\\\\\hline \\\\text{Row 3} & 4.6 & 3.1 & 1.5 & 0.2 & setosa \\\\\\\\\\\\hline \\\\text{Row 4} & 5.0 & 3.6 & 1.4 & 0.2 & setosa \\\\\\\\\\\\hline \\\\end{array} set: $θ = 2$ set: $virginica = -1,$ $\\\\>$ $setosa = 1$ let's say if $z \\\\ge  θ,$ $\\\\>$ $θ(z) = 1$,  $\\\\>$ $θ(z) = -1$ if otherwise Initialize weight vector: $W$ = [ $w_1$ = 0.1, $w_2$ = 0.2, $w_3$ = 0.3, $w_4$ = 0.4] For observation row 0:  $z = w_1*x_1 + w_2*x_2 + w_3*x_3 + w_4*x_4$  $= (0.1* 5.1) + (0.2*3.5) + (0.3*1.4) + (0.4*0.2)$ $= 1.71$ Since $z = 1.71 \\\\lt θ = 2$, $\\\\>$ then $θ(z) = -1$ As for row 0, the perceptron algorithm incorrectly predict $virginica = -1,$, hence the weights would be adjusted. $w_j:= w_j + \\\\Delta w_j$ $\\\\Delta w_j = \\\\eta(y^{(i)} - \\\\hat{y}^{(i)}) x_j^{(i)}$ Hence, $\\\\Delta w_1 = \\\\eta(1-(-1))5.1 = 10.2\\\\eta$ $\\\\Delta w_2 = \\\\eta(1-(-1))3.5 = 7.0\\\\eta$ $\\\\Delta w_3 = \\\\eta(1-(-1))1.4 = 2.8\\\\eta$ $\\\\Delta w_3 = \\\\eta(1-(-1))0.2= 0.4\\\\eta$ Update weight vector: $W$ = [ $w_1 = 0.1 + 10.2\\\\eta$, $w_2 = 0.2 + 7.0\\\\eta$, $w_3 = 0.3 + 2.8\\\\eta$, $w_4 = 0.4 +0.4\\\\eta$] Using the updated weight to Row 1 and repeat the process above. \", 'Tags': {'perceptron', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37631/parameter-of-conditional-gaussian-distribution', 'post': \" I'd like to understand how to determine the parameter of conditional gaussian distribution. Following is the network architecture of VUNET which learns the conditional gaussian distribution $q(z|x, \\\\hat y)$ of appearance of human($z$) upon given two conditional inputs skeleton of body pose and groundTruth image($x$) of 128x128 resolution image  I don't understand how $2\\\\times2\\\\times256$ vector and  $1\\\\times1$ vector can characterize  $q(z|x, \\\\hat y)$. Any advice to understand this point clearly?    \", 'Tags': {'gaussian', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37629/ai-vs-ai-algorithm', 'post': \" What is the name of the chess algorithm that learns by playing itself? I learned about it in an AI class and haven't been able to find it. \", 'Tags': {'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37618/mean-variance-mapping-optimization-mvmo-in-r', 'post': \" Someone know tell me if there is any package in R about Mean-variance mapping optimization (MVMO) algorithm? I already researched, but don't find anything about this. \", 'Tags': {'algorithms', 'optimization', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37615/how-to-extract-all-the-information-from-a-midi-file-monophonic-and-polyphonic', 'post': \" I have tried Music21,Midicsv and all the packages regarding midi in python but can't yield a result yet.In midicsv i can't figure out that how should approach to vectorize the csv file. Does anyone have any idea. I will be grateful for any kind of help. To know more about this issue please check the following links: Issue on music21 Issue on Midicsv Please help me to solve this problem. \", 'Tags': {'machine-learning', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37598/apache-spark-understanding', 'post': \" I have read a lot about Apache Spark. About RDD, DataFrames etc etc. But I haven't come across a simple explanation as to why and where should we be using Apache Spark? If my primary goal is machine/deep learning when should be the case that it be must to use Apache Spark?  \", 'Tags': {'apache-spark', 'machine-learning', 'pyspark'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37577/is-a-neural-network-with-20-times-the-number-of-input-neurons-on-hidden-layers', 'post': ' I\\'m aware of the problem of over-fitting.  One way to describe it is your Neural Network learning your training data to a high accuracy and performing poorly (generalizing) on new data. Was wondering if there are situations where having 20 times the number of input neurons on the first hidden layer wouldn\\'t necessarily produce overfitting always. Here is a screen shot of my work flow.  As you can see I\\'m using test data.  The way I\\'ve done it is split the data myself using a \"Past/Future\" column with a P or an F.  The future is the last 5% of the sequential time data. I don\\'t use randomization since I don\\'t think it makes much sense to unsort the sequential time datapoints.  Thanks. ', 'Tags': {'neural-network', 'overfitting', 'generalization', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37574/julia-to-learn-or-not-to-learn', 'post': \"       Closed. This question is opinion-based. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         I'm relatively new to data science, having done some machine learning projects at work. My background is in R, although I have some experience in Python. I'm considering choosing one language and getting really good at using it for data science / machine learning. Now, I've read various articles touting Julia's strengths - namely ease of use and massive speed benefits. It sounds almost too good to be true! So my question is, is there any particular reason(s) why I should choose Julia over R/Python? Or alternatively, is there any reason(s) why I shouldn't choose Julia over R/Python? \", 'Tags': {'julia', 'machine-learning', 'python', 'r'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37558/can-anyone-recommend-some-good-books-or-articles-on-working-with-time-series', 'post': \" I've read a couple of guides such as this link. I'm having real trouble getting to grips with how to use time series data effectively. Anything to help with the intuition ideally with some case studies. I am currently reading Applied predictive modelling so anything with a similar pedagogy would be ideal.  \", 'Tags': {'time-series', 'books', 'machine-learning', 'data-science-model'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37548/why-does-bagging-or-boosting-algorithm-give-better-accuracy-than-basic-algorithm', 'post': \" I was working with a small dataset, with 392 values, and it was kind of an imbalanced dataset, with 262 values belonging to class 1 and rest 130 to class 0. So I did the upsampling technique, importing  module. However, the total dataset was now around 520 values. I applied basic, algorithms first like Logistic Regression and SVM Classifier, and since we all know that precision is not a good accuracy metric for imbalanced dataset, I use the f1-score and recall score. In logistic Regression I found out, it was giving 78% f1-score for class 1 and 80% for class 0 , and almost 99% f1-score for class 0 in SVM and 72% for class 1, which shows that it is overfitting. But to my surprise I found out that Random Forest gave me a better accuracy, with having around 83% f1-score for class 0 and 82% for class 1 . But till now everywhere I have seen that for bagging and boosting algorithms to work well, we need a lot of data, which is not the case in this scenario. I've searched google a lot, but unfortunately I haven't been able to get any specific answer, and I need to know the fundamentals, why does this happen? Logistic Regression:  SVM with rbf-kernel:  Random Forest Classifier:  \", 'Tags': {'random-forest', 'machine-learning', 'python', 'scikit-learn', 'logistic-regression'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37533/metrics-methods-for-deciding-duration-of-video-retention-for-on-demand-websites', 'post': ' This might be a general question but I thought this might be the best place to brainstorm. If I had a video website that only wants to retain videos on-demand for a certain number of days before getting rid of them to maximize operating dollars, what sort of data measures/methods would I want to consider or use in general to decide the time period? Just like a typical website, the video can be tracked for views, followers etc. and the trade off could be on views/operating cost ', 'Tags': {'data-analysis', 'statistics', 'feature-selection', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37530/simple-object-detection', 'post': \" I want to create a simple object detection tool. So basically an image will be provided to the tool and from that, it has to detect the number of objects. For example An image of a dining table which has certain items present on it such as plates, cups, forks, spoons, bottles etc. The tool has to only identify the number of objects irrespective of the type of object. After identifying it should return the position of the object with its size so that I can draw a border over it. I don't want to use any library or API present such as Tenser Flow, OpenCV etc. If the process is very difficult to be created without using an API then the number of/type of objects which it will count as an object can also be limited but since this project will be for my educational/learning purpose can anyone help me understand the logic using which this can be achieved? For eg, it may ignore a napkin present in the table to be counted as an object. \", 'Tags': {'neural-network', 'computer-vision', 'machine-learning', 'image-classification', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37529/after-one-hot-encoding-instead-of-columns-my-number-of-rows-are-increasing', 'post': ' This is the code:   ', 'Tags': {'scikit-learn', 'categorical-data', 'machine-learning', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37525/anomaly-detection-on-time-series', 'post': ' I\\'ve just started working on an anomaly detection development in Python. My data sets are a collection of timeseries. More in details, data are coming from some sensors/meters which record and collect data on boilers or other equipments. As I said before, the data which I have to work with, are timeseries, so a timestamp and the relative value detected by sensor; a value is anomalous when it\\'s bigger or smaller than the others near it; basically a peak. I need to develop an unsupervised classification model, because I haven\\'t labels for all data. Another important aspect, is that this data are \"season dependent\"; in fact a boiler should be has a higher consumptions in winter than summer. Those values must not be considered as anomalies. Since I\\'ve no experince on this topic, I\\'m here to ask you, what is the best algorithm/approace to solve this problem. Furthermore, do you know some books or links to suggest? ', 'Tags': {'machine-learning', 'python', 'anomaly-detection', 'time-series', 'unsupervised-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37520/can-we-use-a-neural-network-to-perform-arithmetic-operation-between-2-numbers', 'post': ' How to develop a neural network which can perform subtraction? ', 'Tags': {'machine-learning', 'python', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37518/time-horizon-t-in-policy-gradients-actor-critic', 'post': \" I am currently going through the Berkeley lectures on Reinforcement Learning. Specifically, I am at slide 5 of this lecture.  At the bottom of that slide, the gradient of the expected sum of rewards function is given by $$ \\\\nabla J(\\\\theta) = \\\\frac{1}{N} \\\\sum_{i=1}^N \\\\sum_{t=1}^T \\\\nabla_\\\\theta \\\\log{\\\\pi_\\\\theta(a_{i,t} \\\\vert s_{i,t}) (Q(s_{i,t},a_{i,t}) - V(s_{i,t}))} $$ The q-value function is defined as  $$Q(s_t,a_t) = \\\\sum_{t'=t}^T \\\\mathbb{E}_{\\\\pi_\\\\theta}[r(s_{t'},a_{t'})\\\\vert s_t,a_t]$$ At first glance, this makes sense, because I compare the value of taking the chosen action $a_{i,t}$ to the average value in time step $t$ and can evaluate how good my action was. My question is: a specific state $s_{spec}$ can occur in any timestep, for example, $s_1 = s_{spec} = s_{10}$. But isn't there a difference in value depending on whether I hit $s_{spec}$ at timestep 1 or 10 when $T$ is fixed? Does this mean that for every state there is a different q value for each possible $t \\\\in \\\\{0,\\\\ldots,T\\\\}$? I somehow doubt that this is the case, but I don't quite understand how the time horizon $T$ fits in. Or is $T$ not fixed (perhaps it's defined as the time step in which the trajectory ends in a terminal state - but that'd mean that during trajectory sampling, each simulation would take a different number of timesteps)? \", 'Tags': {'machine-learning', 'deep-learning', 'reinforcement-learning', 'policy-gradients', 'actor-critic'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37513/how-do-i-predict-continuous-value-from-time-series-data', 'post': \" I have a glove that have 2 IMUs (Inertial Measurement Unit) attached to it. It can give the rotation value as Quaternion (x,y,z,w) and acceleration of the hand (x,y,z). I put it on my left hand and I want to predict the position of the hand relative to some fixed point on my body (e.g. my head). When I collect the data, I just use the Vive controller to track my hand. Right now I just feed the value into a simple model like SVM to predict (x,y,z) of the hand relative to my head. The problem is that the output is jumpy as the sensors are not stable enough. And because it doesn't take into account data in the previous timestep. I want to utilize time series data as my data is time series in nature. Which algorithms I should use if I'm treating my data as time series? You can suggest me things I should learn about too. \", 'Tags': {'time-series', 'machine-learning', 'prediction'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37503/early-stopping-and-final-loss-or-weights-of-models', 'post': ' In a deep model, I used the Early stopping technique as below in Keras:  Now, when I run this code, in the output it prints the loss value for training and validation of each epoch. I set the  in the early stopping. So, it continues the training process two times after when the validation loss increased instead of decreased. Some things like this:  In the end, what will be the final weights of the model and the Loss values? The final time of training or two times before it? If it considers the final epoch, so should it be better if I set the patience as little as possible to overcome the overfitting? Thank you ', 'Tags': {'overfitting', 'machine-learning', 'deep-learning', 'keras', 'epochs'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37500/how-do-i-perform-sentiment-analysis-on-tweets-in-the-following-pattern', 'post': \" I have tweets obtained based on matches (football) before the match begins. I have tweets which specify a team will win 3-1 and so on which are easily analyzed using regular expressions. I am facing difficulties when it comes to tweets where two team names are specified with win / loss comparison such as: Manchester United will win the match tonight against Spurs. (or) Spurs will find it difficult to break down United defence. I have taken several such tweets and replaced the team names as TEAMONE and TEAMTWO and placed them into two files (teamone winning tweets and team two winning tweets). Using SVM, the model couldn't clearly classify tweets between the two classes (files mentioned above). Is it due to class imbalance? Should I stick to a rule based classification in this case or would any different method work? I'm a newbie in ML and any suggestions would be appreciated. \", 'Tags': {'sentiment-analysis', 'machine-learning', 'natural-language-process'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37495/problem-in-recommendation-for-categorical-data', 'post': ' I have been building a recommendation model to recommend certain questions in an interaction platform to users to help each other. I have calculated an affinity score between categories to find which top categories are to be recommended. But each category has questions by users in itself. The amount of the questions increases with every new post a user posts in a certain category. Now how can I choose which of these questions to recommend when I have chosen the category through my affinity score ? Do I make it random ? Do I display the questions which come first in the data base ? Or is there any better alternative ? ', 'Tags': {'machine-learning', 'python', 'recommender-system'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37490/k-means-clustering-or-classification', 'post': ' Why is choosing the k in the k-means clustering method based on a feature (take a dead or alive patients scenario as an example, k will be 2) considered clustering rather than classification? ', 'Tags': {'neural-network', 'machine-learning', 'deep-learning', 'classification', 'k-means'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37485/decision-tree-vs-neural-network-for-boolean-function', 'post': ' Which structure is more powerful in terms of expressiveness (i.e. it can represent a given Boolean function, accurately)\\u200a—\\u200aa single-layer perceptron or a 2-layer decision tree? (There are 10 features) ', 'Tags': {'neural-network', 'machine-learning', 'predictive-modeling', 'deep-learning', 'decision-trees'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37480/how-can-i-do-classification-with-categorical-data-which-is-not-fixed', 'post': \" I have a classification problem with both categorical and numerical data. The problem I'm facing is that my categorical data is not fixed, that means that the new candidate whose label I want to predict may have a new category which was not observed beforehand.  For example, if my categorical data was , the only possible labels would be ,  and , no matter what. However, my categorical variable is  so it could happen that the person I am trying to predict has a new city that my classifier has never seen.  I am wondering if there is a way to do the classification in these terms or if I should do the training again considering this new categorical data.  \", 'Tags': {'classification', 'categorical-data', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37471/dataset-with-disproportionately-more-of-a-single-label-than-any-other', 'post': \" I'm using the data analysis software Orange to analyze rows of data with labels $\\\\{H, T,L\\\\}$. $T$ is the neutral state of the system I'm trying to model so data is almost always labeled with $T$.  This leads to an extremely high $97$% classification accuracy, that is untrue.  Almost no datapoint with an $H, L$ actually gets labeled with an $H, L$ respectively by the neural net.  The neural net is doing what's logical: flatten everything to $T$ since that occurs most of the time.  How can I repair this situation? \", 'Tags': {'neural-network', 'machine-learning', 'dataset', 'data-cleaning', 'orange'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37470/can-generative-adversarial-network-be-run-on-any-embedded-edge-device', 'post': ' I am using   to generate images. However, I want to run it on embedded devices, such as or .  But, while going through several internet articles, I came to know that embedded devices are only used for inference in case of deep learning. I have a few questions :  GAN is basically a generative network. How can it have inference ? Will the inference and training be same thing for GAN ? Can it (GAN) be run on embedded devices , where memory is really an issue?  ', 'Tags': {'generative-models', 'machine-learning', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37445/triplet-loss-what-threshold-to-use-to-detect-similarity-between-two-embeddings', 'post': \" I have trained my triplet loss model using FaceNet's architecture. I used 11k hands dataset. Now I want to see how well my model performed, so I feed it 2 images of the same class and get back their embeddings. I want to compare the distance between these embeddings and if that distance is not larger than some threshold I can say that the model correctly classifies these 2 images as of the same class. How do I select the threshold value? Is the threshold value the same as the margin hyperparameter used in triplet loss function? () After examining FaceNet's code and OpenFace's code, I saw that they test 400 thresholds raging from 0.0 to 4.0 () to find the one that gives the most accurate results and then treats that as the overall accuracy. You can see it implemented in FaceNet here and in OpenFace here. Why does it select 0.01 as the step from 0.0 to 4.0? Why not 0.001 or 0.0001? Moreover, FaceNet uses LFW dataset. LFW dataset defines its own specific protocol for evaluation which is implemented in FaceNet and OpenFace. So isn't it fair to say, that the way FaceNet and OpenFace evaluate their model is exclusive to only LFW dataset? In that case, how should we evaluate our model on different datasets that LFW? \", 'Tags': {'embeddings', 'machine-learning', 'convnet'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37441/how-to-perform-a-reggression-on-3-functions-using-a-neural-network', 'post': \" I am currently building a neural network using Keras to perform a regression.  I have 4 independent variables . They are used to predict 3 different functions , , . Should my output layer have 1 or 3 neurons? Also, should I be using a relu or linear activation function for the output layer? I'm currently using MSE for my loss function and adam for my optimizer. For my metrics, should I use 'accuracy' or 'r2'? Any suggestions?  I'm sorry, I'm new to deep learning... \", 'Tags': {'neural-network', 'machine-learning', 'deep-learning', 'regression', 'keras'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37410/writing-a-classier-that-puts-extra-weight-on-one-label-for-supervised-learning', 'post': \" I'm a beginner in Machine Learning and am playing around with Python scikit-learn. Given two hypothetical models, Doctors and Health Care Seekers Doctors: (size N = 15)  Doctor_Label: Id of Doctor Field_Label: Numerical Label of what he/she treats in [1 -> Heart, 2 -> Lungs...]  And the last two fields are boolean fields (1 yes, 0 no) if they accept emergencies and can work for free. Training Model Health Care Seekers (size N = 50)  Complication_Label: Numerical Label of treatment needed [1 -> Heart, 2 -> Lungs...]  And the last two fields are boolean fields (1 yes, 0 no) if they are emergencies and can pay  I used Linear Discriminant Analysis on a split train test set to develop a model. Note the algorithm used can be anything, I chose LDA because it gave the best accuracy  The accuracy I got was 0.81 for the training data and 0.77 for the test data. The main problem I got here was that the weights for all 3 labels were around the same. Meaning there were many doctors predicted for patients where the doctors field was different than what the patient needed Is there a way to train the algorithm (or choose another ML algorithm) that puts extra weight on the Complication_Label so it matches the Doctors Field_Label? Or should I look at dropping these labels altogether and running seperate ML algorithms on each field? I can provide more details if needed \", 'Tags': {'scikit-learn', 'dataset', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37406/classifying-whether-a-comment-or-review-is-a-complaint-or-appreciation-of-produc', 'post': \" I need to classify whether a given review or comment is a complaint or appreciation. This is planned to be used in multiple places, product review pages of own site as well as facebook and twitter. Suggestions on how to approach please. The Problems that are confusing me:  In FB/Twitter I don't know which product it is for, I need to extract that from text as well. I need to extract the complaint/appreciation part and group similar ones together, (like good color reproduction and great clarity into just good display) Articles(each document) will be differently sized. Data availability is none, I will prepare data by going through our FB etc.  My initial thought was LSTM based classification, but point 3,4 make that hard. Even with 3,4 solved. How do I go about 1,2. I only have played with word2vec a bit and done some twitter sentiment analysis dummy projects. Point 1,2 seem Information Retrieval, Need pointers for that. \", 'Tags': {'machine-learning', 'nlp', 'natural-language-process', 'information-retrieval', 'classification'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37399/time-series-data-combined-with-multiple-feature-predicting-difference-from-the', 'post': \" I'm trying to predict the % attendance of people to gym classes that have previously been booked. It is heavily dependent on the time of day and also a load of other features (is it raining, fraction of class of class that booked yesterday compare to have booked a week ago etc). Random forest alone performs very poorly. I instead tried to predict the difference from the mean for the hour of the day using random forest then just add that on to the mean. This again performs worse than just the mean itself. My first question is, is this predicting the difference from the mean a bad idea? I cant find people using similar methods which makes me think it isnt a good idea. Secondly is there a better algorithm suited to this task? \", 'Tags': {'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37398/python-neural-network-running-into-x-test-y-test-fit-errors', 'post': \" I have built a neural network and it worked fine with a small dataset of around 300,000 rows with 2 categorical variables and 1 independent variable, but was running into memory errors  when i increased it to 6.5 million rows. So I decided to modify the code and am getting closer but now I am running into an issue with fit errors.  I have 2 categorical variables and one column for the dependent variable of 1's and 0's(suspicious or not suspicious.  To start off the dataset looks like this:  My code followed/with the errors:  If this helps i printed the X_train and y_train:  \", 'Tags': {'neural-network', 'scikit-learn', 'machine-learning', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37390/system-requirements-to-train-a-image-recognition-neural-network', 'post': ' I have 6000 Images to be trained on a Neural Network. My current PC Specs :-  32GB RAM, i5 2 core Processor, Standard GPU (No work going on GPU),   1TB Hard Disk  My Neural Network Specs :-  3000 Epoch Size, 6 Batch Size  Process is getting killed with these specifications.  How much Hardware is required to trained it well? What would be the   ideal Batch Size and Epoch Size?  ', 'Tags': {'tensorflow', 'machine-learning', 'deep-learning', 'python', 'image-recognition'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37387/what-is-difference-between-multi-class-one-vs-all-and-multilabel-classification', 'post': ' Although Multi class is different from Multi label classification, whats difference does adding One vs All make in Multi-class. Edit 1: http://scikit-learn.org/stable/modules/multiclass.html#multilabel-learning In this Link the part where it mentions supported classifiers under bullets of Multiclass as One-Vs-All and Support Multilabel is confusing. ', 'Tags': {'scikit-learn', 'machine-learning', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37378/what-are-kernel-initializers-and-what-is-their-significance', 'post': \" I was looking at code and found this:  I was keen to know about  but wasn't able to understand it's significance? \", 'Tags': {'neural-network', 'machine-learning', 'deep-learning', 'keras', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37367/how-does-stanford-crf-encode-ner-string-features', 'post': \" Most features created by the  are strings e.g. from , ,  etc. From my understanding, that's too many tokens to fit in a dictionary or to use embeddings. I don't see how the  embedding would bring any value given that most features are not known words. I've been looking at the code on Github but haven't figured it out yet.  >  > , , , ,  etc \", 'Tags': {'feature-extraction', 'machine-learning', 'stanford-nlp', 'named-entity-recognition'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37363/help-framing-a-sequence-prediction-problem', 'post': \" I've found lots of tutorial/examples that focus on sequence prediction, which use previous time steps of the input variable(s) in order to create a forecast e.g. predict stock market price based on previous price. What if I want to make a sequence prediction for a variable which is not part of the inputs (e.g. using sequences of X1 and X2 to predict a sequence for X3 at the same time step(s)). Is this the same kind of problem in the previously mentioned tutorials, just in a different context? I've been trying to learn how to make a Sequential model in Keras, but first wanted to get some clarity on my problem and make sure i'm thinking about it in the right way.  If anyone can provide some insight into clearly framing the problem for me to start trying to build a model it would be most appreciated.   \", 'Tags': {'lstm', 'keras', 'machine-learning', 'rnn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37357/why-is-the-softmax-function-often-used-as-activation-function-of-output-layer-in', 'post': ' What special characteristics of the softmax function makes it a favourite choice as activation function in output layer of classification neural networks? ', 'Tags': {'neural-network', 'machine-learning', 'deep-learning', 'activation-function', 'classification'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37356/rules-of-thumb-relating-input-range-of-values-to-choice-of-activation-function', 'post': ' I would like to check with the experts on some observations I made about input value range and choice of activation function in deep learning neural networks. Here are some rules of thumbs I have;  When input values range from -1 to 1, use either tanh or sigmoid. When input values range from 0 and above (with no upper boundary), use ReLu.  Are these rules of thumbs valid? ', 'Tags': {'neural-network', 'activation-function', 'machine-learning', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37355/non-mutually-exclusive-classification-sum-of-probabilities', 'post': \" So I have the following problem: I realized (while writing my master thesis) that I am still not sure/have vague descriptions of some of the machine learning principles. I already asked one question regard definitions that can be found here. Now I stumbled over another definition Problem. Here is an excerpt from my thesis (this is in particular about neural-network classification):   If the classes are mutually exclusive (i.e. if a sample $x^{j} = C_{0}$, $x^{j} \\\\neq C_{i}\\\\setminus~C_{0}$ ), the probabilities of all classes add up to one like \\\\begin{equation} \\\\sum_{i} P(x^{j}=C_{i}) = 1. \\\\end{equation}   In this case the best practice is to use a softmax activation function for the output neurons.   If the classes are not mutually exclusive it would suffice to use a sigmoid output activation function, as the sigmoid function gets independent probabilities for each class \\\\begin{equation} \\\\sum_{i} P(x^{j}=C_{i}) \\\\geq 1. \\\\end{equation}  I already found the following link regarding this topic. However I know that in practise if you don't use softmax activation function in your output layer, the value can be larger than 1 but can a probability be larger 1? Isn't that against its definition?  Is a non-mutual classification really a common case? Can somebody may be linking some cases (paper?) were they needed non-mutual classification? \", 'Tags': {'neural-network', 'classification', 'machine-learning', 'probability'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37345/what-is-the-meaning-of-term-variance-in-machine-learning-model', 'post': ' I am familiar with terms high bias and high variance and their effect on the model. Basically your model has high variance when it is too complex and sensitive too even outliers. But recently I was asked the meaning of term Variance in machine learning model in one of the interview? I would like to know what exactly Variance means in ML Model and how does it get introduce in your model? I would really appreciate if someone could explain this with an example. ', 'Tags': {'variance', 'machine-learning'}}\n",
            "page number ------------------------------------92\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37344/weights-and-bias-set-to-nan', 'post': ' I am performing linear regression on one of UCI Machine learning repository data set. Below is the code :-  Now, weights and biases get set to Nan only after single step.  What all reasons could result weight and biases carry Nan? ', 'Tags': {'tensorflow', 'machine-learning', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37328/which-type-of-clustering-algorithm-to-use-to-identify-the-same-item-in-differe', 'post': \" I'm trying to find a solution for a data quality problem - specifically, identifying which items in different data sets are used to represent the same things. As an example, assume that we're a retailer and we buy out a couple of other retailers.  In the process, we also get their systems and databases.  This might lead to some overlap - different systems can represent the same items, customers, etc. in different ways, but with no single unique identifier. What would the best approach be to determine which rows represent the same thing across data sets in order to come up with a single view of 'unique' entities? I've done a machine learning course, and I understand the bare minimum.  I believe that the solution to this problem requires a clustering algorithm, but what type?  I may be dealing with a multitude of features in the data - dimensions, names, dates, contact details - and some of those would probably require a higher 'weight' in matching. Examples (Items): A:  ABC Notebook, Large, Released: 2018-02-20, 150mm x 100mm. B:  Notebook (ABC), L, Date: 18/02/2018, 150mm x 100mm I'd expect these to be treated as the same item. Examples (Customer): A:  Doe, Jane, DOB 1970-06-23, 123 ML Ave, F B:  John Doe, DOB 1971-04-33, 123 ML Avenue, M C:  J. Doe, Born '71 I'd expected B and C to be identified as the same person, but not A. For this scenario, I don't expect to come up with something with a 100% accuracy, but I would like to be able to come up with a (narrow) list of possible matches that someone can check. Can some please point me in the right direction?  Any case studies that I should look at?  \", 'Tags': {'algorithms', 'machine-learning', 'clustering'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37325/why-would-a-validation-set-wear-out-slower-than-a-test-set', 'post': ' On this page of Google\\'s Machine Learning Crash Course, we find the following statement:  \"Test sets and validation sets \"wear out\" with repeated use. That is, the more you use the same data to make decisions about hyperparameter settings or other model improvements, the less confidence you\\'ll have that these results actually generalize to new, unseen data. Note that validation sets typically wear out more slowly than test sets.\" I don\\'t get the last sentence. Why would a validation set wear out slower than a test set? To me it would wear out faster as the validation set is used a lot more frequently than the test set. What am I missing? ', 'Tags': {'training', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37322/newbie-in-ml-how-to', 'post': '       Closed. This question needs to be more focused. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Update the question so it focuses on one problem only by editing this post.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         We have a data set of n variables (profile attributes) and want to feed through a model, and classify into M buckets (functionally signifying some action to be performed) .  Which MLmodel/ algorithm is best suited for this. Can someone point me some code. ', 'Tags': {'machine-learning', 'machine-learning-model'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37319/algorithms-that-can-determine-whether-a-string-is-an-english-sentence', 'post': ' In particular, I\\'m looking for something that can distinguish between complete sentences and sentence-like grammatical structures like clauses and phrases. It would also be very helpful if a library with this functionality was shared, but even a link to a paper would be appreciated. For example- I would like \"False\" to be returned for strings like:  \\'Via The Daily Currant,\\'  “Waiting for the rain to stop” \\'查看更多请返回网站主页。\\' \\'*  *  *\\' I would like \"True\" to be returned for things like: \"This is a complete sentence.\" \"I don\\'t think there will ever be a Half Life 3, unfortunately.\" Thank you! ', 'Tags': {'machine-learning', 'nlp'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37316/neuroevolution-library-framework-with-gpu-cuda-support', 'post': \" I'm looking for working library/framework allowing you to use neuroevolution algorithms like NEAT with GPU support (CUDA). Are there any working libraries? I know about AccNEAT library but I couldn't get it to work with CUDA according to README, probably it's not possible. Any others? \", 'Tags': {'machine-learning', 'genetic-algorithms', 'gpu'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37314/bias-of-1-in-fully-connected-layers-introduced-dying-relu-problem', 'post': \" While implementing AlexNet (model-code), one of the thing I need to do was to initialize the biases of the convolutional layers and fully connected layers. Normally we initialize biases with 0s, but the paper says:  We initialized the neuron biases in the second, fourth, and fifth convolutional layers,   as well as in the fully-connected hidden layers, with the constant 1.  So I went ahead and initialized the biases to 1 as the paper says. But that didn't make the network learn at all. Basically the last fully connected layer was producing a lot of 0s, which is otherwise known as dying-relu-problem. Out of 4096 neurons only 40 or 50 were producing non-zeros. After lot of debugging, I came to realize that: if I make the fully connected layers' bias to 0 than they are nicely learning. loss decreased nicely. Now I'm wondering:  How bias plays the role for dying-relu-problem here ? Can all dying-relu-problem be corrected using bias searching ?  \", 'Tags': {'machine-learning', 'convolution', 'deep-learning', 'bias', 'convnet'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37307/is-there-an-api-to-extract-questions-and-labels-from-yahoo-answers', 'post': ' I want to work on a ML project which involves the family and relationship category from yahoo answers. I want to extract questions from yahoo answers (only the title) with the label as the category it is in on yahoo answers. I want to use it in csv form to train my naive bayes model. Is there an API or something similar to do so ? ', 'Tags': {'data', 'naive-bayes-classifier', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37304/grouping-prefix-and-term-queries', 'post': ' Consider the following context: You have a website that is offering a search function, which produces a list of matching articles. The search has prefix functionality: if you are looking for \"Samsung Galaxy\", the user can also input \"Sams Galax\" and he will still get a list of results. Now what you try to find out, how many users are using shortened (prefix-) and how many are using full terms. As your dataset you have a list of customer-executed queries which also contains a number how many articles were in the produced results. But you have no data which of the resulted articles the users finally clicked on. So you can\\'t easily indicate whether the search was successful or not and what exactly the users were looking for. How would you analyze the data to split it in the two groups, e.g. prefix- and full-term?  ', 'Tags': {'classification', 'bigdata', 'machine-learning', 'natural-language-process'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37302/unbalanced-multi-class-distribution-might-change-as-more-data-come-in', 'post': \" I am currently working on a problem of multi-class classification on testing logs data.  Basically, I have the context data from tests' execution saved, and want to automate the analysis of the failed tests. The two targets I have are the page (which corresponds to the working team) responsible for the error, and the type of error.  So, for this matter I got the data at a time t and worked on it locally in order to get the best results possible.  I trained a few models, optimized them through hyper-parameter optimization with grid search, and got good results. The f1_scores are around 90+% for the target page and 80+% for the target type.  These results seem good so I was working on ensembling methods to then get even better results from the combinations of my different models.  On the following figures, you can see the f1 scores and variances of different models for the target page.  But as I was working on ways to do ensembling, I got the idea of getting the latest testing data (remember I was working with the same data from the time t).  I went from having only 2162 samples to 4367, so a bit more than double the amount of data. This gives my data a distribution different from before (on the following figures we can see the number of values per class for the target page).  Above, we can see that there are 11 highly unbalanced classes.  And on the second figure, we can see that we have now 14 classes, also very unbalanced. (The differences with the new data are also true for the target type). It resulted in my models loosing between 6 and 10% of f1_score. So, my questions are the following :   How do I work around data in this case in order for the models to work well with new data even when the distribution might change ? Will I need to check the evolution of the scores and when it goes too low, redo the parameter optimization ?  I am for now working with each target independently. Should I try multi-labeling between both targets ? If so, do you have any recommendation on algorithms or ways to go around it ?  Any suggestion on how to improve the work, maybe different models to try, or ensembling methods would be greatly appreciated ☺  Thanks for reading this long post ! \", 'Tags': {'machine-learning', 'multiclass-classification', 'python', 'unbalanced-classes', 'scikit-learn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37293/how-to-recommend-items-after-finding-similar-users-in-recommendation-system', 'post': \" As the title explains my problem, I'm done with creating a recommendation system that can give me similar users for any given new user. The problem I face is, If I extract the list of products that these similar users liked the most, how do I weight these items and recommend, say only 3 of these items. \", 'Tags': {'machine-learning', 'data-mining', 'recommender-system'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37287/machine-learning-train-test-split-function-in-scikit-learn-should-i-repeat', 'post': ' I am a beginner in machine learning, and I hope someone can help me. In Python\\'s \\'scikit-learn\\' library, the function \\'train_test_split\\' splits the dataset into training and test sets. This is done in a random way (possibly using a seed to obtain the same result in repeated executions). But, with a single random split, how much can we trust the result (classification accuracy) obtained through the \\'fit\\' method? I mean... if we are \"unlucky\", we may obtain a very bad outcome (or the opposite, if we are \"lucky\"). Shouldn\\'t we repeat the split/fit procedures multiple times (e.g., 100) and then average the classification accuracies obtained? (possibly after parameter tuning by means of cross validation). I am asking this because previously I used the Python\\'s libraries of Orange Data Mining, which include a method (\\'proportion_test\\') that splits the dataset into training and test sets and then evaluates it according to a specific classifier, repeating the operation a specified number of times (e.g., it performs 100 iterations of 70:30 test). My question is: should I manually do this also with the split/fit functions in scikit-learn? (e.g., 100 iterations using 100 different random seeds). Would results be better? I am very confused about this... I want to stress that I know about cross validation, leave one out, etc. But, if I understand it right, these techniques are used for model validation (i.e., model parameter tuning). My question is whether the final evaluation of the model should be based on repeated split/fit operations. For example, in the book \\'Introduction to Machine Learning with Python\\' (by Andreas C. Müller and Sarah Guido, O\\'Reilly), the suggested operation pipeline is: (1) split the original dataset into training set and test set; (2) perform parameter tuning (i.e., best parameter selection) using cross validation on the training set; (3) re-train using the just found best parameters with the training set; (4) perform the final evaluation (calculating the classification accuracy) using the trained model and the (single) test set. My question is: is this enough? Or, once the classifier has been trained using the best parameters (step 3 above), should I repeat the split/fit procedures multiple times on the whole dataset (original training + test sets) to obtain more reliable results? I did this using the Orange library, but maybe it is not necessary (it is not done in the book quoted above). THANK YOU VERY MUCH in advance for your help! ', 'Tags': {'scikit-learn', 'machine-learning', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37286/identifying-features-which-drives-recommendations', 'post': \" I'm want to a recommendation engine for one of the clients to recommend products to his customers. One of the requirement is that recommendations should be explainable. i.e. Why does the recommendation engine recommend certain products to a given customer, based on which features? I'm thinking of using either hybrid models (content based and collaborative filtering) or deep and wide models. However, I'm not sure whether there is any way to identify features which drive recommendations. Are there any other models in which results are more interpretable?  \", 'Tags': {'machine-learning', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37282/cameras-for-automatic-customer-service-machine', 'post': \"       Closed. This question is off-topic. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Update the question so it's on-topic for Data Science Stack Exchange.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         For my university project, I am planning to build an automated customer service machine. One which recognizes when someone approaches the camera according to says hello, etc. Also, I am planning to add simple speech recognition and language processing features. So my question is. What kind of camera would be suitable? Is there any particular model that you recommend. I was thinking of cameras used for amazon go(as an example). \", 'Tags': {'machine-learning', 'image-classification', 'deep-learning', 'image-recognition', 'project-planning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37274/impact-of-sparse-features-on-tree-based-models', 'post': \" Say you have a highly imbalanced binary classification problem. Some of the features are binary features, where they're false most of the time, but when they're true they tend to be highly predictive (of the minority class). What impact, if any, would the sparseness of the feature have on the likelihood of the feature being picked as a split? \", 'Tags': {'machine-learning', 'decision-trees', 'classification', 'xgboost', 'data-mining'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37265/loss-function-not-working-rnn', 'post': \" I am building an RNN by following Siraj Raval's video implementation. I've adapted it to use my dataset rather than importing one from a file. When the program gets to the loss function, it reports       What does this mean and how can i fix this? Also, in Siraj's version, he calculates the loss as:  Shouldn't it be this instead  as the cross entropy loss is L = -yln(yhat)? My code:  \", 'Tags': {'neural-network', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37262/dealing-with-feature-vectors-of-variable-length', 'post': \" How does one deal with a feature vector that can vary in size? Let's say per object, I calculate 4 features. In order to solve a certain regression problem, I may have 1, 2, or more of these objects (no more than 10).  Thus, the feature vector is 4*N in length. How is this normally addressed? The objects represent physical objects (e.g. other people) w.r.t. an observer. For a time slice, an object can be placed laterally, longitudinally, have some speed and some heading (4 features). Trying to solve: where should a person feel most comfortable. In some cases there is only 1 object, but there can be 2 or more. Disclaimer: I have limited knowledge on ML approaches. I had classes in college years ago and took Andrew Ng's ML course online as a refresher but otherwise not up to speed. A starting place to look is appreciated.  \", 'Tags': {'linear-regression', 'machine-learning', 'regression'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37255/combining-outputs-of-ridge-regression-models', 'post': ' I am facing an issue where I have 7 sets of different variables/columns/predictors. I am trying to predict same target variable and I want to observe the importance/effect of all the sets according to their importance in an ordered manner. (I am trying to use ridge regression models for each of the 7 individual set as I want to keep all the variables and I want to combine the output of these 7 models, each set has more than 20 variables) ', 'Tags': {'machine-learning', 'regression', 'r', 'glm', 'ensemble-modeling'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37249/how-would-code-this-in-nn', 'post': '  is there any python code already available to do this? - I would appreciate if you can elaborate on coding of this problem as I am new to NN. ', 'Tags': {'lstm', 'rnn', 'machine-learning', 'cnn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37238/which-features-of-a-data-set-can-be-used-for-market-campaigning-using-propensity', 'post': ' A dataset contains so many fields in which there is both relevant and irrelevant field. If we want to do a market campaigning using propensity scoring, which fields of the data set are relevant? How can we find which data field should be selected and which drive to the desired propensity score? ', 'Tags': {'dataset', 'machine-learning', 'probability', 'marketing'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37236/why-convolution-over-volume-sums-up-across-channels', 'post': \" A simple question about convolution over volume . Say we have an image with dimensions $(n, n, 3)$ and we apply a filter of dimension $(k, k, 3)$ this outputs an matrix of dimension $(n-k+1, n-k+1)$.  Why do we sum across channels in this case. Don't we lose information by mixing different channels. In case of images, this implies mixing information in  channels? For ex. when trying to detect traffic signal lights, such mixing can be fatal.  \", 'Tags': {'neural-network', 'cnn', 'machine-learning', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37219/how-to-plot-high-dimensional-supervised-k-means-on-a-2d-plot-chart', 'post': ' I\\'m Having a ML problem where my data set contains 80 features labelled into 3 groups (0, 1, -1). I want to plot the data on a 2D surface to see how \"close\" (similar) data with  is to data with , how the data spreads, are the labels separable, etc. I was thinking about using PCA and transform the data from 80D to 2D, but It only retain 40% of the variance!  Is this a good approach for the problem?  If so, does 40% suffice? Are there any other/better approach for this?  EDIT: Plotting is not the main issue. The transformation from 80D to 2D (for an easy visialization) is whats difficult.  Also, all of this is being made to know how much samples with  differs from  and  and vice versa (based on those original 80 features). If there\\'s a different method, that is not visualizing the \"answer\", I\\'ll also be happy to hear about it! ', 'Tags': {'machine-learning', 'visualization', 'pca', 'plotting', 'k-means'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37211/implement-sliding-window-to-detect-text-in-an-image', 'post': '       Closed. This question needs to be more focused. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Update the question so it focuses on one problem only by editing this post.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         I am working on a text detection in the images. How can I implement a sliding window in a python to detect text in an image? ', 'Tags': {'machine-learning', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37208/simplifying-gradients-of-weights-rnn', 'post': ' I understand that these are the gradients of the weights/biases in an RNN (correct me if I am wrong):  This is a lot to compute and I’m aware that these equations can be simplified for ease of use. My question is what are these simpler versions, and how are they derived? I am using 6 timesteps in my network, hence the numbers used above. Here are the formulas for forward prop, just for context:  One other thing. I just want to make sure I’m computing the loss correctly. I’m computing y-yhat at each time step, and appending it to an array, which at the end will have the same dimensions as x ([6, 4] - a very tiny dataset, I know.) ', 'Tags': {'neural-network', 'rnn', 'machine-learning', 'backpropagation'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37193/taking-neural-networks-false-positives-as-the-recommendation-system-result', 'post': \" I am creating a recommendation system and considering two parallel ways of formalizing the problem. One classical, using proximity (recommend the product to the customer if a majority vote of 2k+1 customers closest by has the product), and another one that I have trouble understanding but seems valid to some extent. The approach I'm thinking about is:  1) Fit a highly regularized neural network (to make sure it doesn't overfit the training set) for a classification task that can predict if the person does or doesn't have given product 2) Make sure test accuracy is as close to train accuracy as possible 3) Take false positives (customers who don't have the product originally but the NN predicted that they have it) predicted on the whole dataset (the training set as well) as the result - the people I should recommend the product to Now, I am aware of why in general one wouldn't want to take that approach but I also can't exactly explain why it wouldn't return people 'close by' to each other that 'should' have given product in a similar sense like the KNN-based approach. I'm not sure how to analyse this problem exactly to validate, modify or reject the idea altogether. \", 'Tags': {'neural-network', 'supervised-learning', 'machine-learning', 'recommender-system', 'k-nn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37191/train-and-predict-on-different-data-structures', 'post': ' Is it possible to train a Machine Learning model with a set of the same data structure, but use another data structure as input in prediction/classification ? If so, how ?  Let me elaborate and explain my motivation : I am a summer intern in a research lab (and will be starting my last year in Mathematical Engineering, so I am not too deep into ML yet, I will be specializing this year). I have followed the work of a previous intern on Chronicle Mining. A Sequence is a list of timestamped events, e.g. <({A}, 1), ({A, C}, 2), ({A, B}, 4), ({B, C}, 7)> is a sequence in which the event  happens at time 1, the events  and  happen at time 2, the events  and  happen at time 4 and the events  and  happen at time 7. A Chronicle is a data structure based on graphs. It translates time constraints between events, or in my case, sequences of events.  This tells us that a first  event must come between 1 and 3 units of time before the sequence , but  must come between 1 and 3 units of time before a second  event and between -2 and 2 units of time before  (so  can actually come before ). A first  event must come between 1 and 4 units of time before , and you get the rest. So as you can see, it is a directed graph, there can be mutliple nodes with the same label (which are sequences of events), multiple vertices with the same label (which are time intervals) and there can be negative values within the intervals. In my case, events are partially ordered (in this example it is lexicographical order) to make some things simpler. I have accomplished efficient extraction of these weird things from sequential databases, so let\\'s say I have a lot of them. In my case, there is always a final state (like the middle  in the example), and it is always the same (all sequences end with the same event/sequence). Now, my tutor would like me to \"perform Machine Learning\" on these graphs. But there is a catch : the prediction/classification input has not the same shape as the training input. He would like for me to feed the algorithm a simple sequence (not a chronicle) and output some results. Thus, I have come up with two problems :  Given a large set of chronicles and a partial sequence (without the common final state), predict the sequence (and intervals between each event) between the last event on the sequence and the common final state. Given a large set of chronicles and a partial sequence (without the common final state), tell whether the next k-th step will be the common final state.  Both are similar, but one is a prediction problem and the second one is a classification problem. However, in both cases, I don\\'t know where to start ! I have read papers on k-th steps sequence prediction with LSTM NN and other techniques, but what gets me is the fact that I apparently have to train my model with chronicles, but predict using sequences. Is that even possible ? Do I have to transform my chronicles in some kind of way ? ', 'Tags': {'training', 'machine-learning', 'graphs'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37186/early-stopping-on-validation-loss-or-on-accuracy', 'post': ' I am currently training a neural network and I cannot decide which to use to implement my Early Stopping criteria: validation loss or a metrics like accuracy/f1score/auc/whatever calculated on the validation set. In my research, I came upon articles defending both standpoints. Keras seems to default to the validation loss but I have also come across convincing answers for the opposite approach (e.g. here). Anyone has directions on when to use preferably the validation loss and when to use a specific metric? ', 'Tags': {'neural-network', 'classification', 'machine-learning', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37181/should-highly-correlated-features-be-omitted-before-applying-lasso', 'post': \" I would greatly appreciate if you could let me know whether I should omit highly correlated features before using Lasso logistic regression () to do feature selection. In fact, I want to use logistic regression with  to do prediction as well as feature selection. However, some of my features are highly correlated e.g., -1 or 0.9. Should I omit them before applying Lasso or let the Lasso decide it? Really, I read in Mr. Raschka’s book (Python Machine Learning) that   regularization is a very useful method to handle collinearity (high   correlation among features).  However, this kernel  (by referring to Wikipedia) states that keeping correlated features in the model would adversely affect the feature selection but it doesn't impair the predictions.  \", 'Tags': {'collinearity', 'machine-learning', 'regularization', 'python', 'logistic-regression'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37179/time-series-pixel-classification', 'post': ' Working on an classification problem with images at the pixel level using either keras(tf) or pytorch. The images are all 10,000 pixels wide and high. To elaborate there are two kinds of pixels in this image \"x\" pixels and \"y\" pixels. It is impossible to tell them apart in a single image. However, the pixels for \"x\" change in a very specific way over time and the idea is to identify \"x\" as soon as possible. I have a stack of images (timestep of 50 images) involving these pixels. Additionally, \"x\" pixels are about 25% of the image while \"y\" pixels are the remaining 75%. The final image I have is a black and white image in which constitutes the labels named key.jpg. Basically \"x\" is black and \"y\" is white this is what is used to identify \"x\" and \"y\" pixels over time. The idea is to have the nn determine if a pixel is an \"x\" or a \"y\" pixel by the 5th timestep, that way I can use the nn on a new series of images that have \"x\" and \"y\" pixels in a different configuration have it classify them enabling the creation of a matching key.jpg for them. I theorize that I can solve this with a cnn or an rnn (lstm), but I\\'m not sure of how to implement them. The general idea I have stack the images above one another in the order of time (an array) then select 500 random pixels following them over time with 25% of them being \"x\" and 75% of them being \"y\". Use 200 pixels as training, 200 as validating and 100 for testing (while keeping the pixel ratios). The problem I have here is that I cannot envision what my next step should be like and I\\'m facing two problems.  I\\'m clearly working with time series data so a cnn might not be the right choice.  I cannot figure out how to use this in an rnn. Logically I can envision using a single time series, but since each pixel\\'s tuple/array is an individual time series and I\\'m not sure how to proceed.  Any help/suggestions would be greatly appreciated. ', 'Tags': {'machine-learning', 'keras', 'pytorch', 'time-series', 'classification'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37174/text-classification-with-thousands-of-output-classes-in-keras', 'post': ' Task: I have a dataset with job titles and descriptions. The task is to predict tags for job by job title and description. There are several tags for each job posting. Therefore, the number of labels for the model will be measured in tens of thousands. Number of job postings = 78042 Number of unique classes (tags) = 1369   Questions: Could you advise working types of neural networks (desirable in Keras)? Or maybe you know how to solve this problem with the help of classical machine learning algorithms? Still I will be very grateful for links to articles where similar problems are solved. ', 'Tags': {'neural-network', 'machine-learning', 'nlp', 'deep-learning', 'keras'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37168/high-rmse-and-mae-and-low-mape', 'post': ' I have used a few regression models on the same dataset and obtained error metrics for them as shown below,  The RMSE(Root Mean Squared Error) and MAE(Mean Absolute Error) for model A is lower than that of model B where the R2 score is higher in model A. According to my knowledge this means that model A provides better predictions than model B. But when considering the MAPE (Mean Absolute Percentage Error) model B seems to have a lower value than model A. I would really appreciate it if someone could explain why it is so. Thanks in advance.  ', 'Tags': {'machine-learning', 'regression', 'dataset', 'python', 'scikit-learn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37165/when-memory-errors-occur-with-model-fit-is-it-due-to-gpu-ram-or-normal-ram', 'post': ' With respect to this question,  https://stackoverflow.com/questions/51895278/how-to-know-when-to-use-fit-generator-in-keras-when-training-data-gets-too-big when memory errors are reported due to excessively large training data, are the memory errors caused by lack of normal PC RAM or lack of GPU RAM? I would like to know which one so that I can buy the right kind of RAM. Keras was configured to use GPU for training. ', 'Tags': {'keras', 'machine-learning', 'gpu'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37163/is-it-a-good-practice-to-always-apply-reducelronplateau-given-that-models-b', 'post': ' The rationale behind the keras function  is that models benefit from reducing learning rate once learning stagnates. Is it a good practice to always apply ? What are some situations, if any, to not apply ? ', 'Tags': {'neural-network', 'learning-rate', 'keras', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37154/is-this-a-problem-for-a-seq2seq-model', 'post': \" I'm struggling to find a tutorial/example which covers using an seq2seq model for sequential inputs other then text/translation. I have a multivariate dataset with n number of input variables each composed of sequences, and a single output sequence which is unrelated to any of the input variables (e.g. using weekly wind speed and humidity to predict temperature).  I've converted the features and label to batches with fixed time steps and n dimensions, however I'm confused which model should be used. Ideally the output should be a single sequence (e.g. annual temperature) however which model would achieve this? Is this something an LSTM could achieve, or is this a problem for a seq2seq model?  Any suggestions/insightful would be appreciated. \", 'Tags': {'sequence', 'lstm', 'machine-learning', 'sequence-to-sequence', 'rnn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37153/predicting-composition-of-chemical-compounds', 'post': \" I have a dataset which has names of compounds and their compositions. Like below Sulphuric Acid=>[H,S,O] (Hydrogen, sulphur, oxygen) Oxalic Acid=>[H,C,O] Sodium Oxalate=>[Na,C,O] Potassium Sulphate=>[K,S,O] ... Now I would need to train a model which can tell me Sodium Sulphate as [Na,S,O]. Note that sodium sulphate represents something not in training dataset. I have tried searching for possible ideas but nothing came up. Then I thought it could be hierarchical clustering like  But then in hierarchical clustering the base/leafs are different. But here they are shared. Its like a graph. So What machine learning algorithm can help? Any other clustering? NLP/word clustering (if yes How)? Another approach I could think of is like  where I generate embeddings for each word (C,H,Sodium), all will have embeddings. And based on what is closer to the word I am asking I will give output. But this needs huge amount of data. I only have around 1k common compounds. And the approach won't generalize to any problem like this with less data.  \", 'Tags': {'word-embeddings', 'machine-learning', 'predictive-modeling', 'deep-learning', 'clustering'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37126/what-would-be-the-main-and-essential-criteria-for-evaluating-auto-sklearn-librar', 'post': ' I m running experiments using benchmark datasets with auto-sklearn to see how its performance is different to the standard sklearn library, Since automl does an exhaustive search over parameters and sklearn has to be manually tuned. what could be the essential criteria to judge the performance between these two libraries ', 'Tags': {'metric', 'scikit-learn', 'machine-learning', 'evaluation'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37125/how-to-find-coreset-of-a-given-dataset-in-python', 'post': \" I am trying to implement the core-means algorithm, which is basically k-means using coreset. I have searched up and down but could not find any libraries or modules which could help me with this.  The paper I am following talks about building a coreset using grids, something like a quadtree where you keep dividing the point space in 4 equal parts to find heavy points which makes a coreset. Any help would be greatly appreciated in getting me started as to how should I go about implementing this and if there are any libraries/modules available to help me do so. PS: I'm new to python and machine learning and Hi StackExchange! \", 'Tags': {'k-means', 'machine-learning', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37116/why-is-the-derivative-of-the-product-of-a-vector-and-its-transpose-ctc2-c', 'post': \"       Closed. This question needs details or clarity. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Add details and clarify the problem by editing this post.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         I'm learning principal component analysis and i have to derive the result that will minimize the error $$c^* = \\\\arg \\\\min{[x-g(c)]^2} = \\\\arg \\\\min{[x-g(c)]^\\\\mathsf{T}[x-g(c)]}$$. What I have done so far is to simplify the error by expanding the argument, replacing $g(c)$ by its definition $g(c)=Dc$, and use the fact that $D\\\\mathsf{T}D=I$: $c^* = \\\\arg \\\\min [{x^\\\\mathsf{T}x - 2x^\\\\mathsf{T}g(c) + g(c)^\\\\mathsf{T}g(c)}]$ $c^* = x^\\\\mathsf{T}x - \\\\arg \\\\min {[ -2x^\\\\mathsf{T}g(c) + g(c)^\\\\mathsf{T}g(c)}]$ $c^* = x^\\\\mathsf{T}x - \\\\arg \\\\min {[ -2x^\\\\mathsf{T}g(c) + (Dc)^\\\\mathsf{T}(Dc)]}$ $c^* = x^\\\\mathsf{T}x - \\\\arg \\\\min{[ -2x^\\\\mathsf{T}g(c) + c^\\\\mathsf{T}D^\\\\mathsf{T}Dc}]$ $c^* = x^\\\\mathsf{T}x - \\\\arg \\\\min{[ -2x^\\\\mathsf{T}g(c) + c^\\\\mathsf{T}Ic]}$ $c^* = x^\\\\mathsf{T}x - \\\\arg \\\\min{[ -2x^\\\\mathsf{T}g(c) + c^\\\\mathsf{T}c]}$ $(−2x^\\\\mathsf{T}Dc + c^\\\\mathsf{T}c) = 0$ $-2D^\\\\mathsf{T}x+2c=0$ $c=D^\\\\mathsf{T}x$ \", 'Tags': {'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37109/importance-sampling-in-off-policy-n-step-sarsa', 'post': ' In Chapter 7.3 of Reinforcement Learning: An Introduction by Sutton and Barto, the off-policy pseudocode has the following update equation for $Q$:  Compute importance sampling ratio: $$ \\\\rho \\\\leftarrow \\\\prod^{\\\\min(\\\\tau+n-1, T-1)}_{i=\\\\tau+1} \\\\frac{\\\\pi(A_i \\\\mid S_i)}{b(A_i \\\\mid S_i)} $$ Compute truncated return $$ G \\\\leftarrow \\\\sum^{\\\\min(\\\\tau+n, T)}_{i = \\\\tau+1} \\\\gamma^{i-\\\\tau-1} R_i $$ Compute discounted estimate:   $$ \\\\text{If } \\\\tau+n < T, G \\\\leftarrow G + \\\\gamma^n Q(S_{\\\\tau+n}, A_{\\\\tau+n}) $$  Update $Q$  $$ Q(S_\\\\tau, A_\\\\tau) \\\\leftarrow Q(S_\\\\tau, A_\\\\tau) + \\\\alpha \\\\rho [G - Q(S_\\\\tau, A_\\\\tau)] $$ ($\\\\tau$ is the time whose estimate is being updated, $t$ is current timestep, $n$ is $n$-step return, and $T$ is termination timestep) I believe that the importance sampling ratio $\\\\rho$ should only be multiplied to $G$ and not to $Q$, since $Q$ is for target policy $\\\\pi$ and $G$ is from behavior policy $b$. In other words, this is how I see this equation: $$ Q_\\\\pi \\\\leftarrow Q_\\\\pi + \\\\alpha \\\\rho_{b \\\\to \\\\pi} [G_b - Q_\\\\pi]$$ Am I correct in thinking that this is a typo, or did I miss something? Thank you for your help! ', 'Tags': {'reinforcement-learning', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37103/input-data-for-this-dataset-to-be-feed-into-keras-for-training', 'post': ' Suppose I have 3 csv files which forms the dataset for training a machine learning model in Keras. file1.csv  file2.csv  file3.csv  Based on the data, I will classify whether the person has good or bad performance. For the data above, Joe has good performance while the rest have bad performance. In keras, the above dataset will be transformed into numpy ndarrays  and  to be fed into  like below;  I am confused over how  and  should look like. What should be the shape of  and ? Suppose I have the following dataframes read from the csv files.  How should I use these dataframes to get  and ? I am using python v3, keras with tensorflow. ', 'Tags': {'machine-learning', 'keras', 'preprocessing', 'numpy', 'classification'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37087/improving-the-accuracy-of-my-activity-classifier', 'post': \" I'm extremely new to machine learning.  I recently discovered  and wanted to use it on a project to classify data spit out by the accelerometer and gyroscope.  Not wanting to collect my own data, I downloaded the very large HAPT data set from UC Irvine.  I have followed along with the  tutorial found here. This walks you thru how to combine the HAPT Data set into sessions, create the SFrame, and create a model from it via splitting it into test and train data sets. Then I export the model as a  file to import in to XCode so I can try using it with CoreML and CoreMotion for an app I'm building.  I have a single ViewController in the app, all it does right now is ingest accelerometer and gyrometer data, feed it into a prediction window, and then try to predict based on the data in that window. It's not doing an excellent job, I'm wondering what I can do to improve the quality of this. I'm also wondering if I'm doing something wrong or poorly, since I'm new to this I'm sure I wouldnt easily recognize if I'm doing something wrong.  My code for the ViewController is here. I'd love any feedback on how I can improve this and hopefully receive better accuracy. Right now it feels like its 40% accurate, but when running the evaluation of the model in Spyder it claims 86% accuracy.  Thanks for your time! \", 'Tags': {'sensors', 'machine-learning', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37084/how-to-visualize-inceptionv3-hidden-layers', 'post': ' I am following google codelab: Tensorflow for poet  to train my custom model. This google codelab use the Inception-V3 model for training. The inception-V3 model have 48 layer. My question is that how can i visualize image features at the hidden layers? ', 'Tags': {'tensorflow', 'machine-learning', 'inception', 'machine-learning-model'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37081/what-is-wrong-with-this-reinforcement-learning-environment', 'post': \" I'm working on below reinforcement learning problem: I have bottle of fix capacity (say 5 liters). At the bottom of bottle there is cock to remove water. The distribution of removal of water is not fixed. we can remove any amount of water from bottle, i.e. any continuous value between [0, 5].  At the top of the bottle one tap is mounted to fill water in the bottle.  RL agent can fill [0, 1, 2, 3, 4] liters in the bottle. Initial bottle level is any value between [0, 5]. I want to train the agent in this environment to get optimal sequence of  actions such that bottle will not get empty and overflow which implies continuous supply of water demand. Action space = [0, 1, 2, 3, 4] Discrete Space Observation Space = [0, Capacity of Bottle] i.e. [0, 5] Continuous Space Reward logic = if bottle empty due to action give negative rewards; if bottle overflow due to action give negative rewards I have decided to use python to create an environment.   After running the 1000 epoch, I observed that agent has not learned anything. Unable to find out whats going wrong.  \", 'Tags': {'dqn', 'machine-learning', 'deep-learning', 'reinforcement-learning', 'openai-gym'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37079/how-can-relu-ever-fit-the-curve-of-x%c2%b2', 'post': ' As far as I understand (pardon me if I am wrong) the activation functions in a neural network go through the following transformations:  Multiplication by constants(weights) to x ( $f(ax)$ , $f(x)$ being the activation function). Recursive substitution $f(f(x))$.  Now with the above transformations a ReLU activation function should never be able to fit a x² curve. It can approximate, but as the input grows the error of that approximated function will also grow exponentially, right? Now x² is a simple curve. How can ReLU perform better for real data which will be way more complicated than x²? I am new to machine learning. So please pardon me if there are any blunders in anything I am assuming. ', 'Tags': {'activation-function', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37078/source-of-arthur-samuels-definition-of-machine-learning', 'post': ' Many people seem to agree that Arthur Samuel wrote or said in 1959 that machine learning is the \"Field of study that gives computers the ability to learn without being explicitly programmed\".  For example the quote is contained in this page, that one and in Andrew Ng\\'s ML course. Several articles also contain this quote, and the reference is always the following article, which doesn\\'t actually contain the quote. Samuel, A. L. (1959). Some studies in machine learning using the game of checkers. IBM Journal of research and development, 3(3), 210-229. Is there a reliable source? Or is this actually not a quote, but rather an interpretation of Samuel\\'s article? ', 'Tags': {'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37067/training-accuracy-stuck-in-keras', 'post': \" I have trained a CNN using keras for Image classification with 3 classes. The results are bad and I'm trying to understand what the classifier has learnt and what it has not. It's only giving me an output of 1 class.  I have made changes to the learning rate, activation(relu, sigmoid and softmax for last layer), changed the architecture and the optimizer(SGD and Adam) but the training accuracy is stuck at ~33.33%. It's definitely not a coincidence because I only have 3 classes.  My present architecture is   My first 2 conv layers have 64 filters of size (3, 3) and the remaining conv layers have 32 filters of the same size.  My Dense layer goes like this 128 units-> 64 units -> 3 units. I started with a simple model and made it more complex to improve it. But there has been no improvement after any of these changes.  I have used two activations 'relu' and 'sigmoid' for experimental purposes. I'm thinking of only using sigmoid and softmax for the last layer. I have ~13000 images to train and 1400 for validation. The distribution is almost equal among the 3 classes. I was using this syntax to add the activation. The summary didn't show any activation layers. And my network wasn't improving.  Edit: updated Network. But when I add Activation as a new Layer the architecture changes. And now my network seems to work.    Directory structure:  I think that was the problem in my network. That the argument activation wasn't working as expected and no activations were performed on the network input. What I don't understand is that both syntax's are equivalent(according to the documentation) and yet are producing different results. \", 'Tags': {'neural-network', 'keras', 'machine-learning', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37066/deep-learning-does-starting-the-training-on-a-smaller-subset-of-the-data-make-s', 'post': ' I trained a deep neural network with a small subset of my data, which allowed me to go through many epochs in a short amount of time and allowed the model to perform reasonably, then I gave it the entire data set (10X more data) and it improved further. When I give it just the entire data set the model seems to perform terribly, but also it was difficult to reach many epochs due to the extra time needed for training. My question is: does it make sense to \"warm up\" the training a large deep neural network with a smaller subset of the data, or should one always just provide the full data?  To add a bit more detail: I had a heavy class imbalance. The model always predicts 0 when I give it the full data set, but when I overfit on a smaller training set, it learns at least to not always predict 0. I notice this guide in step 5, suggests something similar to what I did, but I am not sure if it is theoretically sound or just a nice way of checking the model \"can work given enough epochs\".  ', 'Tags': {'neural-network', 'machine-learning', 'data', 'class-imbalance', 'deep-network'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37064/custom-conditional-keras-metric', 'post': ' I am trying to create the following metric for my neural network using keras $$ s = \\\\left\\\\{         \\\\begin{array}{ll}             \\\\sum_{i=1}^{n} e^{\\\\frac{-d_i}{10}}-1 & \\\\quad d < 0 \\\\\\\\             \\\\sum_{i=1}^{n} e^{\\\\frac{d_i}{13}}-1 & \\\\quad d \\\\geq 0         \\\\end{array}     \\\\right. $$ where $d_i=y_{pred}-y_{true}$ and both $y_{pred}$ and $y_{true}$ are vectors With the following code:  For the use of compiling my model:   I received the following error code and I have not been able to correct the issue. Any help would be appreciated.  raise TypeError(\"Using a  as a Python  is not   allowed. \" \"Use  instead of  to test if a \"   \"tensor is defined, and use TensorFlow ops such as \" TypeError: Using a  as a Python  is not allowed. Use    instead of  to test if a tensor is defined,   and use TensorFlow ops such as tf.cond to execute subgraphs   conditioned on the value of a tensor.  ', 'Tags': {'neural-network', 'tensorflow', 'machine-learning', 'keras', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37045/should-the-cost-function-be-zero-using-tensorflows-sigmoid-cross-entropy-with-l', 'post': \" I'm building a CNN to make a binary classification (1 or zero). For this, I'm using the cost function . But for some reason, the cost using this function is never equal to zero even if the prediction is equal to the correct valuel. I tried plotting the output using the formula on TensorFlow's website: https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits This formula:  And by making this plot, I realized that it really isn't zero when the outputs are equal. For example, if z = 0 and x = 0, the result of this function is ~0.693. This isn't really making sense to me. Can someone shed some light on why it isn't zero when the prediction is correct? \", 'Tags': {'cost-function', 'tensorflow', 'machine-learning', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37027/how-to-give-rewards-to-actions-in-rl', 'post': \" I'm working on below reinforcement learning problem: I have bottle of fix capacity (say 5 liters). At the bottom of bottle there is cock to remove water. The distribution of removal of water is not fixed. we can remove any amount of water from bottle, i.e. any continuous value between [0, 5].  At the top of the bottle one tap is mounted to fill water in the bottle.  RL agent can fill [0, 1, 2, 3, 4] liters in the bottle. Initial bottle level is any value between [0, 5]. I want to train the agent in this environment to get optimal sequence of  actions such that bottle will not get empty and overflow which implies continuous supply of water demand. Action space = [0, 1, 2, 3, 4] Discrete Space Observation Space = [0, Capacity of Bottle] i.e. [0, 5] Continuous Space Reward logic = if bottle empty due to action give negative rewards; if bottle overflow due to action give negative rewards I have decided to use python to create an environment.  I'm new to RL. I'm not aware about for which condition I have to give the rewards. Is my reward logic is correct or I have to change it? \", 'Tags': {'dqn', 'machine-learning', 'deep-learning', 'reinforcement-learning', 'python'}}\n",
            "page number ------------------------------------93\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37025/nlp-how-to-perform-semantic-analysis', 'post': ' I\\'d like to perform a textual/sentiment analysis. I was able to analyse samples with 3 labels:  and I used algorithms such as SVM, Random Forest, Logistic Regression and Gradient Boosting. My script works correctly and with the cross validation I can take the best algorithm among the 4.  I use supervised algorithms with the python function \"Countvectorizer\"  But my boss typed \"NLP\" on the internet and looked at some articles. He told me : \"These 3 outputs are not enough, I want a complete semantic analysis that can explain the global meaning of the sentence\"  He didn\\'t seem to have a preference between supervised and unsupervised algorithms. He told me that he wanted an algorithm able to tell that \"The company president is behind bars\" is equivalent to \"the CEO is in jail\".  So do you have any idea how one could perform that ? And how to implement it in Python? I guess we need a great database full of words, I know this is not a very specific question but I\\'d like to present him all the solutions. What scares me is that he don\\'t seem to know a lot about it, for example he told me \"you have to reduce the high dimension of your dataset\" , while my dataset is just 2000 text fields. Thank you very much for your answers :) ', 'Tags': {'machine-learning', 'nlp', 'sentiment-analysis', 'stanford-nlp', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37022/intuition-importance-of-intermediate-supervision-in-deep-learning', 'post': \" These days, I have seen many papers using intermediate supervision. Single Network When using a single neural network, multiple neurons output predictions, perhaps by processing data in different ways. Then, the loss function sums up the individual losses computed over each prediction. For example, consider a part of the FlowNet architecture below:  In this network, all the  layers (, , ...) output predictions at different stages of the network. Then, the loss function is computed by applying a mean-squared-error over all these predictions separately and adding them all. Multiple Networks When using multiple networks, like in Stacked Hourglass Networks:  Each individual network outputs a prediction, and the overall loss function is computed by computing a mean-squared error over each network's prediction and summing them all up. My question is: what is the intuition behind doing this? I thought that this will force the first network to learn to predict the task well, while the remaining networks just perform identity transformations. Why is this not observed in practice? Also, I have only seen this applied in CNNs, but I could be wrong. \", 'Tags': {'neural-network', 'supervised-learning', 'machine-learning', 'deep-learning', 'cnn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37021/why-does-adding-a-dropout-layer-improve-deep-machine-learning-performance-given', 'post': ' If removing some neurons results in a better performing model, why not use a simpler neural network with fewer layers and fewer neurons in the first place? Why build a bigger, more complicated model in the beginning and suppress parts of it later? ', 'Tags': {'machine-learning', 'deep-learning', 'regularization', 'keras', 'dropout'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37011/reinforcement-algorithm-for-binary-classification', 'post': ' I am new to machine learning, but I\\'ve read a lot about Reinforcement Learning in the past 2 days. I have an application that fetches a list of projects (e.g. from Upwork). There is a moderator that manually accepts or rejects a project (based on some parameters explained below). If a project is accepted, I want to send a project proposal and if it is rejected, I\\'ll ignore it. I am looking to replace that moderator with AI (among other reasons) so I want to know which Reinforcement Algorithm should I use for this. Parameters: Some of the parameters that should decide whether the agent accepts or rejects the project are listed below. Assuming I only want to accept projects related to web development (specifically backend/server-side) here is how the parameters should influence the agent.  Sector: If the project is in related to IT sector it should have more chances of being accepted. Category: If the project is in the Web Development category it should have more chances of being accepted. Employer Rating: Employers having a rating of over 4 (out of 5) should have more chances of being accepted.  I thought Q-Learning or SARSA would be able to help me out but most of the examples that I saw were related to Cliff Walking problem where the states are dependent on each other which is not applicable in my case since each project is different from the previous one. Note: I want the agent to be self-learning so that if in the future I start rewarding it for front-end projects too, it should learn that behavior. Therefore, suggesting a \"pure\" supervised learning algorithm won\\'t work. Edit 1: I would like to add that I have data (sector, category, title, employer rating etc.) of 3000 projects along with whether that project was accepted or rejected by my moderator. ', 'Tags': {'classification', 'reinforcement-learning', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/37005/deep-q-learning-with-large-number-of-actions', 'post': \" I'm using DQN with large number of actions in [0, 10000, step = 1000]. This means I have an action space of size 11 (including 0 and 10000). Action space is still discrete. My problem is that, instead of choosing sequence of best actions, DQN is selecting zero as best action at every training epoch, which is not desired. At the starting of training exploration rate is one, as training goes on it decreases by 0.99. Any help in this is appreciated.   \", 'Tags': {'dqn', 'machine-learning', 'deep-learning', 'reinforcement-learning', 'q-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36995/is-it-a-good-idea-to-normalize-the-outputs-of-a-neural-network-for-regression-w', 'post': ' I understood that it is not necessary to scale the output of a neural Network when I predict a single value via regression. Is it necessary do normalize the Outputs of my neural Network if I have multiple outputs that vary in magnitudes between 10^-2 and 10^4? My intuition would tell me that the loss function might ignore the smaller values and only focus on the values of a bigger magnitude. ', 'Tags': {'neural-network', 'machine-learning', 'regression'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36992/process-mining-with-ml', 'post': ' I have a little more general question. My dataset consists of N sequences of events. Example of one sequence could be [A,B,C,D,X,Y] and another [A,B,Z], where letters represent different events. The sequences are at most 80 steps long.  The idea is to predict next letter or next step from known previous events. For very simple example maybe after A will always come B. Next step would be measuring time of each event and the ultimate goal is to predict how long until process reaches specific event.  I tried N-gram, MLP neural network and lastly LSTM network, which had around 80% accuracy. That would not be bad if the events were balanced in the dataset. To account for that I used weighted loss function in training of the LSTM and then the overall accuracy is around 66%. However the less frequent classes have much much higher accuracy (still not perfect, but higher). How can I create model that will have the best of both? That will learn the less frequent AND the most frequent at the same time. Also I have read that tree base methods perform very good on unbalanced dataset. However all examples always consider one big timeseries data. My data are many short timeseries. Is it possible to train RandomForest on such data? How? If you know about different algorithm/method that could be applied to such data please post it :)  Thank you. ', 'Tags': {'lstm', 'machine-learning', 'sequential-pattern-mining'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36975/how-do-you-classify-groups-when-the-inputs-are-paired', 'post': \" My data is like this: there is a choice between two alternatives and the customer chooses one. Each time the alternatives are different. I would like to characterize a desirable alternative. I don't want to use a normal classification algorithm because I don't want to lose the information that one alternative was chosen over the other in the pair. Does anyone have suggestions for ways to do this, or types of algorithms for solving this problem? Thanks!  \", 'Tags': {'classification', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36974/adding-the-input-layer-units-with-a-decimal', 'post': ' I took the course Machine Learning A-Z from Udemy and am trying to apply what I learned in the tutorials.  Theye taught us in the \"Adding the input layer\" portion of an ANN that the units is based off of the input_dim.  Normally the \"units = (input_dim + 1) / 2\".  In the dataset that I am working with my input_dim=754.  (754 + 1) / 2 = 377.5.  Should I use 377.5 or should I round up or down to a whole number?  ', 'Tags': {'neural-network', 'machine-learning', 'classifier'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36970/is-it-appropriate-to-use-polar-images-as-input-to-cnns-or-must-they-be-cartesia', 'post': ' I have built a convolutional neural net which trains on data originally represented in polar space (measurements are a function of angle and distance). My pipeline begins by converting coordinates to a Cartesian grid and re-projecting the images. This however results in a significant increase in the data dimensions (i.e. pixel size of the input images) and requires interpolation steps which I would prefer to avoid. Is there an appropriate method (implemented in keras/tensorflow) I can use for performing (2D) convolutions on the raw polar data? I have followed the cs231n course, so I have some background but am not yet an expert, especially on the theory side. Thanks! ', 'Tags': {'tensorflow', 'keras', 'machine-learning', 'cnn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36964/learning-to-parse-elements-from-text-and-assigning-them-to-categories', 'post': ' I am trying to generate a table with values parsed from unstructured text. Below are a couple of examples of possibly thousands of entries. For each entry, I would like to identify the title, assign the movie to a category (e.g. \"SciFi\"), and so on.  Results should look like this:  A naive approach would be to search for patterns such as \"Lord of the Rings\" or \"Star Wars\". Pattern search could be fuzzy as well. This, however, would require some kind of pre-defined mapping. However, I wondered whether there is a smarter approach from data mining. I know text classification algorithms (e.g. spam classifiers), but they have only two classes. I do not know much about natural language processing, and this does not really seem to be \"natural language\" in the stricter sense. Can a parsing/mapping of such unstructured text be learned in any way (with sufficient training data)? I am looking forward to get a bit of general inspiration on how to approach the problem. ', 'Tags': {'text-mining', 'machine-learning', 'data-mining'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36953/how-to-determine-the-k-in-knn', 'post': ' I am just getting touch with Multi-layer Perceptron. And, I got this accuracy when classifying the DEAP data with MLP. However, I have no idea how to adjust the hyperparameters for improving the result.  Here is the detail of my code and result: .  ', 'Tags': {'scikit-learn', 'hyperparameter-tuning', 'mlp'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36945/interpreting-the-root-mean-squared-error-rmse', 'post': \" I read all about pros and cons of RMSE vs. other absolute errors namely mean absolute error (MAE). See the the following references:  MAE and RMSE\\u200a—\\u200aWhich Metric is Better? What's the bottom line? How to compare models Or this nice blogpost, or this question in stats.stackexchange containing interesting responses, and this one in datascience.stackexchange  Still I can not get my head around something about RMSE: Scenario: Let's say we have a regressor for predicting house prices with a MAE of 20.5\\\\$ and a RMSE of 24.5\\\\$. Based on MAE, I can certainly interpret that the average difference between the predicted and the actual price is 20.5\\\\$. How can I interpret RMSE? Can we still safely say the predicted and the actual price are off by 24.5\\\\$ at the same time base on RMSE (upper-bound of prediction error)? In the first medium post, it says:   RMSE does not describe average error alone and has other implications   that are more difficult to tease out and understand.  It confuses me a little. And I could not find any reliable reference to also clearly state that one can safely interpret RSME as one does MAE. Is RMSE is simply a only mathematically more convenient for optimization etc., and we are better off with MAE for the interpretation? Any detailed explanation is highly appreciated. \", 'Tags': {'loss-function', 'machine-learning', 'regression'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36944/simple-question-about-prediction-classes-of-item-in-question-vs-not-item-in-ques', 'post': \" Let's say I wanted to use transfer learning to train a model to detect object A vs everything else. In this case, do I provide 2 types of input, images of object A and images of everything else, and then have the final layer of the model output either object A or not-object A?  What about in the case where I want object A vs object B vs everything else. Would it make sense in this case to provide images of A and B and then have only two output classes, but based on the confidence of the output, interpret it as 3 classes? Say that it's object A if the confidence in that is > 50%, object B if the confidence in that is > 50%, and anything else if neither of those two conditions are met? \", 'Tags': {'machine-learning', 'transfer-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36943/how-can-spatiotemporal-population-data-be-used-for-modeling-migration', 'post': ' I have a dataset that contains the population of butterflies(5 species) for 15 years for different locations. I want to model it against the climate index collected for same time period and location. The objective is to find how early the migration species migration starts before the onset of a dry period? My idea was to understand all the spatial locations around the dry place and see if the populations have changed over time in the neighbourhood which can indicate a migration, thereby can deduce the time the butterfly realizes before the dry season onset. What type of technique can be used to predict the migration pattern? Does this come under graph theory? If so can someone guide me in a right research direction, what algorithm has to be looked at? ', 'Tags': {'graphical-model', 'machine-learning', 'data-mining'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36920/two-dimensional-list-takes-huge-amount-of-memory', 'post': ' I have a 2d list which is created from lung CT image data and a label (the first item is a 3d array(image data) and the second item is a label(0 or 1)), I need this to data to train CNN model, the list is created using the code below:  I want to save the list in numpy file, my problem is, with approximately 400 images (400, 2) memory gets full, my laptop has 8gb RAM, can anyone tell me what is caused this huge memory usage? is there any way to solve this? ', 'Tags': {'machine-learning', 'python', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36917/decision-tree-where-identical-set-of-features-results-in-different-outcomes', 'post': ' I am following the example described in this page to test my decision tree program.  The initial data set is   age, astigmatism, tp-rate are features and the type of contact lenses is the outcome. When I trace the example (where features with lowest entropies are used as the decision nodes), we reach the following situation a little further down the page:   The blue circles are obviously mine to make tracing easier. As we see in the constructed tree up to this point, the path  will lead to the outcome of [Contact-lenses = None]. But when you look at the data, we see that this path can lead to either a hard or a none type of contact lens.  So has the example made the right decision? And if not, what should we do in situations where the split data-set ends up, under an identical set of features, with more than one outcome? ', 'Tags': {'machine-learning', 'decision-trees'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36909/evaluating-metrics-f1-f2-mean-average-precision-for-object-detection', 'post': \" Up today in the company where I work we are using the F1 Score for evaluating the performance of our model, also our competitor's using the same metric.  I would like to understand what's the difference between F1,F2 and mAP?(Please do not explain me how to calculate them, also I know that F measure gives the same weight to the precision and recall while mAP choose the best precision from all recalls) Why in competitions (e.g. PASCAL VOC) and articles for object detection I am reading it is always preferred to use mAP instead of F1 or F2 scores ?   Thanks !  \", 'Tags': {'machine-learning', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36908/difference-between-advantages-of-experience-replay-in-dqn2013-paper', 'post': \" I've been re-reading the Playing Atari with Deep Reinforcement Learning (2013) paper. It lists three advantages of experience replay:  This approach has several advantages over standard online Q-learning   [23]. First, each step of experience is potentially used in many   weight updates, which allows for greater data efficiency. Second,   learning directly from consecutive samples is inefficient, due to the   strong correlations between the samples; randomizing the samples   breaks these correlations and therefore reduces the variance of the   updates. Third, when learning on-policy the current parameters   determine the next data sample that the parameters are trained on. For   example, if the maximizing action is to move left then the training   samples will be dominated by samples from the left-hand side; if the   maximizing action then switches to the right then the training   distribution will also switch. It is easy to see how unwanted feedback   loops may arise and the parameters could get stuck in a poor local   minimum, or even diverge catastrophically [25]. By using experience   replay the behavior distribution is averaged over many of its previous   states, smoothing out learning and avoiding oscillations or divergence   in the parameters. Note that when learning by experience replay, it is   necessary to learn off-policy (because our current parameters are   different to those used to generate the sample), which motivates the   choice of Q-learning.  I am confused on how the second and third advantages differ. Isn't the third advantage just another case of breaking correlation? Thank you in advance for your help! \", 'Tags': {'dqn', 'reinforcement-learning', 'machine-learning', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36894/how-to-best-estimate-the-coefficients-of-a-confusion-matrix-in-case-of-strong-cl', 'post': ' I have a trained binary classifier (forget about how this was trained and think of it as a magical black box) and I would like to measure its classification performance (e.g. compute a confusion matrix) on a strongly imbalanced population/test dataset.  Suppose the composition (in terms of the two classes) of the population / test set is also known - say 99% of class $A$ vs 1% of $B$. I also would like to reduce as much as possible the uncertainty in the estimate coefficients of the confusion matrix computed on the entire (large) test set under the constraint of doing predictions for a max of $N$ examples of the test set (suppose that the predictions are expensive to run). What\\'s the best approach to do this? The best thing I can come up with is doing something like \"stratified sampling\":   I create from my test set (which is arbitrarily large for simplicity)  a balanced sample with $N/2$  examples belonging to $A$ and $N/2$ examples belonging to $B$. I run the classifier on the balance dataset, and compute the resulting confusion matrix. I \"normalize\" the so-obtained confusion matrix so that its rows sum to 1, obtaining $\\\\hat{\\\\mathbf C}$. To estimate what would be the confusion matrix on a large representative sample (say of $M$ items taken from my large test set) I compute  $$  \\\\mathbf C =  \\\\left( \\\\begin{matrix} 0.99M & 0 \\\\\\\\ 0 & 0.01M \\\\end{matrix} \\\\right) \\\\cdot \\\\hat{\\\\mathbf C}  $$ P.S.: To be really happy, I would like to estimate also the uncertainty of the components of the final confusion matrix $\\\\mathbf C$.  To do that, can I just compute e.g. the credible intervals of the coefficients of $\\\\hat{\\\\mathbf C}$ starting from the balanced sample, and \"rescale\" them using the definition of $\\\\mathbf C$ given above? Bonus track: if somebody could send me with a reference about how to do some kind of \"power test\" (e.g. \"how large should $N$ be to \"bracket\" the values of $\\\\mathbf C$ within given tight intervals?\") I\\'d be glad to review it.  Bonus$²$ track: any tips on how to extend the estimation of the uncertainty to nonbinary classifiers (more than 2 classes) are also welcome! $\\\\ddot\\\\smile$ ', 'Tags': {'confusion-matrix', 'machine-learning', 'bayesian', 'statistics', 'classification'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36889/what-does-it-mean-by-t-sne-retains-the-structure-of-the-data', 'post': ' I was learning about t-SNE when I was told that t-SNE retains the structure of the data in the embeddings. What exactly does this mean ? How does the algorithm achieve this ? So far I have understood this -> t-SNE is an unsupervised learning algorithm that is used for dimensionality reduction and visualization of high dimentioanal data. The algorithm works by measuring the similarity between one point and all the other points using the t-curve. The width of the curve is dependant on the density of the cluster the point belongs to. t-SNE retains the structure of the initial data. My question is what does it mean by retaining the structure of the data? Shouldn\\'t there be some loss in the structure of the data seeing as it is transformed into a lower dimensional space ? Also what does \"strucutre of the data mean\"? Please ask for any further details that are required. ', 'Tags': {'visualization', 'unsupervised-learning', 'machine-learning', 'dimensionality-reduction'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36878/optimizing-expensive-functions', 'post': ' I\\'m trying some different techniques to optimise a Boosted Gradient Regressor by using an evolutionary programming technique to try and find the most efficient set of features. So far I\\'ve been having some good results (Been able to remove 65% of the features with an increase in accuracy), but I\\'m not impressed with how expensive each epoch is to run (in terms of time). Currently, this is a very expensive problem to optimise,  as there are a maximum of 79 features to choose from, which is  possible feature permutations. So far, each individual in my population has a binary encoded gene, where  = use this feature, and  = do not use this feature. Each individual in a population is evaluated by running the boosted regressor with the selected features, and then the RMSLE is calculated. I start to see good results around 3 hours into the optimisation function (I\\'m not doing any parallel computing). I\\'ve been doing a little bit of research into how I can attempt to \"predict\" how good my boosted regressor will be without actually needing to evaluate its performance. So far I\\'ve found the following techniques:  Problem Approximation. I can see how this would be useful in some specific cases, but as I\\'m not really able to reduce the complexity of evaluating how accurate my boosted regressor is it seems irrelevant.  Functional Approximation. I think this is quite an interesting technique. They whole premise is to approximate the fitness of an individual without actually needing to evaluate the function Fitness Inheritance. This also seems rather interesting, and perhaps the most efficient of the methods listed above? Individuals are clustered into specific groups, and then the \"represented\" individual of each cluster has its fitness evaluated, and then the remaining individual\\'s fitness are approximated by upon a distance measurement.   I could see some issues with these techniques though. If the combination of features used don\\'t have a linear relationship with the output, then surely the function approximation would have a hard time approximating the potential fitness of an individual? I\\'m a bit stumped on what to further look at. ', 'Tags': {'feature-selection', 'optimization', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36874/plotting-in-multiple-linear-regression-in-python-3', 'post': \" So I'm working on linear regression. So far I've managed to plot in linear regression, but currently I'm on Multiple Linear Regression and I couldn't manage to plot it, I can get some results if I enter the values manually, but I couldn't manage to plot it. Below is my code block and dataset and error, what can i change to plot it? Dataset:   Code block:  It says  when I try to compile it. \", 'Tags': {'matplotlib', 'machine-learning', 'python', 'numpy'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36862/macro-or-micro-average-for-imbalanced-class-problems', 'post': ' The question of whether to use macro- or micro-averages when the data is imbalanced comes up all the time. Some googling shows that many bloggers tend to say that micro-average is the preferred way to go, e.g.:  Micro-average is preferable if there is a class imbalance problem. On the other hand, micro-average can be a useful measure when your dataset varies in size.  A similar question in this forum suggests a similar answer. However, this seems quite counter-intuitive. For example if we have a data set with 90%-10% class distribution then a baseline classifier can achieve 90% mico-averaged accuracy by assigning the majority class label.  This is corroborated by books, e.g. An Introduction to Information Retrieval says (page 282) \"Microaveraged results are therefore really a measure of effectiveness on the large classes in a test collection.  To get a sense of effectiveness on small classes, you should compute macroaveraged results.\" In the end the real decision about which measure to use should be based on the relative mis-classification costs for the classes. But a quick look at the internet seems to suggest use of micro-averaging. Is this correct or misleading? ', 'Tags': {'class-imbalance', 'evaluation', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36861/how-to-add-confidence-to-models-prediction', 'post': ' I am a newbie in ML working on a time series prediction project. The objective is to predict the future outcome of a time series (int valued, with different upper bound, think of it as different sized parking lot availability prediction) based on its historical value. I\\'m currently using a regression approach using slide windows algorithm. I tried different ML models and they seem to be working okay(better than my baseline at least). Now I\\'m trying to add confidence to my prediction, something like \"I have 95% confidence that the outcome would be 2\". I\\'m thinking about using the prediction mean squared error as a metric. The problem is,  Is it feasible to assume the prediction error follows gaussian distribution and add confidence based on that? What distribution should I use for highly discrete state space? For example, when there is only 4 possible states {0,1,2,3}, and I predicted 2.5 with mse 1, how can I distribute the possibility over those states?  Any advice on the general model architecture and confidence set up will be appreciated! ', 'Tags': {'time-series', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36851/can-machine-learning-deep-learning-used-to-minimize-an-objective-function', 'post': ' I have data of construction site and am wondering if i can use machine learning to reduce the cost it takes to build a building. But, as far as i know, Machine learning can only does function approximation.  But, i remember that google used AI to reduce the costs of the data center. This and  This are the article.  ', 'Tags': {'objective-function', 'machine-learning', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36848/evaluating-clusters-e-g-built-by-kmean-using-random-forest', 'post': ' I have made clusters for my data set (1.5 million samples and 800 features) using k-mean. I am aware of internal indices for evaluating clusters. However, I was thinking about training a supervised classification model (e.g. random forest classifier) in which cluster numbers are dependent variable and all other features are independent variable. Is using the accuracy of this model a good evaluation of the kmean cluster and why? ', 'Tags': {'random-forest', 'machine-learning', 'clustering', 'classification', 'k-means'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36821/how-to-tinker-with-cnn-architectures', 'post': ' I was thinking of creating a CNN. Now it is known CNN takes long times to train so it is advisable to stick to known architectures and hyper-parameters. My question is: I want to tinker with  the CNN architecture (since it is a specialised task). One approach would be to create a CNN and check on small data-sets, but then I would have no way of knowing whether the Fully Connected layer at the end is over-fitting the data while the convolutional layers do nothing (since large FC layers can easily over-fit data). Cross Validation is a good way to check it, but it might not be satisfactory (since my opinion is that a CNN can be replaced with a Fully Connected NN if the data-set is small enough and there is little variation in the future data-sets). So what are some ways to actually tinker with CNN and get a good estimate for future datasets in a reasonable training time? Am I wrong in my previous assumptions? A detailed answer would be nice! ', 'Tags': {'neural-network', 'machine-learning', 'cnn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36818/preprocessing-of-sudoku-dataset-from-kaggle', 'post': ' Dataset: https://www.kaggle.com/bryanpark/sudoku I would like to create a neural network for this Dataset. Feature:  Output:  The zeros represent a blank box, and the data is a flattened grid of 9x9. I tried using the following code, but I found out that the data needs a significant amount of preprocessing.  I would like to know how to work with this data and of course, also the structure of my neural network. ', 'Tags': {'neural-network', 'machine-learning', 'regression', 'keras', 'preprocessing'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36817/why-is-the-f-measure-preferred-for-classification-tasks', 'post': ' Why is the F-measure usually used for (supervised) classification tasks, whereas the G-measure (or Fowlkes–Mallows index) is  generally used for (unsupervised) clustering tasks? The F-measure is the harmonic mean of the precision and recall. The G-measure (or Fowlkes–Mallows index) is the geometric mean of the precision and recall. Below is a plot of the different means.  F1 (harmonic) $= 2\\\\cdot\\\\frac{precision\\\\cdot recall}{precision + recall}$ Geometric $= \\\\sqrt{precision\\\\cdot recall}$ Arithmetic $= \\\\frac{precision + recall}{2}$ The reason I ask is that I need to decide which average to use in a NLG task, where I measured BLEU and ROUGE ( where BLEU is equivalent to precision and ROUGE to recall). How should I calculate the mean of these scores? ', 'Tags': {'machine-learning', 'nlg', 'metric', 'evaluation', 'scoring'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36791/linear-regression-r%c2%b2', 'post': ' When I do a linear regression, R²: 0.90, but the estimates are not correct, why is this happening? (Deep Not : Adjusted R-squared:  -0.3872) ', 'Tags': {'linear-regression', 'machine-learning', 'machine-learning-model'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36788/building-a-model-to-predict-how-likely-someone-is-to-answer-the-phone-based-on-p', 'post': \" Imagine you have a list of everyone in your network. You want to know how likely they are to answer the phone. There are several people in your network that you have called multiple times (some of them two or three times, others many more times, some over 50) and you have a record of whether or not they ended up answering (for every single phone call). This is useful information, as those who have answered the phone in the past are probably likely to answer the phone again in the future. There are also several people that you have never called before and therefore to do not know how often they have answered the phone in the past. However, because they are in your network and you know them, you have basic demographic info such as age, gender, race, where they live (rural/urban), cellphone/landline, etc. You obviously also have the demographic information of the individuals you HAVE called in the past, so you can look at the relationship between answer rate and various demographic variables. Now, you want to build some sort of model (a propensity model?) to predict how likely ANYONE in your network is to answer the phone when you call them. How do you leverage both the call history and demographics information (given that many of your friends and family do not have any call history with you). The result I'm looking for is basically a measure/probability/classification that I can then use to prioritize calling certain individuals over others. From basic data exploration I know that many of these variables I've listed are predictive by themselves, but I don't know how to combine them all into one nice neat model. Is there a machine learning package in R I can use? I don't necessarily need a full answer to this question, I just can't even find where to start! I can build basic linear and logistic regressions in R if that helps, so I am familiar with those concepts. (Now also imagine you have millions of call records and millions of individuals you have yet to call but have demographic data for, does it change how you go about this?) \", 'Tags': {'machine-learning', 'predictive-modeling'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36773/may-the-training-set-and-validation-set-overlap', 'post': ' May the training set and validation set overlap? Similarly, may the testing set and validation set overlap? ', 'Tags': {'training', 'machine-learning', 'data-science-model'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36769/how-to-do-face-recognition-without-using-any-kind-of-neural-networks-like-cnn', 'post': ' Is/was there any way to perform face recognition, instead of using the Convolution Neural Network which uses the technique of mapping(encoding) the face using 128-D vector and then using classifier (like knn/SVM) on it? Before the invention of the CNN approach, what did we use for face recognition?  From a comment by @rahulreddy: Custom object detection is possible using Voila-Jones Haar cascade or using the histogram of oriented gradients (HOG) technique or now by CNN. ', 'Tags': {'classification', 'image-classification', 'machine-learning', 'cnn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36767/how-to-deal-with-missing-not-at-random-data-for-k-means-clustering', 'post': \" I am running k-means clustering on a customer dataset. One of the available demographic fields is inferred homevalue, represented as an integer.  This field has value 0 when it's inferred that the customer is not a homeowner at all (they are more likely a renter, live with relatives, etc). I'm struggling to think of a good way to treat this value.  Does it make sense to keep this value as 0? Then my understanding is that the algorithm will interpret this as someone who doesn't own property is closely related to someone who owns an extremely low value property which doesn't seem intuitively right.  Is there a better way of dealing with this? \", 'Tags': {'k-means', 'machine-learning', 'missing-data'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36764/importerror-cannot-import-name-stanfordcorenlpparser', 'post': \" I've been trying to extract subject-predicate-object triples from sentences and found this awesome API that did just that. However, when it was written, it used the  (), which is now defunct. Instead, what is to be used now is the . Is this the right way to import it?  Anyways, I am trying to modify it  file (found here) to now use the new , while keeping all the functionality intact.  My code is below:  What happens when I run my code is I get the error:  ImportError: cannot import name 'StanfordCoreNLPParser'.  Does anyone have an idea how to fix this? \", 'Tags': {'machine-learning', 'python', 'stanford-nlp', 'nlp'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36763/how-to-utilize-user-survey-answers-and-the-actual-usage-in-forecasting-power-usa', 'post': ' I have the pre-trial survey and post-trial survey conducted of around 5000 users for Smart Meter installation. With this I have power usage reading recorded every 30 min for around one and a half years. survey csv is like this  power usage csv is like this  I want to forecast power usage of the user based on the past power usage and the survey using LSTM. How to start with this? ', 'Tags': {'lstm', 'machine-learning', 'deep-learning', 'keras', 'forecasting'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36755/inconsistency-in-training-iris-dataset', 'post': ' I am a noob in the field of ML. I have been trying to classify the iris dataset. I managed to do it with backpropagation, and with 3 neurons in the hidden layer. But the mean square error that I get when I rerun the entire code is not consistent. It varies between 0 - 8%. Is this expected, or am I doing something wrong here? ', 'Tags': {'machine-learning', 'backpropagation'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36743/how-does-ml-algorithms-treat-unseen-data-a-conceptual-discussion', 'post': ' I want to predict the occurrence of certain events, but these events only occur say 5% of the time in my data, hence in 95% of the data there is nothing to learn.  In order to teach the ML algo something I have learned to single out the 5% and drop the rest of the data. Let us say that I want to predict if a picture is of a dog or a cat. In my data 2.5% of the pictures are of dogs and 2.5% of cats, the rest are just random pictures. So, I single out the cat and dog pictures and label them so that the ML algo can learn from that. Am I broadly right so far?  So, if I train my algo on only cat and dog pictures and get a satisfactory accuracy, what will then happen in live usage when 95% of the pictures are not of cats or dogs? I.e. I show my model a picture of a house, what does it predict? Will my algo always predict either cat or dog, or will it somehow tell me that it has no clue what this picture is?  Any thoughts?  ', 'Tags': {'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36742/two-definitions-of-dcg-measure', 'post': ' I wanted to check the definition of Discounted Cumulative Gain (DCG) measure in the original paper Jarvelin and it seems it differs from the one given in the later literature Wang. Originally, for $n$ documents ranked from $r = 1, \\\\ldots, p$, the $\\\\text{DCG}_p$ is defined as  $$\\\\text{DCG}_p = \\\\sum\\\\limits_{r=1}^{b} G_r + \\\\sum\\\\limits_{r=b}^{p}\\\\frac{G_r}{\\\\log_br},$$ where $G_i$ is the relevance (or gain) of the $i$-th document. So the measure depends on the logarithm base $b$. For ranks below $b$, i.e. $r<b$, gains are not penalized. If $b=2$, then we can write: $$\\\\text{DCG}_p = G_1 + \\\\sum\\\\limits_{r=2}^{p}\\\\frac{G_r}{\\\\log_2 r}.$$ It does not look the same as the one given on wikipedia, where the argument of the logarithm is shifted by $1$: $$\\\\text{DCG}_p = G_1 + \\\\sum\\\\limits_{r=2}^{p}\\\\frac{G_r}{\\\\log_2(r+1)}.$$ Where does this change come from? Why others use different metric? ', 'Tags': {'machine-learning', 'learning-to-rank', 'information-retrieval', 'ranking', 'recommender-system'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36710/is-it-possible-to-train-for-precision-in-tensorflow', 'post': ' Can I train a binary classifier in Tensorflow to maximize precision? ', 'Tags': {'neural-network', 'tensorflow', 'machine-learning', 'deep-learning', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36709/multi-target-classification', 'post': ' I am working on traffic violation data set which contains 36 columns(variable). I have two dependent variables out of this. Example  The dependent variables are \"violation level\" and \"accident\". violation level has 3 classes -- sever medium light Accident has 2 levels -- Yes No I want to predict both violation level and accident. I think this can be multitarget prediction. Can someone help me which algorithm is good for this? I have seen some articles suggesting scikit- multiout put classification and neural network with multiple out put layers. Or Can I go ahead with two models? predicting the traffic violation level. Predicting the accidents using the violation levels. Basically I am trying to predict violation levels and then predicting how these violations contribute to accidents. Any help would be appreciated. ', 'Tags': {'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36706/why-are-observation-probabilities-modelled-as-gaussian-distributions-in-hmm', 'post': ' HMM is a statistical model with unobserved (i.e. hidden) states used for recognition algorithms (speech, handwriting, gesture, ...). What distinguishes DHMM form CHMM is the transition probability matrix P with elements. In CHMM, state space of hidden variable is discrete and observation probabilities are modelled as Gaussian distributions.  Why are observation probabilities modelled as Gaussian distributions in CHMM? Why are they (best)distributions for recognition systems in HMM?  ', 'Tags': {'markov-hidden-model', 'machine-learning', 'speech-to-text', 'python', 'gaussian'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36703/does-mlpclassifier-sklearn-support-different-activations-for-different-layers', 'post': ' According to the documentation, it says the \\'activation\\' argument specifies:   \"Activation function for the hidden layer\"  Does that mean that you cannot use a different activation function in different layers? ', 'Tags': {'neural-network', 'scikit-learn', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36695/deciding-number-of-classes', 'post': ' I have a dataset of 1000 labels. I would like to build a custom model from this dataset with three labels, say , , and . Obviously,  will be large in number for this approach.  I understand that this could be an issue since the classifier has seen more  examples.  Pipeline  Large dataset --> Seperate to Dogs, Cats and others --> Classifier --> Predicts Dogs, Cats or Others.  What are the other approaches which can be taken in this situation? ', 'Tags': {'data', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36694/how-to-classify-a-dataset-into-5-classes-even-though-the-performance-is-low', 'post': ' I have a dataset of 5 classes with 8 features, say A, B, C, D and E. Now when I try to classify these into individual classes, I get accuracy, specificity and sensitivity of approx 50-60%, which is not a good rate. Now since I made the dataset myself, I know that classes [A,B,C,D] can be categorized into 1 class and [E] into another. Once a sample has been predicted to be in class [A,B,C,D], I can again break the class distribution to [A,B,C] and [D]. Once a sample has been predicted to be in class [A,B,C], I can again break the class distribution to 3 final classes [A],[B] and [C]. I pass my dataset by combining multiple classes into one as mentioned above to get the output. In the first stage of classification I get to know whether a sample is class E or not, in the second I get to know whether the sample (if not belonging to E) belongs to D or not. In the third I get to know whether a sample which is not E and D belongs to A or B or C. Is this method of pipelining viable and a logical solution? ', 'Tags': {'classification', 'feature-selection', 'supervised-learning', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36685/can-we-make-two-separate-models-vs-one-for-classification', 'post': ' Suppose I have a binary classification problem and my data is imbalanced, I can build a classification model using any of the algorithms and use an oversampling or undersampling technique to handle the class imbalance.  What If I were to make two separate models independently, each model trained only for one class of data. Will this be a right approach? ', 'Tags': {'classification', 'machine-learning', 'predictive-modeling', 'unbalanced-classes'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36680/classification-algorithms-and-deep-learning', 'post': ' Nowadays we see a big trends of deep learning and a lot of applications using it . So, I was wondering do people still need to use the classification algorithms (traditionnal machine learning ) ? ', 'Tags': {'classification', 'machine-learning', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36677/quantile-regression-with-inhomogeneous-density-of-points', 'post': ' I am working on a dataset that can be represented this way:  We can see that the lower values on the y-axis are increasing linearly along the x-axis. I want to estimate the coefficients of this linear function, and quantile regression is what seems most appropriate for this. However, as we can see the density of points is much higher on the left-hand side, which gives more weight to this part of the dataset:  Is there a way to avoid this ?  ', 'Tags': {'linear-regression', 'dataset', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36672/does-it-make-sense-to-label-a-dataset-manually', 'post': ' I would like to implement a classifier on a dataset which does not have a label. I\\'ve written a script which labels each row of the source file by some specific values like \"IF H > 45 && T <= 89 THEN label = True\". Does it make sense to do something like this? I mean if I\\'m able to label the dataset by the script I\\'ve wrote, why do I need the classifier? There are any arguments why it makes sense to still use a classifier for it? ', 'Tags': {'classification', 'machine-learning', 'labels'}}\n",
            "page number ------------------------------------94\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36656/featuring-scaling-whole-data-set-before-spliting-it', 'post': \"       This question already has an answer here:\\r                         \\r                         Data Snooping, Information Leakage When Performing Feature Normalization \\r                                 (1 answer)\\r                               Closed 2 years ago.    I am wondering why do we use scaling on train and test set separately. I understand that transform () on test data  μ and σ as computed from fit_transform() on  Train. But why can we compute μ and σ from all given data (before split) and then apply them on future data.  Do we do this because we don't know how the size of our future data? \", 'Tags': {'scikit-learn', 'machine-learning', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36632/improve-ner-label-results-on-non-english-text', 'post': ' I am working on some Medieval Latin text and was using various methods of NER such as CLTK (Latin Model), Spacy (Multilingual, Italian, Spanish Model) and StanfordNER (Spanish Model). When I used the non-Latin models I used the original Latin text as the translated one was not making any sense. Fortunately Spacy Multilingual model managed to extract all Persons and Places of the sample documents but with additional words that I am not considering them as Entities. Moreover, the labels are incorrect. Here is an example output:  where the LOCATIONS should be: Panormi, Bruges, Melite and PERSONAL names should be all others except Unde, Apoca, Coram, Testamur, Renunciando which are neither locations nor personal names. I was thinking of ignoring the labels and do some classification ML algorithm. The problem is that I do not have any training data available and the only possible usable corpus that I think it might be useful is Proiel treebank which labels proper nouns as NE. How would you go with such a problem? ', 'Tags': {'classification', 'machine-learning', 'named-entity-recognition'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36627/tuning-lexicon-sentiment-values-using-machine-learning', 'post': \" I'm constructing a sentiment-analysis model using the lexicon-based approach, and wondering if I can tune the weights of each word-feature in the lexicon using machine-learning. Is this achievable with a machine-learning based tool, or by using an optimisation algorithm such as gridsearch? \", 'Tags': {'machine-learning', 'python', 'nltk'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36626/siamese-network-triplet-loss-for-unlabeled-images', 'post': \" Hi I am rather new to machine learning so I'm not really sure whether it's possible or not. Right now I'm trying to train a siamese network to identify whether an image is the same or not(by one-shot learning). I've found a lot of implementations of such network on GitHub but they somehow need labeled data. Is it possible to do something similar with unlabeled data?  \", 'Tags': {'similarity', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36623/modelling-promotions-for-demand-forecasting', 'post': \" I am trying to develop a model to predict future demand for a product. Now, there are always some promotional events that affect the sales. I am trying to solve this problem using dummy variables. Here's how: Supposing that the firm runs 7 promotional events on a particular product. So, I construct 7 dummy variables, that are boolean. For supposing that a for a particular week, promotion 3 was running. So, my training data-point becomes [0,0,1,0,0,0,0] and the corresponding sales. I construct a linear regression model for promotions in this way. Now, here is my problem. When we model seasonality using this method, we construct a base linear model, after deseasonalising the data, and  then use the two models to predict the final output. In case of promotions, how do I 'depromotionalise' the data?  Any tips in solving the problem are appreciated. Thanks! \", 'Tags': {'time-series', 'forecast', 'machine-learning', 'regression'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36615/why-is-elasticnet-performs-worse-than-both-lasso-and-ridge', 'post': \" I am using the following codes to build a few models on the same dataset:  And the  for the 3 models are:  This is confusing to me ... shouldn't the ElasticNet result fall somewhere between Lasso and Ridge? Why is ElasticNet result actually worse than the other two? Thanks! \", 'Tags': {'linear-regression', 'scikit-learn', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36614/autoencoder-ambivalent-about-order-of-input-data', 'post': ' The problem I\\'m working to solve is this: Given a musician\\'s prerecorded free-form playing. I want to analyze each of the individual notes to determine how \"in-rhythm\" it is. See the graph in the screenshot.  The dark shading represents the beat intervals between every note in the vicinity of the one in question (plus or minus say, 10,000 ms) and every other in that vicinity. From this you are able to see peaks where common beat intervals lie and you can begin to deduce the length of a standard quarter note beat, etc.  The light thin lines represent the intervals between the note in question and those in the local vicinity. When I say I want to know how \"in-rhythm\" this note is, what I mean is, \"how close are these lines to their nearest peak?\" It\\'s basically a problem of cluster analysis. So, given a set of numbers (represented by the thin lines in the graph) how close are they as a set to the center of their respective clusters? It strikes me that this could be a good machine learning problem, so I\\'ve been attempting to approach it from that direction. I was thinking a simple autoencoder would be suitable for the task. The input would be lists of numbers representing these beat intervals, but one thing I\\'m not sure about is this: There\\'s no telling how many of which peak/cluster any given note\\'s beat intervals might belong to, so is there a way to keep the neural net ambivalent about the order in the list it finds these numbers? I don\\'t want it determining that a certain note was more out of rhythm simply because it was missing an interval from a large cluster, etc. It seems to me that order would probably always matter, so I\\'m at a bit of an impasse with my understanding. Am I right about this? Do you see some other solution? ', 'Tags': {'neural-network', 'autoencoder', 'machine-learning', 'clustering'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36609/how-to-give-importance-to-recent-data-in-a-large-dataset', 'post': ' I am solving a binary classification problem which is also partly a time-series problem. This means that recent data is more relevant than old data. But how can I assign more importance to the recent data as compared to old data? ', 'Tags': {'classification', 'machine-learning', 'data'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36593/own-implementation-of-neural-networks-heavily-under-fitting-the-data', 'post': \" I tried to implement a Basic Deep Neural Network Algorithm for a classification problem on my own. I have tried on the iris data set for this test but, my implementation has been giving me very poor results, it's heavily under-fitting the data, the best accuracy I get is 66 % and the least even goes to 0 %, for every run of my algorithm , I get heavily varying results even after I've set a low randomness seed.  I've chosen a tanh activation function, a learning rate of 0.01, a softmax activation for the output layer and a Standard Scalar normalization on the input variables. So, I'm wondering whether I'm doing any of the math part wrong, or missing any fundamental part of this algorithm, any advice or correction is much appreciated. Thank you so much in advance. Here's the code:  Here's a sample result:  \", 'Tags': {'neural-network', 'machine-learning', 'deep-learning', 'python', 'data-science-model'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36584/training-gaussian-restricted-boltzmann-machines-with-noisy-rectified-nrelu-or-s', 'post': \" I'm not sure how to implement this architecture. I'm following this thesis (pages 17-19) or this paper but I'm not sure how to train it. I want to use this to extract features from raw audio. I know I have to compute the positive and negative correlations, but I don't know how to do this exactly since I can not find any detailed documentation of this. What I have done so far is: Positive correlation To compute it I do a matrix multiplication:  Where  is be my batch data transposed and  are the hidden units activations. . Not sure if I have to add noise in this point. Negative Correlation To do this I use contrastive divergence (no tag??) with  Same thing:  but I'm really not sure how I have to compute these terms. So for me  would be the samples of , where  here is activation from previous step () without noise. Lastly  is computed as the same as the previous  but using  Is this correct? Because I don't know when I have to use samples or activations or maybe meanfields \", 'Tags': {'rbm', 'machine-learning', 'feature-extraction', 'training', 'gaussian'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36577/what-companies-would-be-great-for-entry-level-data-science-machine-learning-pro', 'post': '       Closed. This question is opinion-based. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         I am just starting to learn about machine learning, and I have taken some intro classes in MATLAB and Python. I want to know about fulfilling careers and fields programmers can pursue to help me determine my career direction. Thank you for your time! ', 'Tags': {'career', 'machine-learning', 'data-science-model'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36576/how-do-you-know-if-your-autoencoder-network-is-fully-connected', 'post': \" I am new in deep learning and I am confused about fully connected network. Is an Autoencoder with more than one hidden layers a type of deep neural network (DNN)? Is DNN always fully connected? Let's say an Autoencoder network with three hidden layers was built using H2O R package, how do you know if the Autoencoder network that you build is fully connected? \", 'Tags': {'neural-network', 'autoencoder', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36568/is-deduction-genetic-programming-pca-or-clustering-machine-learning-according', 'post': ' Tom M. Mitchell defines machine learning as   A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.  For some algorithms that are usually counted towards the field of machine learning, I heavily doubt that they are learning algorithms according to the definition. Interestingly, all of them are part of Tom Mitchell\\'s book \"Machine Learning\". Deduction The \"symbolist tribe\" uses inverse deduction according to Pedro Domingos (see The 5 tribes of ML). While I\\'m not too sure what inverse deduction is, the idea of having a set of true statements and inference rules like Modus Ponens to create more/other true statements is clear. Comming to the definition: What is experience here? How is performance measured? What does a typical task look like? Genetic programming From Wikipedia:  Genetic programming (GP) is a technique whereby computer programs are encoded as a set of genes that are then modified (evolved) using an evolutionary algorithm (often a genetic algorithm, \"GA\").  What is experience in this case? Maybe \"iterations\" / \"epochs\"? (I\\'m not sure about the terminology) Tasks: What are the typical use cases of Genetic Programming? PCA PCA and some other dimensionality reduction algorithms are in some way \"fixed\". The algorithm does not change with more data. It does not have parameters. Thinking about it, one might argue that it estimates the projection in a better way with more data. But then: What is better? What is the performance measure P here? Clustering I don\\'t see a clear performance measure for Clustering (e.g. k-means). Also, the only kind of experience I can see is the number of samples. ', 'Tags': {'pca', 'machine-learning', 'genetic-algorithms', 'clustering'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36557/mixture-density-network-determine-the-parameters-of-each-gaussian-component', 'post': \" I am reading Bishop's Mixture Density Network paper at: https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/bishop-ncrg-94-004.pdf This is a good paper, but I am still confused about a few small details. I am wondering if anyone could give me some help:  Basically the mixing coefficient alpha_i can be computed through a softmax function in eq (25) below. However, in eq (25), what's the upper alpha for each $(z_i)^{alpha}$? Is it a free parameter to be fitted? Similarly, in eq (26), what's the upper sigma in $(z_i)^{sigma}$? Is it a free parameter to be fitted as well? Thanks!    \", 'Tags': {'neural-network', 'statistics', 'gaussian', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36551/gumbel-softmax-vs-vanilla-softmax-for-gan-training', 'post': ' When training a GAN for text generation, i have seen many people feeding the gumbel-softmax from the generator output and feed into the discriminator. This is to bypass the problem of having to sample from the softmax which is a non-differentiable process and hence prevents training. My question is though, why not just feed the regular softmax (no argmax!) from the generator directly into the discriminator? What is the benefit of using the gumbel-softmax? Thanks. ', 'Tags': {'machine-learning', 'training', 'sampling', 'text-generation', 'gan'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36510/missing-value-in-continuous-variable-indicator-variable-vs-indicator-value', 'post': \" Most data has missing values, and as far as I'm aware, these are the options:  Imputation (mean, hot-deck, etc.) Indicator variable. A categorical variable that tells what type the primary variable is. For the missing value case, this is binary. Something still has to be imputed, though. Indicator value. If the model is powerful enough, it can learn to associate a specific imputed value to certain types of predictions.  In my case, a missing value reveals important information, and thus is Missing Not At Random. From what I've read, most imputation methods don't cover this scenario. Thus, I've opted for the indicator value approach. My question is: Is there any point adding an additional indicator variable, since I'm already using an indicator value? Am I completely misguided and should I be looking into some other approach? Example:  | Primary variable (-50 to 50)          | Indicator     | |-------------------------------------  |-----------    | | 20.5                                  | 0             | | -14.2                                 | 0             | |  0.1                                  | 0             | | 500 (out of the usual distribution)   | 1             |  I can provide more information of my problem, if it's required to answer the question. \", 'Tags': {'data-imputation', 'machine-learning', 'missing-data'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36507/should-we-identify-outliers-from-population-prior-to-taking-sample', 'post': ' I am revising undergrad statistics course via this course, where i am learning techniques to pull out sample from population. While ensuring that sample is a decent representative of the population, I am left with a question.  Should we care about identification and rectification of outliers prior to taking sample from population ?  Here is my work from where i came up with this question ', 'Tags': {'statistics', 'machine-learning', 'outlier', 'data-science-model'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36487/what-kinds-of-statistical-analyses-and-machine-learning-techniques-are-most-usef', 'post': \"       Closed. This question needs to be more focused. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Update the question so it focuses on one problem only by editing this post.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         I am just a beginner-level programmer in Python and MATLAB, and I have been looking at tutorials for Keras. Eventually, I want to make a project for my programming portfolio. Can you please share some ideas about how to get started with this project as well as any career advice about getting started with data science and machine learning? Here's an example of a project idea I want to work towards: Machine learning applied to a series of daily logs and selfies to look for patterns of mood and activities that the user inputs as well as identify stress factors: If implemented in a smartphone application, the program can be made to send notifications to the user when the algorithm determines it would be beneficial to the user's wellbeing. With the use of neural networks and other sources of data such as from Apple Health, more categories can be added to identify more specific moods such as anger, excitement, and sadness. \", 'Tags': {'machine-learning', 'data-science-model'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36482/lstm-training-prediction-with-no-starting-sequence', 'post': \" ML newbie here. As an exercise, I'm trying to build a character based language model based on a simple 1 layer LSTM. Based on what I've learned about LSTMs, a common usage is to take in a sequence of characters and then predict the next character. What I don't fully understand is how I would go about predicting the very first character when there's no preceding sequence yet (or, by extension, predicting a character when there's not a long enough preceding sequence as input to the LSTM units). The best solution I can thing of is to reserve a special character in the vocabulary to represent the abscence of any characters. A toy example:  My question is: what's the best practice for doing this type of thing? Am I on the right track? \", 'Tags': {'sequence', 'lstm', 'machine-learning', 'keras', 'language-model'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36480/effect-of-not-changing-filter-weights-of-cnn-during-backprop', 'post': ' What is the effect of NOT changing filter weights of a CNN during backpropagation? I changed only the fully connected layer weights while training on the MNIST dataset and still achieved almost 99 percent accuracy. ', 'Tags': {'mnist', 'machine-learning', 'cnn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36477/entropy-in-a-closed-box', 'post': '  I have not understood why the maximum number of binary questions needed to reduce uncertainty is essentially log(T). If i have a ball in a box and 10 possible classes, is it enough to ask log(10) binary questions to know which is the class of the ball ? Sure ??? ', 'Tags': {'classification', 'machine-learning', 'data'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36461/what-is-the-logic-of-the-epoch', 'post': ' What is the logic of the epoch? for example, 1 time, 2 times ... I just do not know what else is working to give better results than I know.  ', 'Tags': {'neural-network', 'optimization', 'machine-learning', 'deep-learning', 'keras'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36459/gradient-descent-multidimensional-linear-regression-does-learning-rate-affects', 'post': \"       Closed. This question needs details or clarity. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Add details and clarify the problem by editing this post.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         I wonder if gradient descent for multidimensional regression always finds the right result? I feel like this doesn't always have to be true. I have done some calculations and actually got correct results but not for all learning rates I tried to specify. It's not the case that it is too big, cause sometimes with even having too small learning rates I get wrong results. Please tell my if I make mistake in the calculations, or I'm wrong with divergence of the hypothesis. \", 'Tags': {'neural-network', 'logistic-regression', 'machine-learning', 'gradient-descent'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36450/what-is-the-difference-between-gradient-descent-and-stochastic-gradient-descent', 'post': ' What is the difference between Gradient Descent and Stochastic Gradient Descent?  I am not very familiar with these, can you describe the difference with a short example? ', 'Tags': {'neural-network', 'gradient-descent', 'machine-learning', 'deep-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36447/resume-parsing-extracting-skills-from-resume-using-machine-learning', 'post': \" I am trying to extract a skill set of an employee from his/her resume. I have resumes stored as plain text in Database. I do not have predefined skills in this case. How should I approach this problem? I can think of two ways:  Using unsupervised approach as I do not have predefined skillset with me. I will extract the skills from the resume using topic modelling but if I'm not wrong Topic Modelling uses BOW approach which may not be useful in this case as those skills will appear hardly one or two times. (I would appreciate if you enlighten me more about topic modelling). Another approach is manually labeling the skills for resume and making it supervised learning problem. But I have around 500 resumes, manual labeling will be very tedious and very time consuming.   Any suggestions are welcome. Thanks. \", 'Tags': {'topic-model', 'text-mining', 'machine-learning', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36444/how-can-i-use-a-string-as-input-in-a-neural-network', 'post': \" I'm making a program that can determine if a user will like a car from different auctions based on the cars that he/she has bought in the past. Therefore, I want to use the make and model (which are represented as strings) as well as the year, mileage etc... but I'm having trouble to decide on how I'll pass them to the neural network I've been thinking of using a vector of different makes as such:  I could pass those numerical values as input but making this is limited as there are tons of different makes, since I'm looking for cars in many different auctions. I have no idea how I can represent the models too as they also change in every auction. You have any ideas on how I can pass the strings onto the neural network? p.s I'm using python to build the neural network. \", 'Tags': {'neural-network', 'machine-learning', 'python'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36440/whats-the-difference-of-stateless-lstm-and-a-normal-feed-forward-nn', 'post': \" From what I understand, the whole point of LSTM is for the network to establish long-term dependencies in the data, i.e. an event happening now may be in some way determined by something that happened sometime in the past, and that may not be in the batch of data currently being presented, but many sequences or batches previously. So I understand that a stateful LSTM does this, and the network will retain the state until  is called.  But with stateless LSTM, where the state isn't retained between batches, then how does that differ from a normal feed-forward perceptron network? I'm assuming that even though in stateless LSTM you don't control the states as you would in a stateful model, that some state is still retained at passed on between sequences?  Is it passed on between batches? I can understand why someone would use stateful LSTM, but why would someone use stateless LSTM instead of a feed-forward perceptron network?  Or vice-versa, why would someone use a regular FF instead of a stateless LSTM? \", 'Tags': {'neural-network', 'recurrent-neural-net', 'lstm', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36433/combining-image-and-scalar-inputs-into-a-neural-network', 'post': ' I\\'m looking at the best way of combining CNN with image input and a scalar value.  I know that one of the ways is to concatenate flatten layer with this scalar value. But flatten layer consist for example 2048 such scalar values with different magnitude than a single input value. And what if in a real task this scalar value has more influence than image.  Also one of the examples is a combination of a text and image and then some fusion on top of that, but I still think it is a little different task because you get pretty the same vectors from the text model and CNN network.  Another one solution is to apply some ml algorithms, like Xgboost on top of flatten layer from CNN and this scalar value. But in that case, we need to train CNN networks separately, which is not good. Can someone tell what is the best way to combine image input with scalar value so that I can train CNN network together with scalar input and that network will \"decide\" which input more important? ', 'Tags': {'neural-network', 'machine-learning', 'deep-learning', 'ensemble-modeling', 'cnn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36413/4-class-classification-machine-learning-model', 'post': ' I have a data set which contains nearly 150 features and 60k data. And my target feature is continuous variable represents hours. I divided this period into 4 categories of user engagement (4 ranges of hours). Implement  with ,  with logistic regression, Random forest,  with  with suitable normalization of data wherever required. Used  for best feature subset selection. All the algorithms gave similar results of around  accuracy ( for nearly balanced test set).   Note: Training is also done on a balanced data set. I am wondering where am I gone wrong? I believe I went wrong somewhere in input to target mapping. Could anyone please confirm that categorization of the continuous target variable (hrs) into 4 sets are reasonable? ', 'Tags': {'classification', 'supervised-learning', 'machine-learning'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36410/how-to-deal-with-optional-input', 'post': ' I\\'m from the vision world and only worked with pixels from 0-255, ignoring any side effects. My current problem is different, in the way that I cannot rely on the input data. What my problem is:  I have a number of inputs. Each input is categorical (for now) and optional. For example I have a number of user features, {male, female, [..not given]}, {single, relationship, … , [..not given]}, .. What I want: X optional Input-Features mapped to Y Output-Features, with uniform output vales across the features. I tried kernel pca, different kinds of matric factorization for guessing missing inputs and simple autoencoder networks. By sight, the last two yield \"ok\" results. Any advice here?   ', 'Tags': {'representation', 'machine-learning', 'feature-construction', 'feature-scaling'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36404/when-to-remove-correlated-variables', 'post': ' Can somebody please suggest what is the correct stage to remove correlated variables before feature engineering or after feature engineering ? ', 'Tags': {'feature-selection', 'machine-learning', 'data-science-model'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36391/difference-in-between-cnn-and-inception-v3', 'post': ' What is the difference in between the inception v3 and Convolutional neural network? ', 'Tags': {'inception', 'machine-learning', 'cnn'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36390/reducing-the-dependency-between-variables', 'post': ' I am trying to perform a multi linear regression model: $$y_i = β_0 + β_1x_{i1} + β_2x_{i2} +... + β_px_{ip} + ε_i$$ where $$x_{i1}, x_{i2}, ..., x_{ip}$$ are highly correlated with each other (VIFs can be as low as 5 and high as 10). I am just wondering if there exists a procedure with the following properties: 1) reduces the collinearity of the variables (e.g. VIFs should be lower than 5 after the procedure) 2) the variables after the procedure should maintain the original meanings/interpretations.. (so PCA and FA are out).  3) not dropping any of the variables. I should have all p original varaibles.. (So LASSO and RIDGE are out)  ', 'Tags': {'machine-learning', 'regression'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36389/running-a-python-script-with-heavy-memory-requirements', 'post': \"       Closed. This question is off-topic. It is not currently accepting answers.\\r                         \\r                                Want to improve this question? Update the question so it's on-topic for Data Science Stack Exchange.\\r                  Closed 2 years ago.    \\r                         Improve this question\\r                         I need to run a python script where I am loading several(~15) large modules including keras and load several (approx 20) LSTM models and large amount of data for computation. These script has several methods which are needed to build an environment class for a reinforcement learning problem. The process seems to be working very slowly on my 16GB RAM i7 CPU(haven't tried out multiprocessing module yet but might not work as most of the task is not parallelism friendly.) Can you provide me some alternative possibly going for a higher end hardware setup ideal for such heavy computations which can be done on a local machine. Since it's a problem I am facing in ML related work which other researchers over here might have faced, that's why I am adding the question in this forum. Please suggest other forums if possible where I can get a better suggestion \", 'Tags': {'lstm', 'machine-learning', 'deep-learning', 'bigdata', 'gpu'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36388/how-does-xgboosts-exact-greedy-split-finding-algorithm-determine-candidate-spli', 'post': ' Based on the paper by Chen & Guestrin (2016)  \"XGBoost: A Scalable Tree Boosting System\", XGBoost\\'s \"exact split finding algorithm enumerates over all the possible splits on all the features to find the best split\" (page 3). Thus, my understanding was that XGBoost enumerates over all features and uses unique values of each  feature as candidate split points to then choose the split value that maximizes the splitting criterion (gain).  Then, my question is why do the split values for float-type features chosen are often not one of the unique values for that feature? For example, for a certain feature in the data with floating point values, like so: 966.0, 1234.0, 2350.0, 4567.0 .... If Xgboost chooses to split on that feature, the split value can be, e.g. (feature < 1800.5), which is not one of the unique values for that feature, but is often the mid-point between two values.  Also, assuming no missing values in the data, for the ordinal type features, with the values like so: set(1, 2, 3, 4, 5, 6, 7, 8), declared as integers, how does XGBoost enumerate candidate split points? Does the algorithm treat that feature as categorical?  I\\'ve read a lot of good answers on the forum, like here and here, have gone thru xgboost documentation and tutorials, but they don\\'t seem to elucidate on the specific procedure of enumerating candidate split points for exact greedy algorithm.  Could someone, perhaps, familiar with the source code could shed light on these questions?  ', 'Tags': {'boosting', 'xgboost', 'machine-learning', 'decision-trees'}}\n",
            "{'link': 'https://datascience.stackexchange.com/questions/36370/how-to-learn-3d-orientations-reliably', 'post': ' I am working on neural network models for 3D skeletal character animation, where I learn joint positions and orientations. The problem comes with the orientations. There are several ways I can choose to represent a 3D rotation, but all of them have some form of discontinuity that is making my model unable to produce stable correct rotations in all cases. The alternatives I have considered are:  Quaternions. Quaternions (or unit quaternions) are very commonly used for 3D orientation, and are generally fine except a unit quaternion and its opposite represent the same orientation. It is not hard to come up with a loss formula the accounts for this ( works fine), but the problem is that in some cases the network somehow learns to produce the same quaternions with both signs, and kind of randomly \"flips\" from one sign to the other at times, with some frames in between, resulting in a sort of \"flickering\". One possibility would be to take only \"half\" of the quaternions, e.g. by fixing one component to be positive (then learn by squared differences for example). It doesn\\'t always work, though. For example, suppose I fix the first component to be positive and I have a quaternion like , and another one like ; these are rather similar quaternions, but if I make the first component positive then the second one will be , which is numerically very different, and the result is that sometimes the network cannot learn the value properly and comes up with some numbers in between (which are incorrect). Rotation vectors. A rotation vector has the direction of the axis of rotation and the size of the angle of the rotation. They are used for example in the original paper of phase-functioned neural networks, but they still have bad cases. Suppose vector sizes go from 0 to π (greater angles would be taken along the opposite axis). If you have a rotation that is just around the value of π, the rotation vector will be \"swinging\" from one direction to the opposite one, again making it hard for the network to learn it. Euler angles. These are just three rotation angles that are applied along specific axes (X, Y or Z) in specific order. They are generally discouraged because they are not very stable and suffer from the infamous gimbal lock, but geometry aside, they still have the same problems. If my angles go from -180º to 180º (or -π to π in radians), values in the \"frontier\" will always cause instability. Extensive rotation encodings. One stable way of encoding rotations is giving both sine and cosine values. So I could for example have both the sine and cosine of each Euler angle, or just the nine values of a 3D rotation matrix (which are not sines and cosines but are in a way derived from these, and also a stable representation). However this obviously increases the number of values to learn significantly, and I would be trying to learn independent values that actually have a relationship among them.  I haven\\'t found relevant literature addressing this problem specifically, although maybe I am not searching with the right terms. Has anyone faced this issue before? Or has some idea or alternative that I could consider? ', 'Tags': {'neural-network', 'machine-learning'}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ChunkedEncodingError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m                 \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    670\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m                 \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m                 decoded = self._decode(chunk, decode_content=decode_content,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_handle_chunk\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# amt > self.chunk_left\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m             \u001b[0mreturned_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Toss the CRLF at the end of the chunk.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAXAMOUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1011\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1012\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionResetError\u001b[0m: [Errno 104] Connection reset by peer",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    750\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_chunked_reads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_response\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;31m# This includes IncompleteRead.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mProtocolError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Connection broken: %r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mProtocolError\u001b[0m: (\"Connection broken: ConnectionResetError(104, 'Connection reset by peer')\", ConnectionResetError(104, 'Connection reset by peer'))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mChunkedEncodingError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-27b8b5c3b82e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m                   \u001b[0mlink\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbaseLinkIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mquestionLink\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                   \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'User-agent'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'your bot 0.1'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    528\u001b[0m         }\n\u001b[1;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mcontent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    827\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONTENT_CHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content_consumed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    752\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mChunkedEncodingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mDecodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mContentDecodingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mChunkedEncodingError\u001b[0m: (\"Connection broken: ConnectionResetError(104, 'Connection reset by peer')\", ConnectionResetError(104, 'Connection reset by peer'))"
          ]
        }
      ]
    }
  ]
}