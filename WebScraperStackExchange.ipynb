{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WebScraperStackExchange.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPLcrbzuyatzKpue/B9Pvyi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shalinikumari50/Web-Scrapping/blob/master/WebScraperStackExchange.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmuyIM3dQKHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import csv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL2-aAUCQRqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CategoryList = [\n",
        "                \"https://datascience.stackexchange.com/questions/tagged/machine-learning?tab=newest&page=1&pagesize=50\", \n",
        "                \n",
        "                ]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9STbNBeQZ4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import re\n",
        "import time\n",
        "\n",
        "\n",
        "for category in CategoryList:\n",
        "            rowList = []\n",
        "            baseLinkIndex = category.find(\"/questions/\")\n",
        "            pageLinkIndex = category.find(\"1\")  \n",
        "            \n",
        "            categoryHTML = requests.get(category,headers = {'User-agent': 'your bot 0.1'})\n",
        "            if not categoryHTML.ok:\n",
        "                    print(\"scrapping stopped of a category\")\n",
        "                    time.sleep(300)\n",
        "                    continue\n",
        "            \n",
        "            parser = BeautifulSoup(categoryHTML.text, 'html.parser')\n",
        "            pages = parser.find('div',class_=\"fs-body3 grid--cell fl1 mr12 sm:mr0 sm:mb12\").getText()\n",
        "            noOfPages = int(\"\".join(map(str, re.findall('\\d+', pages))))\n",
        "            noOfPages = math.ceil(noOfPages/50)\n",
        "            \n",
        "            for i in range(1,noOfPages+1):\n",
        "              print(\"page number ------------------------------------\" + str(i))\n",
        "              pageLink = category[:pageLinkIndex]+ str(i) + category[pageLinkIndex+1:]\n",
        "             \n",
        "              pageHTML = requests.get(pageLink,headers = {'User-agent': 'your bot 0.1'})\n",
        "              if not pageHTML.ok:\n",
        "                    print(\"scrapping stopped of a page\")\n",
        "                    time.sleep(200)\n",
        "                    continue\n",
        "              parser = BeautifulSoup(pageHTML.text, 'html.parser')\n",
        "              questionLinks = parser.find_all('a',class_=\"question-hyperlink\")\n",
        "              for questionLink in questionLinks:\n",
        "                  anchor = questionLink.get('href')\n",
        "                  if not 'questions' in anchor or 'https://' in anchor:\n",
        "                        continue\n",
        "                  link=category[:baseLinkIndex]+questionLink.get('href')\n",
        "                  \n",
        "                  url = requests.get(link,headers = {'User-agent': 'your bot 0.1'})\n",
        "                \n",
        "                  if not url.ok:\n",
        "                    print(\"scrapping stopped of a question\")\n",
        "                    time.sleep(30)\n",
        "                    break\n",
        "                  ques = BeautifulSoup(url.text,'html.parser')\n",
        "                  post = ques.find('div', class_=\"post-text\")\n",
        "                  maths = post.find_all('span',class_=\"math-container\")\n",
        "                  codes = post.find_all('code')\n",
        "                  for math in maths:\n",
        "                    math.extract()\n",
        "                  for code in codes:\n",
        "                    code.extract() \n",
        "                  posts = post.getText()\n",
        "                  cleaned_post = ''.join([x.replace(\"\\n\",\" \") for x in posts])\n",
        "                  tags = ques.find_all('a', class_=\"post-tag\",rel = 'tag')\n",
        "                  tagList = []\n",
        "                  for t in tags:\n",
        "                    tagList.append(t.getText())\n",
        "                  \n",
        "                  \n",
        "                  row = {'link': link, 'post':cleaned_post,'Tags' : set(tagList)}\n",
        "                  print(row)\n",
        "                  rowList.append(row)\n",
        "                  scraped_data = pd.DataFrame(rowList)\n",
        "                  catIndex = category.find(\".\")\n",
        "                  tagIndex = category.find(\"/tagged/\")\n",
        "                  tagEndIndex = category.find(\"?\")\n",
        "                  name = category[8:catIndex] + \"_\" +category[tagIndex+8:tagEndIndex] + \".csv\"\n",
        "                  scraped_data.to_csv(name)                \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}